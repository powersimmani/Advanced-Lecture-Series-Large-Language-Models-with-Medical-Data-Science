<!DOCTYPE html>
<html lang="ko">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Direct Preference Optimization (DPO)</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: Aptos, 'Segoe UI', sans-serif;
            background: white;
            display: flex;
            justify-content: center;
            align-items: center;
            min-height: 100vh;
        }
        
        .container {
            width: 960px;
            height: 540px;
            padding: 35px 45px;
            background: white;
            overflow-y: auto;
        }
        
        .title {
            font-size: 26px;
            font-weight: 700;
            color: #1E64C8;
            margin-bottom: 22px;
            text-align: center;
        }
        
        .section {
            background: #f0f5fc;
            border-left: 5px solid #1E64C8;
            border-radius: 8px;
            padding: 16px 20px;
            margin-bottom: 18px;
        }
        
        .section-title {
            font-size: 17px;
            font-weight: 700;
            color: #1E64C8;
            margin-bottom: 8px;
        }
        
        .section-text {
            font-size: 14px;
            color: #333;
            line-height: 1.6;
        }
        
        .items-list {
            display: flex;
            flex-direction: column;
            gap: 6px;
        }
        
        .item {
            font-size: 14px;
            color: #333;
            line-height: 1.5;
            padding-left: 18px;
            position: relative;
        }
        
        .item:before {
            content: "â€¢";
            position: absolute;
            left: 0;
            color: #1E64C8;
            font-weight: 700;
            font-size: 16px;
        }
    </style>
</head>
<body>
    <div class="container">
        <div class="title">Direct Preference Optimization (DPO)</div>
        
        <div class="section">
            <div class="section-title">What is DPO?</div>
            <div class="section-text">DPO directly optimizes the language model using preference data, eliminating the need for a separate reward model and RL training loop.</div>
        </div>
        <div class="section">
            <div class="section-title">DPO vs PPO Comparison</div>
            <div class="items-list">
                <div class="item">Simplicity: No separate reward model training required</div>
                <div class="item">Efficiency: Fewer hyperparameters to tune</div>
                <div class="item">Stability: More stable training dynamics</div>
                <div class="item">Performance: Often achieves similar or better results than PPO</div>
                <div class="item">Computational Cost: Lower memory and compute requirements</div>
            </div>
        </div>
        <div class="section">
            <div class="section-title">DPO Objective</div>
            <div class="items-list">
                <div class="item">Maximize likelihood of preferred outputs</div>
                <div class="item">Minimize likelihood of dispreferred outputs</div>
                <div class="item">Maintain KL constraint to reference model</div>
                <div class="item">Single-stage optimization process</div>
            </div>
        </div>
        <div class="section">
            <div class="section-title">When to Use DPO</div>
            <div class="items-list">
                <div class="item">Limited computational resources</div>
                <div class="item">Preference data available but reward modeling challenging</div>
                <div class="item">Need for simpler training pipeline</div>
                <div class="item">Preference for stable, predictable training</div>
            </div>
        </div>
    </div>
</body>
</html>