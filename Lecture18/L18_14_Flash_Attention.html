<!DOCTYPE html>
<html lang="ko">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Flash Attention: IO-Aware Optimization</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: Aptos, 'Segoe UI', sans-serif;
            background: white;
            display: flex;
            justify-content: center;
            align-items: center;
            min-height: 100vh;
        }
        
        .container {
            width: 960px;
            height: 540px;
            padding: 35px 45px;
            background: white;
        }
        
        .title {
            font-size: 20px;
            font-weight: 600;
            color: #1E64C8;
            margin-bottom: 25px;
            text-align: center;
        }
        
        .content-grid {
            display: grid;
            grid-template-columns: repeat(2, 1fr);
            gap: 20px;
            margin-bottom: 20px;
        }
        
        .content-card {
            background: white;
            border: 2.5px solid #1E64C8;
            border-radius: 10px;
            padding: 20px;
            transition: all 0.3s;
        }
        
        .content-card:hover {
            transform: translateY(-5px);
            box-shadow: 0 6px 20px rgba(30, 100, 200, 0.2);
            background: #f8fbff;
        }
        
        .card-title {
            font-size: 17px;
            font-weight: 700;
            color: #1E64C8;
            margin-bottom: 12px;
        }
        
        .card-text {
            font-size: 15px;
            color: #333;
            line-height: 1.5;
            margin-bottom: 8px;
        }
        
        .highlight-box {
            background: #f0f5fc;
            border: 2px solid #1E64C8;
            border-radius: 8px;
            padding: 15px 20px;
            margin-top: 20px;
        }
        
        .highlight-title {
            font-size: 16px;
            font-weight: 700;
            color: #1E64C8;
            margin-bottom: 8px;
            text-align: center;
        }
        
        .highlight-text {
            font-size: 15px;
            color: #333;
            text-align: center;
            line-height: 1.5;
        }
    </style>
</head>
<body>
    <div class="container">
        <div class="title">Flash Attention: IO-Aware Optimization</div>
        <div class="content-grid">
            <div class="content-card">
                <div class="card-title">Memory Hierarchy</div>
                <div class="card-text">â€¢ SRAM (fast): limited capacity</div>
                <div class="card-text">â€¢ HBM (slow): large capacity</div>
                <div class="card-text">â€¢ IO operations are bottleneck</div>
            </div>
            <div class="content-card">
                <div class="card-title">Flash Attention v1</div>
                <div class="card-text">â€¢ Tiling and recomputation</div>
                <div class="card-text">â€¢ Block-wise attention computation</div>
                <div class="card-text">â€¢ 2-4x speedup</div>
            </div>
            <div class="content-card">
                <div class="card-title">Flash Attention v2</div>
                <div class="card-text">â€¢ Improved parallelization</div>
                <div class="card-text">â€¢ Better work partitioning</div>
                <div class="card-text">â€¢ 5-9x faster than standard</div>
            </div>
            <div class="content-card">
                <div class="card-title">Medical Impact</div>
                <div class="card-text">â€¢ Real-time patient history analysis</div>
                <div class="card-text">â€¢ Affordable long-context inference</div>
                <div class="card-text">â€¢ Enables clinical deployment</div>
            </div>
        </div>
        <div class="highlight-box">
            <div class="highlight-title">ðŸš€ Revolutionary Efficiency</div>
            <div class="highlight-text">Flash Attention achieves exact attention with O(n) memory and 3-10x speedup by optimizing GPU memory access patterns, making long-context medical AI practical.</div>
        </div>
    </div>
</body>
</html>