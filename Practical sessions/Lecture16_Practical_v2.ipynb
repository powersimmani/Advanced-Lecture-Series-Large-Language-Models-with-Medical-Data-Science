{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ Medical MLOps: Hands-On Practice\n",
    "\n",
    "## Table of Contents\n",
    "1. [Container Setup with Docker](#practice-1-container-setup-with-docker)\n",
    "2. [Model Registry with MLflow](#practice-2-model-registry-with-mlflow)\n",
    "3. [Data Versioning with DVC](#practice-3-data-versioning-with-dvc)\n",
    "4. [CI/CD Pipeline Setup](#practice-4-cicd-pipeline-setup)\n",
    "5. [Model Performance Monitoring](#practice-5-model-performance-monitoring)\n",
    "6. [Data Drift Detection](#practice-6-data-drift-detection)\n",
    "7. [Complete MLOps Pipeline](#practice-7-complete-mlops-pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing and Importing Essential Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (run once)\n",
    "# !pip install mlflow scikit-learn numpy pandas matplotlib seaborn scipy\n",
    "\n",
    "# Import essential libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from scipy import stats\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Visualization settings\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 12\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "print(\"‚úÖ All libraries loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Practice 1: Container Setup with Docker\n",
    "\n",
    "### üéØ Learning Objectives\n",
    "- Understand containerization concepts\n",
    "- Create a simple Dockerfile for ML models\n",
    "- Learn reproducible environment setup\n",
    "\n",
    "### üìñ Key Concepts\n",
    "**Docker**: Platform for packaging applications with all dependencies into containers  \n",
    "**Benefits**: Reproducibility, Isolation, Portability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.1 Create a simple Dockerfile template\n",
    "def create_dockerfile_template():\n",
    "    \"\"\"Generate a Dockerfile template for ML model deployment\"\"\"\n",
    "    \n",
    "    dockerfile_content = \"\"\"\n",
    "# Dockerfile for Medical ML Model\n",
    "FROM python:3.9-slim\n",
    "\n",
    "# Set working directory\n",
    "WORKDIR /app\n",
    "\n",
    "# Copy requirements\n",
    "COPY requirements.txt .\n",
    "\n",
    "# Install dependencies\n",
    "RUN pip install --no-cache-dir -r requirements.txt\n",
    "\n",
    "# Copy application code\n",
    "COPY . .\n",
    "\n",
    "# Expose port\n",
    "EXPOSE 8000\n",
    "\n",
    "# Run application\n",
    "CMD [\"python\", \"app.py\"]\n",
    "\"\"\"\n",
    "    \n",
    "    requirements_content = \"\"\"\n",
    "numpy==1.24.3\n",
    "pandas==2.0.3\n",
    "scikit-learn==1.3.0\n",
    "mlflow==2.8.0\n",
    "fastapi==0.104.1\n",
    "uvicorn==0.24.0\n",
    "\"\"\"\n",
    "    \n",
    "    print(\"üì¶ Docker Configuration\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"\\n[Dockerfile]\")\n",
    "    print(dockerfile_content)\n",
    "    print(\"\\n[requirements.txt]\")\n",
    "    print(requirements_content)\n",
    "    \n",
    "    print(\"\\nüí° Docker Commands:\")\n",
    "    print(\"  Build: docker build -t medical-ml-model .\")\n",
    "    print(\"  Run:   docker run -p 8000:8000 medical-ml-model\")\n",
    "    print(\"  Push:  docker push registry/medical-ml-model:v1.0\")\n",
    "    \n",
    "    return dockerfile_content, requirements_content\n",
    "\n",
    "dockerfile, requirements = create_dockerfile_template()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Practice 2: Model Registry with MLflow\n",
    "\n",
    "### üéØ Learning Objectives\n",
    "- Track model experiments with MLflow\n",
    "- Version and register models\n",
    "- Compare model performance across versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1 Generate synthetic medical data\n",
    "def generate_medical_data(n_samples=1000):\n",
    "    \"\"\"Create synthetic medical diagnostic data\"\"\"\n",
    "    \n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Generate features (patient measurements)\n",
    "    X, y = make_classification(\n",
    "        n_samples=n_samples,\n",
    "        n_features=10,\n",
    "        n_informative=8,\n",
    "        n_redundant=2,\n",
    "        n_classes=2,\n",
    "        weights=[0.7, 0.3],\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Create DataFrame\n",
    "    feature_names = [f'Feature_{i+1}' for i in range(10)]\n",
    "    df = pd.DataFrame(X, columns=feature_names)\n",
    "    df['Diagnosis'] = y  # 0: Negative, 1: Positive\n",
    "    \n",
    "    print(\"üè• Synthetic Medical Dataset Generated\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Total samples: {len(df)}\")\n",
    "    print(f\"Features: {len(feature_names)}\")\n",
    "    print(f\"\\nClass distribution:\")\n",
    "    print(df['Diagnosis'].value_counts())\n",
    "    print(f\"\\nFirst 5 rows:\")\n",
    "    print(df.head())\n",
    "    \n",
    "    return df\n",
    "\n",
    "medical_data = generate_medical_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2 Train and log model with MLflow\n",
    "def train_and_log_model(data, model_name=\"Medical_Diagnosis_Model\"):\n",
    "    \"\"\"Train a model and track with MLflow\"\"\"\n",
    "    \n",
    "    # Prepare data\n",
    "    X = data.drop('Diagnosis', axis=1)\n",
    "    y = data['Diagnosis']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    # Set MLflow experiment\n",
    "    mlflow.set_experiment(\"Medical_MLOps_Demo\")\n",
    "    \n",
    "    print(\"üî¨ Training Model with MLflow Tracking\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    with mlflow.start_run(run_name=\"RandomForest_v1\"):\n",
    "        # Train model\n",
    "        model = RandomForestClassifier(\n",
    "            n_estimators=100,\n",
    "            max_depth=10,\n",
    "            random_state=42\n",
    "        )\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Make predictions\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred)\n",
    "        recall = recall_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        \n",
    "        # Log parameters\n",
    "        mlflow.log_param(\"n_estimators\", 100)\n",
    "        mlflow.log_param(\"max_depth\", 10)\n",
    "        mlflow.log_param(\"model_type\", \"RandomForest\")\n",
    "        \n",
    "        # Log metrics\n",
    "        mlflow.log_metric(\"accuracy\", accuracy)\n",
    "        mlflow.log_metric(\"precision\", precision)\n",
    "        mlflow.log_metric(\"recall\", recall)\n",
    "        mlflow.log_metric(\"f1_score\", f1)\n",
    "        \n",
    "        # Log model\n",
    "        mlflow.sklearn.log_model(model, \"model\")\n",
    "        \n",
    "        print(\"\\nüìä Model Performance:\")\n",
    "        print(f\"  Accuracy:  {accuracy:.4f}\")\n",
    "        print(f\"  Precision: {precision:.4f}\")\n",
    "        print(f\"  Recall:    {recall:.4f}\")\n",
    "        print(f\"  F1 Score:  {f1:.4f}\")\n",
    "        \n",
    "        print(\"\\n‚úÖ Model logged to MLflow successfully!\")\n",
    "        print(\"üí° View results: mlflow ui\")\n",
    "    \n",
    "    return model, X_test, y_test, y_pred\n",
    "\n",
    "model, X_test, y_test, y_pred = train_and_log_model(medical_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Practice 3: Data Versioning with DVC\n",
    "\n",
    "### üéØ Learning Objectives\n",
    "- Understand the importance of data versioning\n",
    "- Learn DVC commands and workflow\n",
    "- Track dataset changes over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1 DVC workflow demonstration\n",
    "def demonstrate_dvc_workflow():\n",
    "    \"\"\"Show DVC commands for data versioning\"\"\"\n",
    "    \n",
    "    dvc_workflow = \"\"\"\n",
    "üì¶ Data Version Control (DVC) Workflow\n",
    "==========================================\n",
    "\n",
    "1. Initialize DVC:\n",
    "   $ dvc init\n",
    "   $ git add .dvc .dvcignore\n",
    "   $ git commit -m \"Initialize DVC\"\n",
    "\n",
    "2. Add data to DVC tracking:\n",
    "   $ dvc add data/medical_images.zip\n",
    "   $ git add data/medical_images.zip.dvc data/.gitignore\n",
    "   $ git commit -m \"Add medical images dataset v1.0\"\n",
    "\n",
    "3. Configure remote storage:\n",
    "   $ dvc remote add -d storage s3://mybucket/dvc-storage\n",
    "   $ git add .dvc/config\n",
    "   $ git commit -m \"Configure DVC remote storage\"\n",
    "\n",
    "4. Push data to remote:\n",
    "   $ dvc push\n",
    "\n",
    "5. Pull data from remote:\n",
    "   $ dvc pull\n",
    "\n",
    "6. Checkout specific version:\n",
    "   $ git checkout v1.0 data/medical_images.zip.dvc\n",
    "   $ dvc checkout\n",
    "\n",
    "üí° Benefits:\n",
    "   - Version large datasets efficiently\n",
    "   - Reproducible experiments\n",
    "   - Collaborate on data without Git bloat\n",
    "   - Track data lineage\n",
    "\"\"\"\n",
    "    \n",
    "    print(dvc_workflow)\n",
    "    \n",
    "    # Save dataset with version info\n",
    "    dataset_info = {\n",
    "        'version': '1.0.0',\n",
    "        'samples': len(medical_data),\n",
    "        'features': len(medical_data.columns) - 1,\n",
    "        'date': pd.Timestamp.now().strftime('%Y-%m-%d'),\n",
    "        'description': 'Initial medical diagnostic dataset'\n",
    "    }\n",
    "    \n",
    "    print(\"\\nüìã Current Dataset Version:\")\n",
    "    for key, value in dataset_info.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    \n",
    "    return dataset_info\n",
    "\n",
    "dataset_version = demonstrate_dvc_workflow()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Practice 4: CI/CD Pipeline Setup\n",
    "\n",
    "### üéØ Learning Objectives\n",
    "- Understand CI/CD concepts for ML\n",
    "- Create automated testing pipeline\n",
    "- Implement deployment strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1 Create GitHub Actions workflow\n",
    "def create_cicd_pipeline():\n",
    "    \"\"\"Generate CI/CD pipeline configuration\"\"\"\n",
    "    \n",
    "    github_actions = \"\"\"\n",
    "# .github/workflows/ml-pipeline.yml\n",
    "name: ML Model CI/CD\n",
    "\n",
    "on:\n",
    "  push:\n",
    "    branches: [ main ]\n",
    "  pull_request:\n",
    "    branches: [ main ]\n",
    "\n",
    "jobs:\n",
    "  test:\n",
    "    runs-on: ubuntu-latest\n",
    "    steps:\n",
    "      - uses: actions/checkout@v3\n",
    "      \n",
    "      - name: Set up Python\n",
    "        uses: actions/setup-python@v4\n",
    "        with:\n",
    "          python-version: '3.9'\n",
    "      \n",
    "      - name: Install dependencies\n",
    "        run: |\n",
    "          pip install -r requirements.txt\n",
    "          pip install pytest\n",
    "      \n",
    "      - name: Run unit tests\n",
    "        run: pytest tests/\n",
    "      \n",
    "      - name: Check model performance\n",
    "        run: python scripts/validate_model.py\n",
    "  \n",
    "  deploy:\n",
    "    needs: test\n",
    "    runs-on: ubuntu-latest\n",
    "    if: github.ref == 'refs/heads/main'\n",
    "    steps:\n",
    "      - name: Deploy to staging\n",
    "        run: |\n",
    "          echo \"Deploying to staging environment\"\n",
    "          # kubectl apply -f k8s/staging/\n",
    "      \n",
    "      - name: Run integration tests\n",
    "        run: python scripts/integration_test.py\n",
    "      \n",
    "      - name: Deploy to production\n",
    "        run: |\n",
    "          echo \"Deploying to production\"\n",
    "          # kubectl apply -f k8s/production/\n",
    "\"\"\"\n",
    "    \n",
    "    print(\"üîÑ CI/CD Pipeline Configuration\")\n",
    "    print(\"=\" * 60)\n",
    "    print(github_actions)\n",
    "    \n",
    "    # Pipeline stages visualization\n",
    "    stages = [\n",
    "        \"1Ô∏è‚É£ Code Commit ‚Üí GitHub\",\n",
    "        \"2Ô∏è‚É£ Build & Test ‚Üí Run pytest\",\n",
    "        \"3Ô∏è‚É£ Model Validation ‚Üí Check metrics\",\n",
    "        \"4Ô∏è‚É£ Deploy to Staging ‚Üí Test environment\",\n",
    "        \"5Ô∏è‚É£ Integration Tests ‚Üí End-to-end validation\",\n",
    "        \"6Ô∏è‚É£ Deploy to Production ‚Üí Live system\"\n",
    "    ]\n",
    "    \n",
    "    print(\"\\nüìä Pipeline Stages:\")\n",
    "    for stage in stages:\n",
    "        print(f\"  {stage}\")\n",
    "    \n",
    "    return github_actions\n",
    "\n",
    "cicd_config = create_cicd_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Practice 5: Model Performance Monitoring\n",
    "\n",
    "### üéØ Learning Objectives\n",
    "- Track model metrics over time\n",
    "- Detect performance degradation\n",
    "- Set up alerting thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.1 Simulate model performance over time\n",
    "def simulate_model_monitoring(n_days=30):\n",
    "    \"\"\"Simulate model performance metrics over time\"\"\"\n",
    "    \n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Simulate metrics with slight degradation\n",
    "    days = np.arange(1, n_days + 1)\n",
    "    accuracy = 0.95 - 0.001 * days + np.random.normal(0, 0.01, n_days)\n",
    "    latency = 100 + 2 * days + np.random.normal(0, 5, n_days)  # milliseconds\n",
    "    error_rate = 0.05 + 0.001 * days + np.random.normal(0, 0.005, n_days)\n",
    "    \n",
    "    # Create monitoring DataFrame\n",
    "    monitoring_data = pd.DataFrame({\n",
    "        'Day': days,\n",
    "        'Accuracy': accuracy,\n",
    "        'Latency_ms': latency,\n",
    "        'Error_Rate': error_rate\n",
    "    })\n",
    "    \n",
    "    # Plot metrics\n",
    "    fig, axes = plt.subplots(3, 1, figsize=(12, 10))\n",
    "    \n",
    "    # Accuracy plot\n",
    "    axes[0].plot(monitoring_data['Day'], monitoring_data['Accuracy'], \n",
    "                 marker='o', color='blue', alpha=0.7)\n",
    "    axes[0].axhline(y=0.92, color='red', linestyle='--', label='Alert Threshold')\n",
    "    axes[0].set_xlabel('Day')\n",
    "    axes[0].set_ylabel('Accuracy')\n",
    "    axes[0].set_title('Model Accuracy Over Time')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Latency plot\n",
    "    axes[1].plot(monitoring_data['Day'], monitoring_data['Latency_ms'], \n",
    "                 marker='s', color='green', alpha=0.7)\n",
    "    axes[1].axhline(y=150, color='red', linestyle='--', label='SLA Threshold')\n",
    "    axes[1].set_xlabel('Day')\n",
    "    axes[1].set_ylabel('Latency (ms)')\n",
    "    axes[1].set_title('Inference Latency Over Time')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Error rate plot\n",
    "    axes[2].plot(monitoring_data['Day'], monitoring_data['Error_Rate'], \n",
    "                 marker='^', color='orange', alpha=0.7)\n",
    "    axes[2].axhline(y=0.10, color='red', linestyle='--', label='Alert Threshold')\n",
    "    axes[2].set_xlabel('Day')\n",
    "    axes[2].set_ylabel('Error Rate')\n",
    "    axes[2].set_title('System Error Rate Over Time')\n",
    "    axes[2].legend()\n",
    "    axes[2].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Check for alerts\n",
    "    print(\"\\n‚ö†Ô∏è Alert Configuration:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    alerts = []\n",
    "    if monitoring_data['Accuracy'].iloc[-1] < 0.92:\n",
    "        alerts.append(\"üö® Accuracy dropped below 92%\")\n",
    "    if monitoring_data['Latency_ms'].iloc[-1] > 150:\n",
    "        alerts.append(\"üö® Latency exceeded 150ms SLA\")\n",
    "    if monitoring_data['Error_Rate'].iloc[-1] > 0.10:\n",
    "        alerts.append(\"üö® Error rate exceeded 10%\")\n",
    "    \n",
    "    if alerts:\n",
    "        for alert in alerts:\n",
    "            print(f\"  {alert}\")\n",
    "    else:\n",
    "        print(\"  ‚úÖ All metrics within normal range\")\n",
    "    \n",
    "    return monitoring_data\n",
    "\n",
    "monitoring_data = simulate_model_monitoring()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Practice 6: Data Drift Detection\n",
    "\n",
    "### üéØ Learning Objectives\n",
    "- Detect distribution changes in input data\n",
    "- Apply statistical tests (KS test, Chi-square)\n",
    "- Trigger model retraining when needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.1 Detect data drift using statistical tests\n",
    "def detect_data_drift(original_data, new_data, threshold=0.05):\n",
    "    \"\"\"Detect drift using Kolmogorov-Smirnov test\"\"\"\n",
    "    \n",
    "    print(\"üîç Data Drift Detection\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    drift_detected = False\n",
    "    feature_drifts = {}\n",
    "    \n",
    "    for column in original_data.columns:\n",
    "        if column == 'Diagnosis':\n",
    "            continue\n",
    "        \n",
    "        # Perform KS test\n",
    "        statistic, p_value = stats.ks_2samp(\n",
    "            original_data[column],\n",
    "            new_data[column]\n",
    "        )\n",
    "        \n",
    "        feature_drifts[column] = {\n",
    "            'statistic': statistic,\n",
    "            'p_value': p_value,\n",
    "            'drift': p_value < threshold\n",
    "        }\n",
    "        \n",
    "        if p_value < threshold:\n",
    "            drift_detected = True\n",
    "    \n",
    "    # Print results\n",
    "    print(\"\\nüìä Feature Drift Analysis:\")\n",
    "    print(f\"{'Feature':<15} {'KS Statistic':<15} {'P-Value':<12} {'Drift?'}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for feature, metrics in feature_drifts.items():\n",
    "        drift_status = \"‚ö†Ô∏è YES\" if metrics['drift'] else \"‚úÖ NO\"\n",
    "        print(f\"{feature:<15} {metrics['statistic']:<15.4f} \"\n",
    "              f\"{metrics['p_value']:<12.4f} {drift_status}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    if drift_detected:\n",
    "        print(\"üö® Data drift detected! Consider retraining the model.\")\n",
    "    else:\n",
    "        print(\"‚úÖ No significant drift detected.\")\n",
    "    \n",
    "    return feature_drifts, drift_detected\n",
    "\n",
    "# Generate new data with slight drift\n",
    "def generate_drifted_data(n_samples=500):\n",
    "    \"\"\"Generate data with distribution shift\"\"\"\n",
    "    np.random.seed(100)\n",
    "    \n",
    "    X, y = make_classification(\n",
    "        n_samples=n_samples,\n",
    "        n_features=10,\n",
    "        n_informative=8,\n",
    "        n_redundant=2,\n",
    "        n_classes=2,\n",
    "        weights=[0.6, 0.4],  # Changed distribution\n",
    "        random_state=100\n",
    "    )\n",
    "    \n",
    "    # Add slight shift to features\n",
    "    X = X + np.random.normal(0.3, 0.1, X.shape)\n",
    "    \n",
    "    feature_names = [f'Feature_{i+1}' for i in range(10)]\n",
    "    df = pd.DataFrame(X, columns=feature_names)\n",
    "    df['Diagnosis'] = y\n",
    "    \n",
    "    return df\n",
    "\n",
    "new_data = generate_drifted_data()\n",
    "drift_results, has_drift = detect_data_drift(medical_data, new_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Practice 7: Complete MLOps Pipeline\n",
    "\n",
    "### üéØ Learning Objectives\n",
    "- Integrate all MLOps components\n",
    "- Create end-to-end workflow\n",
    "- Understand production deployment cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.1 Complete MLOps pipeline implementation\n",
    "def complete_mlops_pipeline():\n",
    "    \"\"\"Demonstrate complete MLOps workflow\"\"\"\n",
    "    \n",
    "    pipeline_steps = \"\"\"\n",
    "üöÄ Complete MLOps Pipeline\n",
    "==========================================\n",
    "\n",
    "Step 1: Data Versioning\n",
    "  ‚úÖ Track datasets with DVC\n",
    "  ‚úÖ Version control with Git\n",
    "  ‚úÖ Store in cloud (S3/Azure/GCS)\n",
    "\n",
    "Step 2: Model Training\n",
    "  ‚úÖ Train model with versioned data\n",
    "  ‚úÖ Log experiments to MLflow\n",
    "  ‚úÖ Track hyperparameters and metrics\n",
    "\n",
    "Step 3: Model Registry\n",
    "  ‚úÖ Register model in MLflow\n",
    "  ‚úÖ Tag with metadata (version, author, date)\n",
    "  ‚úÖ Stage: Development ‚Üí Staging ‚Üí Production\n",
    "\n",
    "Step 4: Containerization\n",
    "  ‚úÖ Create Dockerfile\n",
    "  ‚úÖ Build Docker image\n",
    "  ‚úÖ Push to container registry\n",
    "\n",
    "Step 5: CI/CD Pipeline\n",
    "  ‚úÖ Automated testing (unit, integration)\n",
    "  ‚úÖ Model validation checks\n",
    "  ‚úÖ Gradual rollout (canary/blue-green)\n",
    "\n",
    "Step 6: Deployment\n",
    "  ‚úÖ Deploy to Kubernetes cluster\n",
    "  ‚úÖ Configure auto-scaling\n",
    "  ‚úÖ Set up load balancing\n",
    "\n",
    "Step 7: Monitoring\n",
    "  ‚úÖ Track performance metrics\n",
    "  ‚úÖ Detect data drift\n",
    "  ‚úÖ Alert on anomalies\n",
    "  ‚úÖ Collect audit logs\n",
    "\n",
    "Step 8: Maintenance\n",
    "  ‚úÖ Regular model retraining\n",
    "  ‚úÖ A/B testing new versions\n",
    "  ‚úÖ Rollback if needed\n",
    "  ‚úÖ Continuous improvement\n",
    "\"\"\"\n",
    "    \n",
    "    print(pipeline_steps)\n",
    "    \n",
    "    # Create visualization of pipeline\n",
    "    fig, ax = plt.subplots(figsize=(14, 8))\n",
    "    \n",
    "    stages = [\n",
    "        'Data\\nVersioning',\n",
    "        'Model\\nTraining',\n",
    "        'Model\\nRegistry',\n",
    "        'Container\\nization',\n",
    "        'CI/CD\\nPipeline',\n",
    "        'Deploy\\nment',\n",
    "        'Monitor\\ning',\n",
    "        'Main\\ntenance'\n",
    "    ]\n",
    "    \n",
    "    x_positions = np.arange(len(stages))\n",
    "    colors = plt.cm.viridis(np.linspace(0.2, 0.9, len(stages)))\n",
    "    \n",
    "    # Plot stages\n",
    "    for i, (stage, color) in enumerate(zip(stages, colors)):\n",
    "        ax.bar(i, 1, color=color, alpha=0.7, edgecolor='black', linewidth=2)\n",
    "        ax.text(i, 0.5, stage, ha='center', va='center', \n",
    "                fontsize=11, fontweight='bold', color='white')\n",
    "        \n",
    "        # Add arrows\n",
    "        if i < len(stages) - 1:\n",
    "            ax.annotate('', xy=(i + 0.5, 0.5), xytext=(i + 0.5, 0.5),\n",
    "                       arrowprops=dict(arrowstyle='->', lw=3, color='gray'))\n",
    "    \n",
    "    ax.set_xlim(-0.5, len(stages) - 0.5)\n",
    "    ax.set_ylim(0, 1.2)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_title('Complete MLOps Pipeline Flow', fontsize=16, fontweight='bold', pad=20)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines('right').set_visible(False)\n",
    "    ax.spines['left'].set_visible(False)\n",
    "    ax.spines['bottom'].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nüí° Key Takeaways:\")\n",
    "    print(\"  ‚Ä¢ MLOps is more than just deploying a model\")\n",
    "    print(\"  ‚Ä¢ Automation and monitoring are critical\")\n",
    "    print(\"  ‚Ä¢ Version everything: data, code, models\")\n",
    "    print(\"  ‚Ä¢ Continuous improvement through feedback loops\")\n",
    "    print(\"  ‚Ä¢ Healthcare requires extra compliance and governance\")\n",
    "\n",
    "complete_mlops_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üéØ Practice Complete!\n",
    "\n",
    "### Summary of What We Learned:\n",
    "\n",
    "1. **Containerization**: Packaging models with Docker for reproducibility\n",
    "2. **Model Registry**: Tracking experiments and versions with MLflow\n",
    "3. **Data Versioning**: Managing datasets with DVC\n",
    "4. **CI/CD**: Automating testing and deployment workflows\n",
    "5. **Monitoring**: Tracking performance metrics in production\n",
    "6. **Drift Detection**: Identifying when models need retraining\n",
    "7. **Complete Pipeline**: Integrating all components for production ML\n",
    "\n",
    "### Key Insights:\n",
    "- **MLOps ‚â† Just Deployment**: It's a complete lifecycle management system\n",
    "- **Automation is Key**: Reduce manual errors and speed up iterations\n",
    "- **Monitoring is Critical**: Catch issues before they impact users\n",
    "- **Version Everything**: Data, code, models, and infrastructure\n",
    "\n",
    "### Next Steps:\n",
    "1. Set up a complete MLOps environment locally\n",
    "2. Practice with real medical datasets (MIMIC, PhysioNet)\n",
    "3. Learn Kubernetes for orchestration\n",
    "4. Explore advanced monitoring with Prometheus + Grafana\n",
    "5. Study healthcare compliance (HIPAA, FDA, GDPR)\n",
    "\n",
    "### Additional Resources:\n",
    "- MLflow Documentation: https://mlflow.org/\n",
    "- DVC Documentation: https://dvc.org/\n",
    "- Kubernetes Documentation: https://kubernetes.io/\n",
    "- Medical MLOps Papers: Search on Papers with Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üìù Exercise: Build Your Own Pipeline\n",
    "\n",
    "### Challenge\n",
    "Using the concepts learned, create a complete MLOps pipeline for a medical image classification task:\n",
    "\n",
    "1. Load a medical image dataset\n",
    "2. Train a CNN model\n",
    "3. Log experiments to MLflow\n",
    "4. Create a Docker container\n",
    "5. Set up monitoring metrics\n",
    "6. Implement drift detection\n",
    "\n",
    "**Bonus**: Deploy to a cloud platform (AWS/GCP/Azure) and set up CI/CD!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
