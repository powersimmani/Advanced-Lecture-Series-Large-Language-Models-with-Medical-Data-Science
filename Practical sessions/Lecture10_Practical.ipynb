{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üè• Multimodal Medical AI: Practical Implementation\n",
    "\n",
    "## Table of Contents\n",
    "1. [Image-Text Alignment with Medical Data](#practice-1-image-text-alignment)\n",
    "2. [ECG Signal Processing](#practice-2-ecg-signal-processing)\n",
    "3. [Multimodal Fusion Strategies](#practice-3-multimodal-fusion)\n",
    "4. [Building a Simple Medical VLM](#practice-4-medical-vlm)\n",
    "5. [Attention-based Fusion](#practice-5-attention-fusion)\n",
    "\n",
    "### üéØ Learning Objectives\n",
    "- Implement basic multimodal fusion techniques\n",
    "- Process medical signals (ECG) for deep learning\n",
    "- Build image-text encoders for medical data\n",
    "- Apply attention mechanisms for cross-modal fusion\n",
    "- Evaluate multimodal model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing and Importing Essential Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (uncomment if needed)\n",
    "# !pip install torch torchvision transformers scikit-learn pandas numpy matplotlib seaborn\n",
    "\n",
    "# Import essential libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Visualization settings\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "print(\"‚úÖ All libraries loaded successfully!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device: {'CUDA' if torch.cuda.is_available() else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Practice 1: Image-Text Alignment with Medical Data\n",
    "\n",
    "### üéØ Learning Objectives\n",
    "- Understand contrastive learning for image-text pairs\n",
    "- Implement a simple CLIP-style alignment\n",
    "- Calculate cosine similarity between modalities\n",
    "\n",
    "### üìñ Key Concepts\n",
    "**Contrastive Learning:** Maximize similarity for matching pairs, minimize for non-matching pairs\n",
    "\n",
    "**InfoNCE Loss:** $\\mathcal{L} = -\\log \\frac{\\exp(\\text{sim}(i, t) / \\tau)}{\\sum_{j} \\exp(\\text{sim}(i, t_j) / \\tau)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.1 Simulate medical image and text embeddings\n",
    "def generate_medical_embeddings(n_samples=100, embedding_dim=128):\n",
    "    \"\"\"\n",
    "    Simulate embeddings for chest X-rays and radiology reports\n",
    "    In practice, these would come from CNN and text encoders\n",
    "    \"\"\"\n",
    "    # Simulate image embeddings (from ResNet/ViT)\n",
    "    image_embeddings = np.random.randn(n_samples, embedding_dim)\n",
    "    \n",
    "    # Simulate text embeddings (from BERT/BioClinicalBERT)\n",
    "    # Add some correlation with images for matching pairs\n",
    "    text_embeddings = image_embeddings + np.random.randn(n_samples, embedding_dim) * 0.3\n",
    "    \n",
    "    # Normalize to unit vectors\n",
    "    image_embeddings = image_embeddings / np.linalg.norm(image_embeddings, axis=1, keepdims=True)\n",
    "    text_embeddings = text_embeddings / np.linalg.norm(text_embeddings, axis=1, keepdims=True)\n",
    "    \n",
    "    return image_embeddings, text_embeddings\n",
    "\n",
    "# Generate embeddings\n",
    "img_emb, txt_emb = generate_medical_embeddings(n_samples=50)\n",
    "\n",
    "print(f\"Image embeddings shape: {img_emb.shape}\")\n",
    "print(f\"Text embeddings shape: {txt_emb.shape}\")\n",
    "print(f\"\\nSample image embedding (first 5 dims): {img_emb[0][:5]}\")\n",
    "print(f\"Sample text embedding (first 5 dims): {txt_emb[0][:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.2 Calculate similarity matrix and visualize\n",
    "def calculate_similarity_matrix(img_emb, txt_emb):\n",
    "    \"\"\"\n",
    "    Calculate cosine similarity between all image-text pairs\n",
    "    \"\"\"\n",
    "    # Cosine similarity = dot product of normalized vectors\n",
    "    similarity_matrix = img_emb @ txt_emb.T\n",
    "    \n",
    "    return similarity_matrix\n",
    "\n",
    "# Calculate similarities\n",
    "sim_matrix = calculate_similarity_matrix(img_emb, txt_emb)\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(sim_matrix[:20, :20], cmap='RdYlBu_r', center=0, \n",
    "            square=True, linewidths=0.5, cbar_kws={\"shrink\": 0.8})\n",
    "plt.title('Image-Text Similarity Matrix (First 20 samples)', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Text Index', fontsize=12)\n",
    "plt.ylabel('Image Index', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n‚úÖ Similarity matrix shape: {sim_matrix.shape}\")\n",
    "print(f\"Diagonal (matching pairs) mean similarity: {np.diag(sim_matrix).mean():.4f}\")\n",
    "print(f\"Off-diagonal (non-matching) mean similarity: {(sim_matrix.sum() - np.diag(sim_matrix).sum()) / (sim_matrix.size - len(sim_matrix)):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.3 Implement InfoNCE (Contrastive) Loss\n",
    "def contrastive_loss(similarity_matrix, temperature=0.07):\n",
    "    \"\"\"\n",
    "    Calculate InfoNCE loss for contrastive learning\n",
    "    \n",
    "    Args:\n",
    "        similarity_matrix: (N, N) cosine similarity matrix\n",
    "        temperature: temperature parameter for scaling\n",
    "    \"\"\"\n",
    "    n = similarity_matrix.shape[0]\n",
    "    \n",
    "    # Scale by temperature\n",
    "    logits = similarity_matrix / temperature\n",
    "    \n",
    "    # Labels are diagonal indices (matching pairs)\n",
    "    labels = np.arange(n)\n",
    "    \n",
    "    # Calculate cross-entropy loss\n",
    "    # Image-to-text\n",
    "    i2t_loss = -np.mean(np.log(np.exp(logits[labels, labels]) / np.exp(logits).sum(axis=1)))\n",
    "    \n",
    "    # Text-to-image\n",
    "    t2i_loss = -np.mean(np.log(np.exp(logits[labels, labels]) / np.exp(logits).sum(axis=0)))\n",
    "    \n",
    "    # Total loss\n",
    "    total_loss = (i2t_loss + t2i_loss) / 2\n",
    "    \n",
    "    return total_loss, i2t_loss, t2i_loss\n",
    "\n",
    "# Calculate loss\n",
    "total_loss, i2t_loss, t2i_loss = contrastive_loss(sim_matrix)\n",
    "\n",
    "print(\"üìä Contrastive Loss Results:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Image-to-Text Loss: {i2t_loss:.4f}\")\n",
    "print(f\"Text-to-Image Loss: {t2i_loss:.4f}\")\n",
    "print(f\"Total Loss: {total_loss:.4f}\")\n",
    "print(\"\\nüí° Lower loss indicates better alignment between modalities\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Practice 2: ECG Signal Processing\n",
    "\n",
    "### üéØ Learning Objectives\n",
    "- Generate and visualize ECG waveforms\n",
    "- Apply signal preprocessing techniques\n",
    "- Extract features for arrhythmia detection\n",
    "\n",
    "### üìñ Key Concepts\n",
    "**ECG Components:** P wave (atrial depolarization), QRS complex (ventricular depolarization), T wave (repolarization)\n",
    "\n",
    "**Preprocessing:** Filtering, baseline correction, normalization, R-peak detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1 Generate synthetic ECG signal\n",
    "def generate_ecg_signal(duration=10, sampling_rate=360, heart_rate=75):\n",
    "    \"\"\"\n",
    "    Generate synthetic ECG signal (simplified)\n",
    "    \n",
    "    Args:\n",
    "        duration: signal duration in seconds\n",
    "        sampling_rate: samples per second\n",
    "        heart_rate: beats per minute\n",
    "    \"\"\"\n",
    "    t = np.linspace(0, duration, duration * sampling_rate)\n",
    "    \n",
    "    # Calculate beats\n",
    "    beat_interval = 60 / heart_rate  # seconds per beat\n",
    "    \n",
    "    ecg = np.zeros_like(t)\n",
    "    \n",
    "    # Generate QRS complexes\n",
    "    for beat_time in np.arange(0, duration, beat_interval):\n",
    "        beat_idx = int(beat_time * sampling_rate)\n",
    "        \n",
    "        # QRS complex (simplified Gaussian)\n",
    "        qrs_width = 0.08  # seconds\n",
    "        qrs_sigma = qrs_width * sampling_rate / 6\n",
    "        qrs_indices = np.arange(max(0, beat_idx - int(3*qrs_sigma)), \n",
    "                                min(len(t), beat_idx + int(3*qrs_sigma)))\n",
    "        \n",
    "        if len(qrs_indices) > 0:\n",
    "            ecg[qrs_indices] += 1.5 * np.exp(-0.5 * ((qrs_indices - beat_idx) / qrs_sigma) ** 2)\n",
    "        \n",
    "        # P wave (before QRS)\n",
    "        p_idx = beat_idx - int(0.15 * sampling_rate)\n",
    "        if p_idx > 0 and p_idx < len(t):\n",
    "            p_width = int(0.08 * sampling_rate)\n",
    "            p_indices = np.arange(max(0, p_idx - p_width), min(len(t), p_idx + p_width))\n",
    "            if len(p_indices) > 0:\n",
    "                ecg[p_indices] += 0.3 * np.exp(-0.5 * ((p_indices - p_idx) / (p_width/3)) ** 2)\n",
    "        \n",
    "        # T wave (after QRS)\n",
    "        t_idx = beat_idx + int(0.25 * sampling_rate)\n",
    "        if t_idx < len(t):\n",
    "            t_width = int(0.15 * sampling_rate)\n",
    "            t_indices = np.arange(max(0, t_idx - t_width), min(len(t), t_idx + t_width))\n",
    "            if len(t_indices) > 0:\n",
    "                ecg[t_indices] += 0.5 * np.exp(-0.5 * ((t_indices - t_idx) / (t_width/3)) ** 2)\n",
    "    \n",
    "    # Add baseline and noise\n",
    "    baseline = 0.1 * np.sin(2 * np.pi * 0.2 * t)  # baseline wander\n",
    "    noise = np.random.normal(0, 0.05, len(t))  # random noise\n",
    "    ecg = ecg + baseline + noise\n",
    "    \n",
    "    return t, ecg\n",
    "\n",
    "# Generate normal and abnormal ECG\n",
    "t_normal, ecg_normal = generate_ecg_signal(duration=5, heart_rate=75)\n",
    "t_tachy, ecg_tachy = generate_ecg_signal(duration=5, heart_rate=120)  # Tachycardia\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 8))\n",
    "\n",
    "axes[0].plot(t_normal, ecg_normal, 'b-', linewidth=1.5)\n",
    "axes[0].set_title('Normal ECG (HR: 75 bpm)', fontsize=13, fontweight='bold')\n",
    "axes[0].set_ylabel('Amplitude (mV)', fontsize=11)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(t_tachy, ecg_tachy, 'r-', linewidth=1.5)\n",
    "axes[1].set_title('Tachycardia ECG (HR: 120 bpm)', fontsize=13, fontweight='bold')\n",
    "axes[1].set_xlabel('Time (seconds)', fontsize=11)\n",
    "axes[1].set_ylabel('Amplitude (mV)', fontsize=11)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"‚úÖ Generated ECG signals\")\n",
    "print(f\"Signal length: {len(ecg_normal)} samples\")\n",
    "print(f\"Sampling rate: 360 Hz\")\n",
    "print(f\"Duration: 5 seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2 ECG Preprocessing Pipeline\n",
    "def preprocess_ecg(ecg_signal, sampling_rate=360):\n",
    "    \"\"\"\n",
    "    Apply preprocessing steps to ECG signal\n",
    "    \"\"\"\n",
    "    # 1. Baseline correction (simple detrending)\n",
    "    from scipy import signal as scipy_signal\n",
    "    ecg_detrended = scipy_signal.detrend(ecg_signal)\n",
    "    \n",
    "    # 2. Band-pass filtering (0.5-40 Hz typical for ECG)\n",
    "    nyquist = sampling_rate / 2\n",
    "    low = 0.5 / nyquist\n",
    "    high = 40 / nyquist\n",
    "    b, a = scipy_signal.butter(4, [low, high], btype='band')\n",
    "    ecg_filtered = scipy_signal.filtfilt(b, a, ecg_detrended)\n",
    "    \n",
    "    # 3. Normalization (0-1 scaling)\n",
    "    ecg_normalized = (ecg_filtered - ecg_filtered.min()) / (ecg_filtered.max() - ecg_filtered.min())\n",
    "    \n",
    "    return ecg_normalized\n",
    "\n",
    "# Preprocess signals\n",
    "ecg_normal_processed = preprocess_ecg(ecg_normal)\n",
    "ecg_tachy_processed = preprocess_ecg(ecg_tachy)\n",
    "\n",
    "# Visualize preprocessing effect\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 8))\n",
    "\n",
    "# Before preprocessing\n",
    "axes[0, 0].plot(t_normal[:720], ecg_normal[:720], 'b-', alpha=0.7)\n",
    "axes[0, 0].set_title('Before Preprocessing (Normal)', fontweight='bold')\n",
    "axes[0, 0].set_ylabel('Amplitude')\n",
    "\n",
    "axes[1, 0].plot(t_tachy[:720], ecg_tachy[:720], 'r-', alpha=0.7)\n",
    "axes[1, 0].set_title('Before Preprocessing (Tachycardia)', fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Time (s)')\n",
    "axes[1, 0].set_ylabel('Amplitude')\n",
    "\n",
    "# After preprocessing\n",
    "axes[0, 1].plot(t_normal[:720], ecg_normal_processed[:720], 'b-', linewidth=1.5)\n",
    "axes[0, 1].set_title('After Preprocessing (Normal)', fontweight='bold')\n",
    "\n",
    "axes[1, 1].plot(t_tachy[:720], ecg_tachy_processed[:720], 'r-', linewidth=1.5)\n",
    "axes[1, 1].set_title('After Preprocessing (Tachycardia)', fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Time (s)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ ECG preprocessing completed\")\n",
    "print(\"Applied: Detrending ‚Üí Band-pass Filtering ‚Üí Normalization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Practice 3: Multimodal Fusion Strategies\n",
    "\n",
    "### üéØ Learning Objectives\n",
    "- Implement Early, Late, and Intermediate Fusion\n",
    "- Compare fusion strategies on synthetic medical data\n",
    "- Understand trade-offs between fusion approaches\n",
    "\n",
    "### üìñ Key Concepts\n",
    "**Early Fusion:** Concatenate features before model\n",
    "\n",
    "**Late Fusion:** Combine predictions from separate models\n",
    "\n",
    "**Intermediate Fusion:** Combine at middle layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1 Generate synthetic multimodal medical data\n",
    "def generate_multimodal_data(n_samples=1000):\n",
    "    \"\"\"\n",
    "    Simulate patient data with multiple modalities\n",
    "    Modality 1: Clinical features (age, BP, lab results)\n",
    "    Modality 2: Image features (from X-ray/CT)\n",
    "    Task: Binary classification (disease present or not)\n",
    "    \"\"\"\n",
    "    # Modality 1: Clinical features (5 features)\n",
    "    clinical_features = np.random.randn(n_samples, 5)\n",
    "    \n",
    "    # Modality 2: Image features (10 features)\n",
    "    image_features = np.random.randn(n_samples, 10)\n",
    "    \n",
    "    # Create labels with correlation to both modalities\n",
    "    clinical_score = clinical_features[:, 0] + clinical_features[:, 1] * 0.5\n",
    "    image_score = image_features[:, 0] + image_features[:, 2] * 0.7\n",
    "    \n",
    "    combined_score = clinical_score + image_score\n",
    "    labels = (combined_score > 0).astype(int)\n",
    "    \n",
    "    return clinical_features, image_features, labels\n",
    "\n",
    "# Generate data\n",
    "clinical_data, image_data, labels = generate_multimodal_data(n_samples=1000)\n",
    "\n",
    "# Split data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "clinical_train, clinical_test, image_train, image_test, y_train, y_test = train_test_split(\n",
    "    clinical_data, image_data, labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(\"üìä Multimodal Dataset Statistics:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Total samples: {len(labels)}\")\n",
    "print(f\"Training samples: {len(y_train)}\")\n",
    "print(f\"Test samples: {len(y_test)}\")\n",
    "print(f\"\\nClinical features shape: {clinical_data.shape}\")\n",
    "print(f\"Image features shape: {image_data.shape}\")\n",
    "print(f\"\\nClass distribution: {np.bincount(labels)}\")\n",
    "print(f\"Positive rate: {labels.mean():.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.2 Implement different fusion strategies\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "\n",
    "def early_fusion(clinical_train, image_train, clinical_test, image_test, y_train, y_test):\n",
    "    \"\"\"Early Fusion: Concatenate features before model\"\"\"\n",
    "    # Concatenate features\n",
    "    X_train = np.concatenate([clinical_train, image_train], axis=1)\n",
    "    X_test = np.concatenate([clinical_test, image_test], axis=1)\n",
    "    \n",
    "    # Train model\n",
    "    model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    auc = roc_auc_score(y_test, y_proba)\n",
    "    \n",
    "    return acc, auc, model\n",
    "\n",
    "def late_fusion(clinical_train, image_train, clinical_test, image_test, y_train, y_test):\n",
    "    \"\"\"Late Fusion: Train separate models and combine predictions\"\"\"\n",
    "    # Train clinical model\n",
    "    clinical_model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "    clinical_model.fit(clinical_train, y_train)\n",
    "    clinical_proba = clinical_model.predict_proba(clinical_test)[:, 1]\n",
    "    \n",
    "    # Train image model\n",
    "    image_model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "    image_model.fit(image_train, y_train)\n",
    "    image_proba = image_model.predict_proba(image_test)[:, 1]\n",
    "    \n",
    "    # Combine predictions (average)\n",
    "    combined_proba = (clinical_proba + image_proba) / 2\n",
    "    y_pred = (combined_proba > 0.5).astype(int)\n",
    "    \n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    auc = roc_auc_score(y_test, combined_proba)\n",
    "    \n",
    "    return acc, auc, (clinical_model, image_model)\n",
    "\n",
    "def unimodal_baseline(features_train, features_test, y_train, y_test, modality_name):\n",
    "    \"\"\"Unimodal baseline: Single modality only\"\"\"\n",
    "    model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "    model.fit(features_train, y_train)\n",
    "    \n",
    "    y_pred = model.predict(features_test)\n",
    "    y_proba = model.predict_proba(features_test)[:, 1]\n",
    "    \n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    auc = roc_auc_score(y_test, y_proba)\n",
    "    \n",
    "    return acc, auc, model\n",
    "\n",
    "# Run all fusion strategies\n",
    "print(\"üîÑ Training models with different fusion strategies...\\n\")\n",
    "\n",
    "# Unimodal baselines\n",
    "clinical_acc, clinical_auc, _ = unimodal_baseline(clinical_train, clinical_test, y_train, y_test, \"Clinical\")\n",
    "image_acc, image_auc, _ = unimodal_baseline(image_train, image_test, y_train, y_test, \"Image\")\n",
    "\n",
    "# Fusion methods\n",
    "early_acc, early_auc, _ = early_fusion(clinical_train, image_train, clinical_test, image_test, y_train, y_test)\n",
    "late_acc, late_auc, _ = late_fusion(clinical_train, image_train, clinical_test, image_test, y_train, y_test)\n",
    "\n",
    "# Display results\n",
    "results_df = pd.DataFrame({\n",
    "    'Method': ['Clinical Only', 'Image Only', 'Early Fusion', 'Late Fusion'],\n",
    "    'Accuracy': [clinical_acc, image_acc, early_acc, late_acc],\n",
    "    'AUC': [clinical_auc, image_auc, early_auc, late_auc]\n",
    "})\n",
    "\n",
    "print(\"üìä Fusion Strategy Comparison:\")\n",
    "print(\"=\" * 60)\n",
    "print(results_df.to_string(index=False))\n",
    "print(\"\\nüí° Multimodal fusion typically outperforms unimodal approaches!\")\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "methods = results_df['Method']\n",
    "colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4']\n",
    "\n",
    "axes[0].bar(methods, results_df['Accuracy'], color=colors, alpha=0.8)\n",
    "axes[0].set_ylabel('Accuracy', fontsize=12)\n",
    "axes[0].set_title('Accuracy by Fusion Method', fontsize=13, fontweight='bold')\n",
    "axes[0].set_ylim([0.5, 1.0])\n",
    "axes[0].tick_params(axis='x', rotation=15)\n",
    "\n",
    "axes[1].bar(methods, results_df['AUC'], color=colors, alpha=0.8)\n",
    "axes[1].set_ylabel('AUC-ROC', fontsize=12)\n",
    "axes[1].set_title('AUC-ROC by Fusion Method', fontsize=13, fontweight='bold')\n",
    "axes[1].set_ylim([0.5, 1.0])\n",
    "axes[1].tick_params(axis='x', rotation=15)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Practice 4: Building a Simple Medical Vision-Language Model\n",
    "\n",
    "### üéØ Learning Objectives\n",
    "- Build simple image and text encoders\n",
    "- Implement projection layers for common embedding space\n",
    "- Train with contrastive loss\n",
    "\n",
    "### üìñ Key Concepts\n",
    "**Vision-Language Model:** Aligns visual and textual representations in a shared space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1 Define simple multimodal model architecture\n",
    "class SimpleMedicalVLM(nn.Module):\n",
    "    \"\"\"\n",
    "    Simplified Medical Vision-Language Model\n",
    "    \"\"\"\n",
    "    def __init__(self, image_dim=512, text_dim=768, projection_dim=256):\n",
    "        super(SimpleMedicalVLM, self).__init__()\n",
    "        \n",
    "        # Image encoder (simplified - normally would be ResNet/ViT)\n",
    "        self.image_encoder = nn.Sequential(\n",
    "            nn.Linear(image_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(512, 256)\n",
    "        )\n",
    "        \n",
    "        # Text encoder (simplified - normally would be BERT)\n",
    "        self.text_encoder = nn.Sequential(\n",
    "            nn.Linear(text_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(512, 256)\n",
    "        )\n",
    "        \n",
    "        # Projection heads\n",
    "        self.image_projection = nn.Linear(256, projection_dim)\n",
    "        self.text_projection = nn.Linear(256, projection_dim)\n",
    "        \n",
    "        # Temperature parameter for contrastive loss\n",
    "        self.temperature = nn.Parameter(torch.ones([]) * np.log(1 / 0.07))\n",
    "    \n",
    "    def forward(self, image_features, text_features):\n",
    "        # Encode\n",
    "        image_encoded = self.image_encoder(image_features)\n",
    "        text_encoded = self.text_encoder(text_features)\n",
    "        \n",
    "        # Project to common space\n",
    "        image_projected = self.image_projection(image_encoded)\n",
    "        text_projected = self.text_projection(text_encoded)\n",
    "        \n",
    "        # Normalize\n",
    "        image_projected = F.normalize(image_projected, dim=-1)\n",
    "        text_projected = F.normalize(text_projected, dim=-1)\n",
    "        \n",
    "        return image_projected, text_projected\n",
    "    \n",
    "    def contrastive_loss(self, image_embeddings, text_embeddings):\n",
    "        \"\"\"\n",
    "        Calculate bidirectional contrastive loss\n",
    "        \"\"\"\n",
    "        # Calculate similarity matrix\n",
    "        logits = torch.matmul(image_embeddings, text_embeddings.T) * self.temperature.exp()\n",
    "        \n",
    "        # Labels are diagonal\n",
    "        batch_size = image_embeddings.shape[0]\n",
    "        labels = torch.arange(batch_size, device=image_embeddings.device)\n",
    "        \n",
    "        # Cross entropy loss (both directions)\n",
    "        loss_i2t = F.cross_entropy(logits, labels)\n",
    "        loss_t2i = F.cross_entropy(logits.T, labels)\n",
    "        \n",
    "        return (loss_i2t + loss_t2i) / 2\n",
    "\n",
    "# Initialize model\n",
    "model = SimpleMedicalVLM(image_dim=512, text_dim=768, projection_dim=256)\n",
    "\n",
    "print(\"‚úÖ Medical VLM Model Architecture:\")\n",
    "print(\"=\" * 60)\n",
    "print(model)\n",
    "print(f\"\\nTotal parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.2 Generate synthetic training data and train\n",
    "def generate_synthetic_vlm_data(n_samples=500, image_dim=512, text_dim=768):\n",
    "    \"\"\"\n",
    "    Generate synthetic image-text pairs for training\n",
    "    \"\"\"\n",
    "    image_features = torch.randn(n_samples, image_dim)\n",
    "    # Add correlation between image and text\n",
    "    text_features = torch.randn(n_samples, text_dim)\n",
    "    # First few dimensions correlated\n",
    "    text_features[:, :image_dim//2] += image_features[:, :image_dim//2] * 0.5\n",
    "    \n",
    "    return image_features, text_features\n",
    "\n",
    "# Generate data\n",
    "train_images, train_texts = generate_synthetic_vlm_data(n_samples=400)\n",
    "val_images, val_texts = generate_synthetic_vlm_data(n_samples=100)\n",
    "\n",
    "# Training setup\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "batch_size = 32\n",
    "n_epochs = 10\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "print(\"üöÄ Training Medical VLM...\\n\")\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    n_batches = len(train_images) // batch_size\n",
    "    \n",
    "    for i in range(n_batches):\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = start_idx + batch_size\n",
    "        \n",
    "        batch_images = train_images[start_idx:end_idx]\n",
    "        batch_texts = train_texts[start_idx:end_idx]\n",
    "        \n",
    "        # Forward pass\n",
    "        image_emb, text_emb = model(batch_images, batch_texts)\n",
    "        loss = model.contrastive_loss(image_emb, text_emb)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    avg_train_loss = epoch_loss / n_batches\n",
    "    train_losses.append(avg_train_loss)\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_image_emb, val_text_emb = model(val_images, val_texts)\n",
    "        val_loss = model.contrastive_loss(val_image_emb, val_text_emb).item()\n",
    "        val_losses.append(val_loss)\n",
    "    \n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{n_epochs}] - Train Loss: {avg_train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "print(\"\\n‚úÖ Training completed!\")\n",
    "\n",
    "# Plot training curve\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses, label='Training Loss', linewidth=2)\n",
    "plt.plot(val_losses, label='Validation Loss', linewidth=2)\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Contrastive Loss', fontsize=12)\n",
    "plt.title('Medical VLM Training Curve', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Practice 5: Attention-based Fusion\n",
    "\n",
    "### üéØ Learning Objectives\n",
    "- Implement cross-attention mechanism\n",
    "- Apply attention for multimodal fusion\n",
    "- Visualize attention weights\n",
    "\n",
    "### üìñ Key Concepts\n",
    "**Cross-Attention:** One modality queries another modality\n",
    "\n",
    "**Attention Formula:** $\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.1 Implement cross-attention mechanism\n",
    "class CrossAttentionFusion(nn.Module):\n",
    "    \"\"\"\n",
    "    Cross-attention module for multimodal fusion\n",
    "    \"\"\"\n",
    "    def __init__(self, dim=128, num_heads=4):\n",
    "        super(CrossAttentionFusion, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.dim = dim\n",
    "        self.head_dim = dim // num_heads\n",
    "        \n",
    "        assert self.head_dim * num_heads == dim, \"dim must be divisible by num_heads\"\n",
    "        \n",
    "        # Query, Key, Value projections\n",
    "        self.query = nn.Linear(dim, dim)\n",
    "        self.key = nn.Linear(dim, dim)\n",
    "        self.value = nn.Linear(dim, dim)\n",
    "        \n",
    "        # Output projection\n",
    "        self.out = nn.Linear(dim, dim)\n",
    "        \n",
    "        self.scale = self.head_dim ** -0.5\n",
    "    \n",
    "    def forward(self, query_input, key_value_input):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            query_input: (batch, seq_len_q, dim) - e.g., image features\n",
    "            key_value_input: (batch, seq_len_kv, dim) - e.g., text features\n",
    "        \"\"\"\n",
    "        batch_size = query_input.shape[0]\n",
    "        \n",
    "        # Linear projections and reshape for multi-head attention\n",
    "        Q = self.query(query_input).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        K = self.key(key_value_input).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        V = self.value(key_value_input).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        # Attention scores\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) * self.scale  # (batch, heads, seq_q, seq_kv)\n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        \n",
    "        # Apply attention to values\n",
    "        attended = torch.matmul(attention_weights, V)  # (batch, heads, seq_q, head_dim)\n",
    "        \n",
    "        # Concatenate heads\n",
    "        attended = attended.transpose(1, 2).contiguous().view(batch_size, -1, self.dim)\n",
    "        \n",
    "        # Output projection\n",
    "        output = self.out(attended)\n",
    "        \n",
    "        return output, attention_weights\n",
    "\n",
    "# Test cross-attention\n",
    "cross_attn = CrossAttentionFusion(dim=128, num_heads=4)\n",
    "\n",
    "# Create sample inputs\n",
    "sample_image_features = torch.randn(8, 10, 128)  # batch=8, 10 image patches, dim=128\n",
    "sample_text_features = torch.randn(8, 20, 128)   # batch=8, 20 text tokens, dim=128\n",
    "\n",
    "# Forward pass\n",
    "output, attn_weights = cross_attn(sample_image_features, sample_text_features)\n",
    "\n",
    "print(\"‚úÖ Cross-Attention Module:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Image features shape: {sample_image_features.shape}\")\n",
    "print(f\"Text features shape: {sample_text_features.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Attention weights shape: {attn_weights.shape}\")\n",
    "print(f\"  (batch, num_heads, seq_query, seq_key_value)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.2 Visualize attention weights\n",
    "def visualize_attention(attention_weights, sample_idx=0, head_idx=0):\n",
    "    \"\"\"\n",
    "    Visualize attention weights for interpretation\n",
    "    \"\"\"\n",
    "    # Get attention for specific sample and head\n",
    "    attn = attention_weights[sample_idx, head_idx].detach().numpy()\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.heatmap(attn, cmap='YlOrRd', cbar=True, \n",
    "                xticklabels=[f'T{i}' for i in range(attn.shape[1])],\n",
    "                yticklabels=[f'I{i}' for i in range(attn.shape[0])],\n",
    "                linewidths=0.5, linecolor='gray')\n",
    "    plt.title(f'Cross-Attention Weights (Sample {sample_idx}, Head {head_idx})', \n",
    "              fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Text Token Index', fontsize=12)\n",
    "    plt.ylabel('Image Patch Index', fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Statistics\n",
    "    print(f\"\\nüìä Attention Statistics:\")\n",
    "    print(f\"Min attention: {attn.min():.4f}\")\n",
    "    print(f\"Max attention: {attn.max():.4f}\")\n",
    "    print(f\"Mean attention: {attn.mean():.4f}\")\n",
    "    print(f\"\\nüí° Higher values indicate stronger alignment between image patch and text token\")\n",
    "\n",
    "visualize_attention(attn_weights, sample_idx=0, head_idx=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üéØ Practice Complete!\n",
    "\n",
    "### Summary of What We Learned:\n",
    "\n",
    "1. **Image-Text Alignment**: Implemented contrastive learning for medical image-report pairs\n",
    "2. **ECG Signal Processing**: Generated and preprocessed ECG signals for arrhythmia detection\n",
    "3. **Multimodal Fusion**: Compared Early, Late, and Intermediate fusion strategies\n",
    "4. **Medical VLM**: Built a simplified vision-language model from scratch\n",
    "5. **Attention Mechanisms**: Implemented cross-attention for multimodal fusion\n",
    "\n",
    "### Key Insights:\n",
    "- Multimodal fusion typically outperforms unimodal approaches\n",
    "- Different fusion strategies have different trade-offs\n",
    "- Contrastive learning aligns different modalities in a shared space\n",
    "- Attention mechanisms allow dynamic weighting of information\n",
    "\n",
    "### Next Steps:\n",
    "- Implement more complex architectures (Transformers, Graph Neural Networks)\n",
    "- Work with real medical datasets (MIMIC-CXR, PhysioNet)\n",
    "- Add missing modality handling\n",
    "- Explore interpretability techniques (Grad-CAM, SHAP)\n",
    "- Deploy models for clinical decision support\n",
    "\n",
    "### üìö Recommended Resources:\n",
    "- **Papers**: \"CLIP\" (Radford et al.), \"BiomedCLIP\", \"MedCLIP\"\n",
    "- **Datasets**: MIMIC-CXR, PhysioNet, CheXpert\n",
    "- **Libraries**: Hugging Face Transformers, PyTorch, timm\n",
    "\n",
    "---\n",
    "\n",
    "## üôè Thank you for completing this practice!\n",
    "\n",
    "**Instructor**: Ho-min Park  \n",
    "**Email**: homin.park@ghent.ac.kr | powersimmani@gmail.com"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
