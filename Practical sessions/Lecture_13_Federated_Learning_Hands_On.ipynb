{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ü§ù Federated Learning: Hands-On Practice\n",
    "\n",
    "## Table of Contents\n",
    "1. [Setting Up Federated Learning Environment](#practice-1-setting-up-federated-learning-environment)\n",
    "2. [Simulating Multiple Hospital Clients](#practice-2-simulating-multiple-hospital-clients)\n",
    "3. [Implementing FedAvg Algorithm](#practice-3-implementing-fedavg-algorithm)\n",
    "4. [Adding Differential Privacy](#practice-4-adding-differential-privacy)\n",
    "5. [Handling Non-IID Medical Data](#practice-5-handling-non-iid-medical-data)\n",
    "6. [Secure Aggregation Basics](#practice-6-secure-aggregation-basics)\n",
    "7. [Performance Comparison: Centralized vs Federated](#practice-7-performance-comparison-centralized-vs-federated)\n",
    "8. [Communication Efficiency Optimization](#practice-8-communication-efficiency-optimization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing and Importing Essential Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Flower framework (uncomment if needed)\n",
    "# !pip install flwr\n",
    "\n",
    "# Import essential libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.datasets import make_classification\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Visualization settings\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "print(\"‚úÖ All libraries loaded successfully!\")\n",
    "print(\"üè• Ready for Federated Learning Practice\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Practice 1: Setting Up Federated Learning Environment\n",
    "\n",
    "### üéØ Learning Objectives\n",
    "- Understand the basic architecture of federated learning\n",
    "- Create a simple client-server simulation\n",
    "- Learn how data stays distributed\n",
    "\n",
    "### üìñ Key Concepts\n",
    "**Federated Learning**: Training ML models across multiple decentralized devices/servers holding local data samples, without exchanging the data itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.1 Create a simple FL environment\n",
    "class FederatedLearningEnvironment:\n",
    "    \"\"\"Simulates a basic federated learning setup\"\"\"\n",
    "    \n",
    "    def __init__(self, n_clients=3):\n",
    "        self.n_clients = n_clients\n",
    "        self.global_model = None\n",
    "        self.client_models = [None] * n_clients\n",
    "        \n",
    "        print(f\"üåê Federated Learning Environment Created\")\n",
    "        print(f\"   Number of clients (hospitals): {n_clients}\")\n",
    "        print(f\"   Central server: Ready\")\n",
    "        print(f\"   Data sharing: Disabled (Privacy preserved!) üîí\")\n",
    "    \n",
    "    def initialize_global_model(self):\n",
    "        \"\"\"Initialize the global model\"\"\"\n",
    "        self.global_model = LogisticRegression(max_iter=100, random_state=42)\n",
    "        print(\"\\n‚úÖ Global model initialized on central server\")\n",
    "    \n",
    "    def distribute_model(self):\n",
    "        \"\"\"Send global model to all clients\"\"\"\n",
    "        print(\"\\nüì§ Distributing global model to all clients...\")\n",
    "        for i in range(self.n_clients):\n",
    "            # In real FL, we'd send model parameters\n",
    "            self.client_models[i] = \"Model distributed\"\n",
    "            print(f\"   ‚úì Client {i+1} (Hospital {i+1}): Received model\")\n",
    "        print(\"‚úÖ Model distribution complete!\")\n",
    "    \n",
    "    def show_architecture(self):\n",
    "        \"\"\"Visualize FL architecture\"\"\"\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"        FEDERATED LEARNING ARCHITECTURE\")\n",
    "        print(\"=\"*50)\n",
    "        print(\"\\n            ‚òÅÔ∏è  Central Server\")\n",
    "        print(\"                    |\")\n",
    "        print(\"          +---------+---------+\")\n",
    "        print(\"          |         |         |\")\n",
    "        print(\"         üè•        üè•        üè•\")\n",
    "        print(\"      Hospital1  Hospital2  Hospital3\")\n",
    "        print(\"        üíæ         üíæ         üíæ\")\n",
    "        print(\"     Local Data Local Data Local Data\")\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "# Create environment\n",
    "fl_env = FederatedLearningEnvironment(n_clients=3)\n",
    "fl_env.show_architecture()\n",
    "fl_env.initialize_global_model()\n",
    "fl_env.distribute_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Practice 2: Simulating Multiple Hospital Clients\n",
    "\n",
    "### üéØ Learning Objectives\n",
    "- Create synthetic medical datasets for different hospitals\n",
    "- Simulate data heterogeneity (Non-IID data)\n",
    "- Understand why hospital data differs\n",
    "\n",
    "### üìñ Key Concepts\n",
    "**Non-IID Data**: Different hospitals have different patient demographics, disease prevalence, and protocols."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1 Generate heterogeneous data for hospitals\n",
    "def create_hospital_data(n_hospitals=3, samples_per_hospital=200):\n",
    "    \"\"\"\n",
    "    Create synthetic medical data with different distributions\n",
    "    for each hospital (simulating Non-IID data)\n",
    "    \"\"\"\n",
    "    hospital_datasets = []\n",
    "    \n",
    "    print(\"üè• Generating data for each hospital...\\n\")\n",
    "    \n",
    "    for i in range(n_hospitals):\n",
    "        # Each hospital has slightly different data distribution\n",
    "        # This simulates regional differences, demographics, etc.\n",
    "        X, y = make_classification(\n",
    "            n_samples=samples_per_hospital,\n",
    "            n_features=10,\n",
    "            n_informative=7,\n",
    "            n_redundant=2,\n",
    "            n_classes=2,\n",
    "            flip_y=0.1 + i*0.05,  # Different noise levels\n",
    "            class_sep=1.5 - i*0.2,  # Different separability\n",
    "            random_state=42 + i*10\n",
    "        )\n",
    "        \n",
    "        # Store data\n",
    "        hospital_datasets.append((X, y))\n",
    "        \n",
    "        # Statistics\n",
    "        class_0 = np.sum(y == 0)\n",
    "        class_1 = np.sum(y == 1)\n",
    "        \n",
    "        print(f\"Hospital {i+1}:\")\n",
    "        print(f\"  Total patients: {len(y)}\")\n",
    "        print(f\"  Class 0 (Healthy): {class_0} ({class_0/len(y)*100:.1f}%)\")\n",
    "        print(f\"  Class 1 (Disease): {class_1} ({class_1/len(y)*100:.1f}%)\")\n",
    "        print(f\"  Data distribution: {'Imbalanced' if abs(class_0-class_1) > 30 else 'Balanced'}\")\n",
    "        print()\n",
    "    \n",
    "    return hospital_datasets\n",
    "\n",
    "# Generate data\n",
    "hospital_data = create_hospital_data(n_hospitals=3, samples_per_hospital=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2 Visualize data heterogeneity\n",
    "def visualize_data_heterogeneity(hospital_data):\n",
    "    \"\"\"Visualize how data differs across hospitals\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "    \n",
    "    for i, (X, y) in enumerate(hospital_data):\n",
    "        # Use first two features for visualization\n",
    "        axes[i].scatter(X[y==0][:, 0], X[y==0][:, 1], \n",
    "                       alpha=0.6, label='Healthy', c='blue', s=30)\n",
    "        axes[i].scatter(X[y==1][:, 0], X[y==1][:, 1], \n",
    "                       alpha=0.6, label='Disease', c='red', s=30)\n",
    "        axes[i].set_title(f'Hospital {i+1} Data Distribution', fontsize=12, fontweight='bold')\n",
    "        axes[i].set_xlabel('Feature 1')\n",
    "        axes[i].set_ylabel('Feature 2')\n",
    "        axes[i].legend()\n",
    "        axes[i].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"üìä Notice how each hospital has different data patterns!\")\n",
    "    print(\"   This is called Non-IID (Non-Independently and Identically Distributed) data.\")\n",
    "\n",
    "visualize_data_heterogeneity(hospital_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Practice 3: Implementing FedAvg Algorithm\n",
    "\n",
    "### üéØ Learning Objectives\n",
    "- Implement the core FedAvg (Federated Averaging) algorithm\n",
    "- Understand weighted aggregation\n",
    "- Compare with centralized training\n",
    "\n",
    "### üìñ Key Concepts\n",
    "**FedAvg Formula**: $w_{global} = \\sum_{i=1}^{n} \\frac{n_i}{N} \\times w_i$\n",
    "\n",
    "where $n_i$ is the number of samples at client $i$, and $N$ is the total number of samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1 Implement FedAvg step by step\n",
    "class FedAvgTrainer:\n",
    "    \"\"\"Federated Averaging implementation\"\"\"\n",
    "    \n",
    "    def __init__(self, hospital_data):\n",
    "        self.hospital_data = hospital_data\n",
    "        self.n_hospitals = len(hospital_data)\n",
    "        self.global_weights = None\n",
    "        self.global_bias = None\n",
    "        \n",
    "        # Calculate dataset sizes\n",
    "        self.dataset_sizes = [len(y) for _, y in hospital_data]\n",
    "        self.total_samples = sum(self.dataset_sizes)\n",
    "        \n",
    "        print(\"üéØ FedAvg Trainer Initialized\")\n",
    "        print(f\"   Total hospitals: {self.n_hospitals}\")\n",
    "        print(f\"   Total samples: {self.total_samples}\")\n",
    "        print(f\"   Samples per hospital: {self.dataset_sizes}\")\n",
    "    \n",
    "    def local_training(self, hospital_id):\n",
    "        \"\"\"Train model locally at a hospital\"\"\"\n",
    "        X, y = self.hospital_data[hospital_id]\n",
    "        \n",
    "        # Local model\n",
    "        model = LogisticRegression(max_iter=50, random_state=42)\n",
    "        model.fit(X, y)\n",
    "        \n",
    "        # Get trained weights\n",
    "        weights = model.coef_[0]\n",
    "        bias = model.intercept_[0]\n",
    "        \n",
    "        return weights, bias\n",
    "    \n",
    "    def aggregate_models(self, all_weights, all_biases):\n",
    "        \"\"\"\n",
    "        Aggregate models using weighted averaging (FedAvg)\n",
    "        \"\"\"\n",
    "        print(\"\\nüìä Aggregating models using FedAvg...\")\n",
    "        print(\"   Formula: w_global = Œ£(n_i/N √ó w_i)\\n\")\n",
    "        \n",
    "        # Initialize\n",
    "        aggregated_weights = np.zeros_like(all_weights[0])\n",
    "        aggregated_bias = 0.0\n",
    "        \n",
    "        # Weighted averaging\n",
    "        for i in range(self.n_hospitals):\n",
    "            weight_factor = self.dataset_sizes[i] / self.total_samples\n",
    "            aggregated_weights += weight_factor * all_weights[i]\n",
    "            aggregated_bias += weight_factor * all_biases[i]\n",
    "            \n",
    "            print(f\"   Hospital {i+1}: weight = {weight_factor:.3f} \"\n",
    "                  f\"(samples: {self.dataset_sizes[i]}/{self.total_samples})\")\n",
    "        \n",
    "        print(\"\\n‚úÖ Aggregation complete!\")\n",
    "        return aggregated_weights, aggregated_bias\n",
    "    \n",
    "    def federated_round(self, round_num):\n",
    "        \"\"\"Execute one round of federated training\"\"\"\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Round {round_num}: Federated Training\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # Step 1: Local training at each hospital\n",
    "        print(\"\\nüìç Step 1: Local Training at Each Hospital\")\n",
    "        all_weights = []\n",
    "        all_biases = []\n",
    "        \n",
    "        for i in range(self.n_hospitals):\n",
    "            print(f\"   Training at Hospital {i+1}... \", end=\"\")\n",
    "            weights, bias = self.local_training(i)\n",
    "            all_weights.append(weights)\n",
    "            all_biases.append(bias)\n",
    "            print(\"‚úì Complete\")\n",
    "        \n",
    "        # Step 2: Send updates to server\n",
    "        print(\"\\nüì§ Step 2: Sending Model Updates to Server\")\n",
    "        print(\"   (Only model parameters sent, not raw data!)\")\n",
    "        \n",
    "        # Step 3: Aggregate on server\n",
    "        print(\"\\nüîÑ Step 3: Server Aggregation (FedAvg)\")\n",
    "        self.global_weights, self.global_bias = self.aggregate_models(\n",
    "            all_weights, all_biases\n",
    "        )\n",
    "        \n",
    "        # Step 4: Distribute back to clients\n",
    "        print(\"\\nüì• Step 4: Distributing Global Model to All Hospitals\")\n",
    "        print(\"   ‚úì Model parameters updated at all hospitals\")\n",
    "        \n",
    "        return self.global_weights, self.global_bias\n",
    "\n",
    "# Create trainer and run one round\n",
    "fedavg_trainer = FedAvgTrainer(hospital_data)\n",
    "global_w, global_b = fedavg_trainer.federated_round(round_num=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Practice 4: Adding Differential Privacy\n",
    "\n",
    "### üéØ Learning Objectives\n",
    "- Understand differential privacy mechanisms\n",
    "- Add Gaussian noise to model updates\n",
    "- Balance privacy and accuracy\n",
    "\n",
    "### üìñ Key Concepts\n",
    "**Differential Privacy**: Adding calibrated noise to protect individual data points.\n",
    "\n",
    "**Formula**: $\\tilde{w} = w + \\mathcal{N}(0, \\sigma^2)$ where $\\sigma$ is determined by privacy budget $\\epsilon$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1 Implement differential privacy\n",
    "def add_differential_privacy(weights, bias, epsilon=1.0, sensitivity=1.0):\n",
    "    \"\"\"\n",
    "    Add Gaussian noise for differential privacy\n",
    "    \n",
    "    Parameters:\n",
    "    - epsilon: Privacy budget (smaller = more privacy, less accuracy)\n",
    "    - sensitivity: How much one data point can change the output\n",
    "    \"\"\"\n",
    "    print(f\"\\nüîí Adding Differential Privacy\")\n",
    "    print(f\"   Privacy budget (Œµ): {epsilon}\")\n",
    "    print(f\"   Sensitivity (Œîf): {sensitivity}\")\n",
    "    \n",
    "    # Calculate noise scale\n",
    "    sigma = (sensitivity * np.sqrt(2 * np.log(1.25))) / epsilon\n",
    "    print(f\"   Noise scale (œÉ): {sigma:.4f}\")\n",
    "    \n",
    "    # Add Gaussian noise\n",
    "    noisy_weights = weights + np.random.normal(0, sigma, size=weights.shape)\n",
    "    noisy_bias = bias + np.random.normal(0, sigma)\n",
    "    \n",
    "    # Calculate noise magnitude\n",
    "    noise_magnitude = np.linalg.norm(noisy_weights - weights)\n",
    "    print(f\"   Noise magnitude added: {noise_magnitude:.4f}\")\n",
    "    \n",
    "    print(\"\\n‚úÖ Differential privacy applied!\")\n",
    "    print(\"   üé≠ Individual data points are now protected\")\n",
    "    \n",
    "    return noisy_weights, noisy_bias\n",
    "\n",
    "# Test with different privacy levels\n",
    "print(\"Comparing Different Privacy Levels:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# High privacy (low epsilon)\n",
    "print(\"\\n1Ô∏è‚É£ High Privacy (Œµ=0.1):\")\n",
    "private_w1, private_b1 = add_differential_privacy(global_w, global_b, epsilon=0.1)\n",
    "\n",
    "# Medium privacy\n",
    "print(\"\\n2Ô∏è‚É£ Medium Privacy (Œµ=1.0):\")\n",
    "private_w2, private_b2 = add_differential_privacy(global_w, global_b, epsilon=1.0)\n",
    "\n",
    "# Low privacy (high epsilon)\n",
    "print(\"\\n3Ô∏è‚É£ Low Privacy (Œµ=10.0):\")\n",
    "private_w3, private_b3 = add_differential_privacy(global_w, global_b, epsilon=10.0)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Key Insight: Lower Œµ = More Privacy = More Noise = Lower Accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Practice 5: Handling Non-IID Medical Data\n",
    "\n",
    "### üéØ Learning Objectives\n",
    "- Understand challenges of Non-IID data\n",
    "- Implement FedProx for better handling\n",
    "- Compare FedAvg vs FedProx\n",
    "\n",
    "### üìñ Key Concepts\n",
    "**FedProx**: Adds a proximal term to handle heterogeneity\n",
    "\n",
    "**Formula**: $\\min F(w) + \\frac{\\mu}{2}||w - w_t||^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.1 Visualize Non-IID challenges\n",
    "def analyze_data_heterogeneity(hospital_data):\n",
    "    \"\"\"Analyze how different the hospital datasets are\"\"\"\n",
    "    \n",
    "    print(\"üìä Analyzing Data Heterogeneity\\n\")\n",
    "    \n",
    "    # Calculate statistics for each hospital\n",
    "    stats = []\n",
    "    for i, (X, y) in enumerate(hospital_data):\n",
    "        mean_features = X.mean(axis=0)\n",
    "        std_features = X.std(axis=0)\n",
    "        class_ratio = y.sum() / len(y)\n",
    "        \n",
    "        stats.append({\n",
    "            'hospital': i+1,\n",
    "            'mean': mean_features.mean(),\n",
    "            'std': std_features.mean(),\n",
    "            'positive_ratio': class_ratio\n",
    "        })\n",
    "    \n",
    "    # Display statistics\n",
    "    df_stats = pd.DataFrame(stats)\n",
    "    print(df_stats.to_string(index=False))\n",
    "    \n",
    "    # Calculate heterogeneity score\n",
    "    mean_variance = df_stats['mean'].var()\n",
    "    ratio_variance = df_stats['positive_ratio'].var()\n",
    "    \n",
    "    print(f\"\\nüìà Heterogeneity Metrics:\")\n",
    "    print(f\"   Mean variance across hospitals: {mean_variance:.4f}\")\n",
    "    print(f\"   Class ratio variance: {ratio_variance:.4f}\")\n",
    "    \n",
    "    if ratio_variance > 0.01:\n",
    "        print(\"\\n‚ö†Ô∏è  High heterogeneity detected!\")\n",
    "        print(\"   Recommendation: Use FedProx instead of FedAvg\")\n",
    "    else:\n",
    "        print(\"\\n‚úÖ Low heterogeneity - FedAvg should work well\")\n",
    "\n",
    "analyze_data_heterogeneity(hospital_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Practice 6: Secure Aggregation Basics\n",
    "\n",
    "### üéØ Learning Objectives\n",
    "- Understand secure aggregation concept\n",
    "- Implement simple masking mechanism\n",
    "- Verify that individual updates remain hidden\n",
    "\n",
    "### üìñ Key Concepts\n",
    "**Secure Aggregation**: Server learns only the aggregate, not individual updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.1 Simple secure aggregation simulation\n",
    "def secure_aggregation_demo():\n",
    "    \"\"\"\n",
    "    Demonstrate secure aggregation where individual updates are masked\n",
    "    \"\"\"\n",
    "    print(\"üõ°Ô∏è Secure Aggregation Demonstration\\n\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Simulate client updates\n",
    "    n_clients = 3\n",
    "    update_dimension = 5\n",
    "    \n",
    "    client_updates = []\n",
    "    client_masks = []\n",
    "    \n",
    "    print(\"Step 1: Clients Create Updates and Masks\\n\")\n",
    "    \n",
    "    for i in range(n_clients):\n",
    "        # Real update\n",
    "        update = np.random.randn(update_dimension)\n",
    "        # Random mask\n",
    "        mask = np.random.randn(update_dimension)\n",
    "        \n",
    "        client_updates.append(update)\n",
    "        client_masks.append(mask)\n",
    "        \n",
    "        print(f\"Client {i+1}:\")\n",
    "        print(f\"  Update: {update[:3]}... (showing first 3 values)\")\n",
    "        print(f\"  Mask:   {mask[:3]}...\")\n",
    "        print()\n",
    "    \n",
    "    print(\"\\nStep 2: Clients Send Masked Updates to Server\\n\")\n",
    "    \n",
    "    masked_updates = []\n",
    "    for i in range(n_clients):\n",
    "        # Add mask to update\n",
    "        masked = client_updates[i] + client_masks[i]\n",
    "        masked_updates.append(masked)\n",
    "        print(f\"Client {i+1} ‚Üí Server: {masked[:3]}... (masked, server can't see real update!)\")\n",
    "    \n",
    "    print(\"\\nüîí Server only sees masked values!\\n\")\n",
    "    \n",
    "    print(\"Step 3: Server Aggregates Masked Updates\\n\")\n",
    "    \n",
    "    # Aggregate masked updates\n",
    "    sum_masked = sum(masked_updates)\n",
    "    print(f\"Sum of masked updates: {sum_masked[:3]}...\")\n",
    "    \n",
    "    # Calculate sum of masks (which cancels out in sum)\n",
    "    sum_masks = sum(client_masks)\n",
    "    print(f\"Sum of masks: {sum_masks[:3]}...\")\n",
    "    \n",
    "    print(\"\\nStep 4: Masks Cancel Out in Aggregation!\\n\")\n",
    "    \n",
    "    # Remove masks\n",
    "    final_aggregate = sum_masked - sum_masks\n",
    "    \n",
    "    # True aggregate (without masking)\n",
    "    true_aggregate = sum(client_updates)\n",
    "    \n",
    "    print(f\"Aggregate after mask removal: {final_aggregate[:3]}...\")\n",
    "    print(f\"True aggregate (for verification): {true_aggregate[:3]}...\")\n",
    "    print(f\"\\n‚úÖ Results match: {np.allclose(final_aggregate, true_aggregate)}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üéâ Key Achievement: Server got correct aggregate WITHOUT seeing\")\n",
    "    print(\"   individual client updates! Privacy preserved!\")\n",
    "\n",
    "secure_aggregation_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Practice 7: Performance Comparison - Centralized vs Federated\n",
    "\n",
    "### üéØ Learning Objectives\n",
    "- Compare centralized training with federated learning\n",
    "- Measure accuracy retention\n",
    "- Understand the privacy-performance tradeoff\n",
    "\n",
    "### üìñ Key Concepts\n",
    "**Accuracy Retention**: FL typically achieves 90-95% of centralized performance while preserving privacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.1 Centralized training (baseline)\n",
    "def train_centralized(hospital_data):\n",
    "    \"\"\"\n",
    "    Train on all data centrally (what we DON'T want due to privacy)\n",
    "    \"\"\"\n",
    "    print(\"üè¢ Centralized Training (Baseline)\\n\")\n",
    "    \n",
    "    # Combine all data\n",
    "    X_all = np.vstack([X for X, _ in hospital_data])\n",
    "    y_all = np.hstack([y for _, y in hospital_data])\n",
    "    \n",
    "    print(f\"Total samples: {len(y_all)}\")\n",
    "    print(\"‚ö†Ô∏è  Privacy concern: All data is centralized!\\n\")\n",
    "    \n",
    "    # Split for evaluation\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_all, y_all, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    # Train\n",
    "    model = LogisticRegression(max_iter=100, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"‚úÖ Centralized Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    return accuracy, X_test, y_test\n",
    "\n",
    "# 7.2 Federated training\n",
    "def train_federated(hospital_data, n_rounds=3):\n",
    "    \"\"\"\n",
    "    Train using federated learning\n",
    "    \"\"\"\n",
    "    print(\"\\nü§ù Federated Training\\n\")\n",
    "    \n",
    "    trainer = FedAvgTrainer(hospital_data)\n",
    "    \n",
    "    # Prepare test set\n",
    "    X_all = np.vstack([X for X, _ in hospital_data])\n",
    "    y_all = np.hstack([y for _, y in hospital_data])\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_all, y_all, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    # Run federated rounds\n",
    "    for round_num in range(1, n_rounds + 1):\n",
    "        global_w, global_b = trainer.federated_round(round_num)\n",
    "        \n",
    "        # Evaluate global model\n",
    "        model = LogisticRegression(max_iter=1)\n",
    "        model.coef_ = global_w.reshape(1, -1)\n",
    "        model.intercept_ = np.array([global_b])\n",
    "        model.classes_ = np.array([0, 1])\n",
    "        \n",
    "        y_pred = model.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        \n",
    "        print(f\"\\nüìä Round {round_num} Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    print(\"\\n‚úÖ Federated training complete!\")\n",
    "    print(\"üîí Privacy preserved: Data never left hospitals!\")\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "# Run comparison\n",
    "print(\"=\"*70)\n",
    "print(\"        CENTRALIZED vs FEDERATED LEARNING COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "centralized_acc, X_test, y_test = train_centralized(hospital_data)\n",
    "federated_acc = train_federated(hospital_data, n_rounds=3)\n",
    "\n",
    "# Final comparison\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìä FINAL RESULTS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Centralized Learning: {centralized_acc:.4f} (100.0%)\")\n",
    "print(f\"Federated Learning:   {federated_acc:.4f} ({federated_acc/centralized_acc*100:.1f}% retention)\")\n",
    "print(f\"\\nAccuracy difference: {abs(centralized_acc - federated_acc):.4f}\")\n",
    "print(f\"\\nüéâ Achievement: {federated_acc/centralized_acc*100:.1f}% of centralized performance\")\n",
    "print(\"   while keeping all data private!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Practice 8: Communication Efficiency Optimization\n",
    "\n",
    "### üéØ Learning Objectives\n",
    "- Understand communication costs in federated learning\n",
    "- Implement gradient compression\n",
    "- Optimize local training epochs\n",
    "\n",
    "### üìñ Key Concepts\n",
    "**Communication Cost**: Sending model updates is expensive (1-10 GB per round)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.1 Analyze communication costs\n",
    "def analyze_communication_cost():\n",
    "    \"\"\"\n",
    "    Calculate and visualize communication costs\n",
    "    \"\"\"\n",
    "    print(\"üì° Communication Cost Analysis\\n\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Model parameters\n",
    "    n_features = 10\n",
    "    model_params = n_features + 1  # weights + bias\n",
    "    bytes_per_param = 4  # float32\n",
    "    \n",
    "    # Scenario parameters\n",
    "    n_clients = 3\n",
    "    n_rounds = 10\n",
    "    \n",
    "    print(f\"Model parameters: {model_params}\")\n",
    "    print(f\"Bytes per parameter: {bytes_per_param}\")\n",
    "    print(f\"Clients: {n_clients}\")\n",
    "    print(f\"Training rounds: {n_rounds}\\n\")\n",
    "    \n",
    "    # Calculate costs\n",
    "    size_per_update = model_params * bytes_per_param / 1024  # KB\n",
    "    upload_per_round = size_per_update * n_clients  # KB\n",
    "    download_per_round = size_per_update * n_clients  # KB\n",
    "    total_per_round = upload_per_round + download_per_round  # KB\n",
    "    total_training = total_per_round * n_rounds  # KB\n",
    "    \n",
    "    print(\"üíæ Communication Costs:\")\n",
    "    print(f\"   Size per model update: {size_per_update:.2f} KB\")\n",
    "    print(f\"   Upload per round (all clients): {upload_per_round:.2f} KB\")\n",
    "    print(f\"   Download per round (all clients): {download_per_round:.2f} KB\")\n",
    "    print(f\"   Total per round: {total_per_round:.2f} KB\")\n",
    "    print(f\"   Total for training: {total_training:.2f} KB = {total_training/1024:.2f} MB\")\n",
    "    \n",
    "    # Optimization strategies\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üí° Communication Efficiency Strategies:\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(\"\\n1Ô∏è‚É£ Gradient Compression (10-100x reduction)\")\n",
    "    compressed_size = size_per_update * 0.1  # 10x compression\n",
    "    print(f\"   Compressed update size: {compressed_size:.2f} KB\")\n",
    "    print(f\"   Savings: {(1 - 0.1)*100:.0f}%\")\n",
    "    \n",
    "    print(\"\\n2Ô∏è‚É£ More Local Epochs (Reduce communication rounds)\")\n",
    "    print(\"   E=1 (1 local epoch): 10 rounds = 10 communications\")\n",
    "    print(\"   E=5 (5 local epochs): 2 rounds = 2 communications\")\n",
    "    print(f\"   Savings: {(1 - 2/10)*100:.0f}%\")\n",
    "    \n",
    "    print(\"\\n3Ô∏è‚É£ Model Compression (Pruning/Distillation)\")\n",
    "    print(\"   Original model: 10 parameters\")\n",
    "    print(\"   Compressed model: 5 parameters (50% pruning)\")\n",
    "    print(f\"   Savings: 50%\")\n",
    "    \n",
    "    # Visualize\n",
    "    strategies = ['Original', 'Gradient\\nCompression', 'Fewer Rounds\\n(Local Epochs)', 'Model\\nPruning']\n",
    "    costs = [total_training, total_training*0.1, total_training*0.2, total_training*0.5]\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    bars = plt.bar(strategies, costs, color=['red', 'orange', 'green', 'blue'], alpha=0.7)\n",
    "    plt.ylabel('Total Communication (KB)', fontsize=12)\n",
    "    plt.title('Communication Cost: Original vs Optimized Strategies', \n",
    "              fontsize=14, fontweight='bold')\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, (bar, cost) in enumerate(zip(bars, costs)):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,\n",
    "                f'{cost:.1f} KB', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n‚úÖ By combining strategies, we can reduce communication by 90%+!\")\n",
    "\n",
    "analyze_communication_cost()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üéØ Practice Complete!\n",
    "\n",
    "### Summary of What We Learned:\n",
    "\n",
    "1. **FL Architecture**: Client-server setup with distributed data\n",
    "2. **Non-IID Data**: Different hospitals have different data distributions\n",
    "3. **FedAvg Algorithm**: Weighted averaging - $(X^T X)^{-1} X^T y$\n",
    "4. **Differential Privacy**: Adding noise to protect individual data\n",
    "5. **Secure Aggregation**: Server sees only aggregates, not individual updates\n",
    "6. **Performance**: FL achieves 90-95% of centralized accuracy\n",
    "7. **Communication Efficiency**: Compression and local epochs reduce costs by 90%+\n",
    "\n",
    "### Key Insights:\n",
    "\n",
    "‚úÖ **Privacy Preserved**: Data never leaves hospitals  \n",
    "‚úÖ **Good Performance**: 90-95% accuracy retention  \n",
    "‚úÖ **Practical**: Real deployments in COVID-19 research (20+ countries, 100+ hospitals)  \n",
    "‚úÖ **Compliant**: GDPR/HIPAA compatible with proper design\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. Try with real medical datasets (MIMIC-III, eICU)\n",
    "2. Implement FedProx for better heterogeneity handling\n",
    "3. Explore production frameworks (Flower, PySyft, NVIDIA FLARE)\n",
    "4. Read key papers: FedAvg (McMahan et al., 2017), FedProx (Li et al., 2020)\n",
    "\n",
    "### üìö Resources:\n",
    "\n",
    "- **Flower Framework**: https://flower.dev/\n",
    "- **PySyft**: https://github.com/OpenMined/PySyft\n",
    "- **NVIDIA FLARE**: https://nvidia.github.io/NVFlare/\n",
    "\n",
    "---\n",
    "\n",
    "## üéâ Congratulations!\n",
    "\n",
    "You've completed the Federated Learning hands-on practice!\n",
    "\n",
    "You now understand:\n",
    "- How federated learning works\n",
    "- How to implement FedAvg\n",
    "- How to add privacy protection\n",
    "- How to optimize communication\n",
    "- The tradeoffs between privacy and performance\n",
    "\n",
    "**Keep learning and building privacy-preserving AI! üöÄ**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
