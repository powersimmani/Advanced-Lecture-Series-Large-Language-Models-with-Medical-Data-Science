{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéØ RLHF in Healthcare: Hands-On Practice\n",
    "\n",
    "## Table of Contents\n",
    "1. [Preference Data Creation and Analysis](#practice-1-preference-data-creation-and-analysis)\n",
    "2. [Bradley-Terry Reward Model Training](#practice-2-bradley-terry-reward-model-training)\n",
    "3. [Simple Reward Model Implementation](#practice-3-simple-reward-model-implementation)\n",
    "4. [KL Divergence Calculation](#practice-4-kl-divergence-calculation)\n",
    "5. [Safety Constraint Implementation](#practice-5-safety-constraint-implementation)\n",
    "6. [Performance Monitoring Dashboard](#practice-6-performance-monitoring-dashboard)\n",
    "7. [A/B Testing Simulation](#practice-7-ab-testing-simulation)\n",
    "8. [Complete Mini RLHF Pipeline](#practice-8-complete-mini-rlhf-pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing and Importing Essential Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import essential libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from scipy.special import expit  # sigmoid function\n",
    "from scipy.stats import entropy\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Visualization settings\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 12\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "print(\"‚úÖ All libraries loaded successfully!\")\n",
    "print(\"üìö Ready for RLHF practice in healthcare AI\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Practice 1: Preference Data Creation and Analysis\n",
    "\n",
    "### üéØ Learning Objectives\n",
    "- Understand the structure of preference datasets\n",
    "- Create synthetic medical preference data\n",
    "- Analyze preference agreement patterns\n",
    "\n",
    "### üìñ Key Concepts\n",
    "**Preference Dataset:** Contains pairs of model outputs with expert rankings\n",
    "- Input: Clinical query or task description\n",
    "- Output A and Output B: Two candidate responses\n",
    "- Preference Label: Which output is better (A > B or B > A)\n",
    "- Confidence Score: Strength of preference (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.1 Create synthetic preference dataset\n",
    "def create_preference_dataset(n_samples=100):\n",
    "    \"\"\"Generate synthetic medical preference data\"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Sample clinical scenarios\n",
    "    scenarios = [\n",
    "        \"Patient with fever and cough\",\n",
    "        \"Elderly patient with chest pain\",\n",
    "        \"Child with abdominal pain\",\n",
    "        \"Patient with persistent headache\",\n",
    "        \"Diabetic patient with foot wound\"\n",
    "    ]\n",
    "    \n",
    "    data = []\n",
    "    for i in range(n_samples):\n",
    "        scenario = np.random.choice(scenarios)\n",
    "        \n",
    "        # Simulate quality scores for two outputs\n",
    "        quality_A = np.random.uniform(0.4, 0.9)\n",
    "        quality_B = np.random.uniform(0.4, 0.9)\n",
    "        \n",
    "        # Determine preference based on quality\n",
    "        preference = \"A\" if quality_A > quality_B else \"B\"\n",
    "        \n",
    "        # Confidence based on quality difference\n",
    "        confidence = abs(quality_A - quality_B)\n",
    "        \n",
    "        data.append({\n",
    "            'id': i,\n",
    "            'scenario': scenario,\n",
    "            'quality_A': quality_A,\n",
    "            'quality_B': quality_B,\n",
    "            'preference': preference,\n",
    "            'confidence': confidence\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"Preference Dataset Created\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Total samples: {len(df)}\")\n",
    "    print(f\"\\nPreference distribution:\")\n",
    "    print(df['preference'].value_counts())\n",
    "    print(f\"\\nAverage confidence: {df['confidence'].mean():.3f}\")\n",
    "    print(f\"\\nFirst 5 samples:\")\n",
    "    print(df[['scenario', 'quality_A', 'quality_B', 'preference', 'confidence']].head())\n",
    "    \n",
    "    return df\n",
    "\n",
    "preference_df = create_preference_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.2 Visualize preference distribution\n",
    "def visualize_preferences(df):\n",
    "    \"\"\"Visualize preference patterns\"\"\"\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "    \n",
    "    # Preference distribution\n",
    "    df['preference'].value_counts().plot(kind='bar', ax=axes[0], color=['#1E64C8', '#6bcf7f'])\n",
    "    axes[0].set_title('Preference Distribution')\n",
    "    axes[0].set_xlabel('Preferred Output')\n",
    "    axes[0].set_ylabel('Count')\n",
    "    axes[0].set_xticklabels(['A', 'B'], rotation=0)\n",
    "    \n",
    "    # Confidence distribution\n",
    "    axes[1].hist(df['confidence'], bins=20, color='#1E64C8', alpha=0.7, edgecolor='black')\n",
    "    axes[1].set_title('Confidence Score Distribution')\n",
    "    axes[1].set_xlabel('Confidence')\n",
    "    axes[1].set_ylabel('Frequency')\n",
    "    \n",
    "    # Quality scatter\n",
    "    colors = ['#1E64C8' if p == 'A' else '#6bcf7f' for p in df['preference']]\n",
    "    axes[2].scatter(df['quality_A'], df['quality_B'], c=colors, alpha=0.6, s=50)\n",
    "    axes[2].plot([0.4, 0.9], [0.4, 0.9], 'r--', label='Equal Quality')\n",
    "    axes[2].set_title('Quality Comparison')\n",
    "    axes[2].set_xlabel('Quality A')\n",
    "    axes[2].set_ylabel('Quality B')\n",
    "    axes[2].legend(['Equal', 'Prefer A', 'Prefer B'])\n",
    "    axes[2].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nüìä Visualization complete!\")\n",
    "\n",
    "visualize_preferences(preference_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Practice 2: Bradley-Terry Reward Model Training\n",
    "\n",
    "### üéØ Learning Objectives\n",
    "- Understand the Bradley-Terry model formula\n",
    "- Implement preference probability calculation\n",
    "- Train a simple reward model\n",
    "\n",
    "### üìñ Key Concepts\n",
    "**Bradley-Terry Model:**\n",
    "$$P(A > B) = \\sigma(r(A) - r(B)) = \\frac{1}{1 + e^{-(r(A) - r(B))}}$$\n",
    "\n",
    "Where:\n",
    "- $r(A)$ = reward score for output A\n",
    "- $r(B)$ = reward score for output B\n",
    "- $\\sigma$ = sigmoid function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1 Bradley-Terry model implementation\n",
    "def bradley_terry_probability(r_A, r_B):\n",
    "    \"\"\"\n",
    "    Calculate probability that A is preferred over B\n",
    "    P(A > B) = sigmoid(r(A) - r(B))\n",
    "    \"\"\"\n",
    "    return expit(r_A - r_B)  # expit is the sigmoid function\n",
    "\n",
    "def demonstrate_bradley_terry():\n",
    "    \"\"\"Demonstrate Bradley-Terry calculations\"\"\"\n",
    "    print(\"Bradley-Terry Model Demonstration\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Example scenarios\n",
    "    scenarios = [\n",
    "        (0.8, 0.3, \"A much better than B\"),\n",
    "        (0.6, 0.5, \"A slightly better than B\"),\n",
    "        (0.5, 0.5, \"A and B equal\"),\n",
    "        (0.3, 0.8, \"B much better than A\")\n",
    "    ]\n",
    "    \n",
    "    results = []\n",
    "    for r_A, r_B, description in scenarios:\n",
    "        prob_A_wins = bradley_terry_probability(r_A, r_B)\n",
    "        results.append({\n",
    "            'r(A)': r_A,\n",
    "            'r(B)': r_B,\n",
    "            'Œîr': r_A - r_B,\n",
    "            'P(A>B)': prob_A_wins,\n",
    "            'Description': description\n",
    "        })\n",
    "    \n",
    "    results_df = pd.DataFrame(results)\n",
    "    print(results_df.to_string(index=False))\n",
    "    \n",
    "    # Visualization\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    delta_r = np.linspace(-3, 3, 100)\n",
    "    prob = expit(delta_r)\n",
    "    \n",
    "    ax.plot(delta_r, prob, 'b-', linewidth=2, label='P(A>B) = œÉ(r(A) - r(B))')\n",
    "    ax.axhline(y=0.5, color='r', linestyle='--', alpha=0.5, label='Equal preference')\n",
    "    ax.axvline(x=0, color='gray', linestyle='--', alpha=0.3)\n",
    "    \n",
    "    # Mark example points\n",
    "    for _, row in results_df.iterrows():\n",
    "        ax.plot(row['Œîr'], row['P(A>B)'], 'ro', markersize=8)\n",
    "        ax.annotate(f\"Œîr={row['Œîr']:.1f}\", \n",
    "                   xy=(row['Œîr'], row['P(A>B)']), \n",
    "                   xytext=(10, 10), \n",
    "                   textcoords='offset points',\n",
    "                   fontsize=9)\n",
    "    \n",
    "    ax.set_xlabel('Reward Difference: r(A) - r(B)', fontsize=12)\n",
    "    ax.set_ylabel('Probability P(A > B)', fontsize=12)\n",
    "    ax.set_title('Bradley-Terry Model: Sigmoid Function', fontsize=14, fontweight='bold')\n",
    "    ax.legend(fontsize=11)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n‚úÖ Key insight: Higher reward difference ‚Üí Stronger preference probability\")\n",
    "\n",
    "demonstrate_bradley_terry()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Practice 3: Simple Reward Model Implementation\n",
    "\n",
    "### üéØ Learning Objectives\n",
    "- Build a simple reward model from preference data\n",
    "- Calculate reward scores for medical outputs\n",
    "- Evaluate model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1 Simple reward model training\n",
    "class SimpleRewardModel:\n",
    "    \"\"\"A simple reward model based on preference learning\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.rewards = {}\n",
    "    \n",
    "    def train(self, preference_df):\n",
    "        \"\"\"Train reward model from preference data\"\"\"\n",
    "        print(\"Training Simple Reward Model...\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # Use quality scores as proxy for rewards\n",
    "        # In real RLHF, these would be learned from a neural network\n",
    "        all_scenarios = preference_df['scenario'].unique()\n",
    "        \n",
    "        for scenario in all_scenarios:\n",
    "            scenario_data = preference_df[preference_df['scenario'] == scenario]\n",
    "            avg_quality_A = scenario_data['quality_A'].mean()\n",
    "            avg_quality_B = scenario_data['quality_B'].mean()\n",
    "            \n",
    "            self.rewards[scenario] = {\n",
    "                'base_reward_A': avg_quality_A,\n",
    "                'base_reward_B': avg_quality_B\n",
    "            }\n",
    "        \n",
    "        print(f\"Trained on {len(all_scenarios)} scenarios\")\n",
    "        print(f\"Total preference pairs: {len(preference_df)}\")\n",
    "        \n",
    "    def predict_preference(self, scenario, quality_A, quality_B):\n",
    "        \"\"\"Predict which output is preferred\"\"\"\n",
    "        if scenario in self.rewards:\n",
    "            r_A = quality_A\n",
    "            r_B = quality_B\n",
    "        else:\n",
    "            r_A = quality_A\n",
    "            r_B = quality_B\n",
    "        \n",
    "        # Bradley-Terry probability\n",
    "        prob_A_preferred = bradley_terry_probability(r_A, r_B)\n",
    "        \n",
    "        return {\n",
    "            'prob_A': prob_A_preferred,\n",
    "            'prob_B': 1 - prob_A_preferred,\n",
    "            'predicted': 'A' if prob_A_preferred > 0.5 else 'B',\n",
    "            'confidence': abs(prob_A_preferred - 0.5) * 2\n",
    "        }\n",
    "    \n",
    "    def evaluate(self, test_df):\n",
    "        \"\"\"Evaluate reward model on test data\"\"\"\n",
    "        predictions = []\n",
    "        \n",
    "        for _, row in test_df.iterrows():\n",
    "            pred = self.predict_preference(\n",
    "                row['scenario'], \n",
    "                row['quality_A'], \n",
    "                row['quality_B']\n",
    "            )\n",
    "            predictions.append(pred['predicted'])\n",
    "        \n",
    "        accuracy = accuracy_score(test_df['preference'], predictions)\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"Model Evaluation Results\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"Accuracy: {accuracy:.2%}\")\n",
    "        print(f\"\\nConfusion Matrix:\")\n",
    "        cm = confusion_matrix(test_df['preference'], predictions)\n",
    "        print(cm)\n",
    "        \n",
    "        return accuracy\n",
    "\n",
    "# Train and evaluate\n",
    "train_df, test_df = train_test_split(preference_df, test_size=0.2, random_state=42)\n",
    "\n",
    "reward_model = SimpleRewardModel()\n",
    "reward_model.train(train_df)\n",
    "accuracy = reward_model.evaluate(test_df)\n",
    "\n",
    "print(\"\\n‚úÖ Reward model training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Practice 4: KL Divergence Calculation\n",
    "\n",
    "### üéØ Learning Objectives\n",
    "- Understand KL divergence as a measure of distribution difference\n",
    "- Calculate KL divergence between policy and reference\n",
    "- Visualize the effect of KL penalty\n",
    "\n",
    "### üìñ Key Concepts\n",
    "**KL Divergence:** Measures how much the optimized policy $\\pi_{\\theta}$ deviates from the reference policy $\\pi_{ref}$\n",
    "\n",
    "$$KL(\\pi_{\\theta} || \\pi_{ref}) = \\mathbb{E}[\\log(\\pi_{\\theta}(a|s)) - \\log(\\pi_{ref}(a|s))]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1 KL divergence calculation\n",
    "def calculate_kl_divergence(p, q):\n",
    "    \"\"\"\n",
    "    Calculate KL divergence between two probability distributions\n",
    "    KL(P || Q) = sum(P(x) * log(P(x) / Q(x)))\n",
    "    \"\"\"\n",
    "    # Ensure distributions are normalized\n",
    "    p = np.array(p) / np.sum(p)\n",
    "    q = np.array(q) / np.sum(q)\n",
    "    \n",
    "    # Add small epsilon to avoid log(0)\n",
    "    epsilon = 1e-10\n",
    "    p = p + epsilon\n",
    "    q = q + epsilon\n",
    "    \n",
    "    kl = np.sum(p * np.log(p / q))\n",
    "    return kl\n",
    "\n",
    "def demonstrate_kl_divergence():\n",
    "    \"\"\"Demonstrate KL divergence with examples\"\"\"\n",
    "    print(\"KL Divergence Demonstration\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Reference policy (original model)\n",
    "    pi_ref = np.array([0.6, 0.3, 0.1])  # Probabilities for 3 actions\n",
    "    \n",
    "    # Different optimized policies\n",
    "    scenarios = [\n",
    "        (np.array([0.6, 0.3, 0.1]), \"No change (identical)\"),\n",
    "        (np.array([0.65, 0.25, 0.1]), \"Slight deviation\"),\n",
    "        (np.array([0.7, 0.2, 0.1]), \"Moderate deviation\"),\n",
    "        (np.array([0.8, 0.15, 0.05]), \"Large deviation\"),\n",
    "        (np.array([0.1, 0.3, 0.6]), \"Very large deviation\")\n",
    "    ]\n",
    "    \n",
    "    results = []\n",
    "    for pi_theta, description in scenarios:\n",
    "        kl = calculate_kl_divergence(pi_theta, pi_ref)\n",
    "        results.append({\n",
    "            'Policy': description,\n",
    "            'Distribution': str(pi_theta),\n",
    "            'KL Divergence': kl\n",
    "        })\n",
    "    \n",
    "    results_df = pd.DataFrame(results)\n",
    "    print(results_df.to_string(index=False))\n",
    "    \n",
    "    # Visualization\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Bar plot of KL values\n",
    "    colors = ['green', 'yellow', 'orange', 'red', 'darkred']\n",
    "    axes[0].barh(range(len(results)), [r['KL Divergence'] for r in results], color=colors)\n",
    "    axes[0].set_yticks(range(len(results)))\n",
    "    axes[0].set_yticklabels([r['Policy'] for r in results])\n",
    "    axes[0].set_xlabel('KL Divergence')\n",
    "    axes[0].set_title('KL Divergence from Reference Policy')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Distribution comparison\n",
    "    x = np.arange(3)\n",
    "    width = 0.15\n",
    "    \n",
    "    axes[1].bar(x - 2*width, pi_ref, width, label='Reference', color='blue', alpha=0.7)\n",
    "    for i, (pi_theta, desc) in enumerate(scenarios[1:4], 1):  # Show first 3 deviations\n",
    "        axes[1].bar(x + (i-1)*width, pi_theta, width, label=desc, alpha=0.7)\n",
    "    \n",
    "    axes[1].set_xlabel('Action')\n",
    "    axes[1].set_ylabel('Probability')\n",
    "    axes[1].set_title('Policy Distribution Comparison')\n",
    "    axes[1].set_xticks(x)\n",
    "    axes[1].set_xticklabels(['Action 1', 'Action 2', 'Action 3'])\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n‚úÖ Key insight: Higher KL divergence ‚Üí More deviation from original model\")\n",
    "    print(\"‚ö†Ô∏è  In medical AI: Keep KL low to preserve base knowledge!\")\n",
    "\n",
    "demonstrate_kl_divergence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.2 Effect of beta (KL penalty coefficient)\n",
    "def demonstrate_kl_penalty():\n",
    "    \"\"\"Show how different beta values affect optimization\"\"\"\n",
    "    print(\"\\nKL Penalty Effect (Œ≤ coefficient)\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Simulate reward and KL values\n",
    "    reward = 5.0  # Base reward\n",
    "    kl_values = np.linspace(0, 2, 50)\n",
    "    beta_values = [0.01, 0.05, 0.1, 0.2]  # Different penalty strengths\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    for beta in beta_values:\n",
    "        penalized_reward = reward - beta * kl_values\n",
    "        plt.plot(kl_values, penalized_reward, label=f'Œ≤ = {beta}', linewidth=2)\n",
    "    \n",
    "    plt.axhline(y=0, color='red', linestyle='--', alpha=0.5, label='Zero reward')\n",
    "    plt.xlabel('KL Divergence', fontsize=12)\n",
    "    plt.ylabel('Penalized Reward', fontsize=12)\n",
    "    plt.title('Effect of KL Penalty: Reward - Œ≤ √ó KL', fontsize=14, fontweight='bold')\n",
    "    plt.legend(fontsize=11)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nüìä Interpretation:\")\n",
    "    print(\"  ‚Ä¢ Low Œ≤ (0.01): More freedom to optimize, risk of forgetting\")\n",
    "    print(\"  ‚Ä¢ High Œ≤ (0.1-0.2): Strong constraint, minimal deviation\")\n",
    "    print(\"  ‚Ä¢ Medical AI: Typical range Œ≤ ‚àà [0.01, 0.1]\")\n",
    "\n",
    "demonstrate_kl_penalty()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Practice 5: Safety Constraint Implementation\n",
    "\n",
    "### üéØ Learning Objectives\n",
    "- Implement hard safety constraints for medical AI\n",
    "- Build a constraint checker system\n",
    "- Simulate constraint violation scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.1 Medical safety constraint checker\n",
    "class MedicalSafetyChecker:\n",
    "    \"\"\"Enforce safety constraints for medical AI outputs\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Define contraindications\n",
    "        self.contraindications = {\n",
    "            'aspirin': ['bleeding_disorder', 'ulcer'],\n",
    "            'ace_inhibitor': ['pregnancy', 'angioedema_history'],\n",
    "            'beta_blocker': ['asthma', 'heart_block']\n",
    "        }\n",
    "        \n",
    "        # Define dosage limits (mg/day)\n",
    "        self.dosage_limits = {\n",
    "            'aspirin': (81, 325),\n",
    "            'ace_inhibitor': (2.5, 40),\n",
    "            'beta_blocker': (25, 200)\n",
    "        }\n",
    "    \n",
    "    def check_contraindication(self, medication, patient_conditions):\n",
    "        \"\"\"Check if medication is contraindicated\"\"\"\n",
    "        if medication not in self.contraindications:\n",
    "            return True, \"No known contraindications\"\n",
    "        \n",
    "        contraindicated_conditions = self.contraindications[medication]\n",
    "        violations = [c for c in patient_conditions if c in contraindicated_conditions]\n",
    "        \n",
    "        if violations:\n",
    "            return False, f\"Contraindicated: {', '.join(violations)}\"\n",
    "        return True, \"Safe\"\n",
    "    \n",
    "    def check_dosage(self, medication, dosage):\n",
    "        \"\"\"Check if dosage is within safe limits\"\"\"\n",
    "        if medication not in self.dosage_limits:\n",
    "            return True, \"No dosage limit defined\"\n",
    "        \n",
    "        min_dose, max_dose = self.dosage_limits[medication]\n",
    "        \n",
    "        if dosage < min_dose:\n",
    "            return False, f\"Dose too low (min: {min_dose} mg)\"\n",
    "        elif dosage > max_dose:\n",
    "            return False, f\"Dose too high (max: {max_dose} mg)\"\n",
    "        else:\n",
    "            return True, \"Dosage within safe range\"\n",
    "    \n",
    "    def validate_recommendation(self, medication, dosage, patient_conditions):\n",
    "        \"\"\"Comprehensive safety check\"\"\"\n",
    "        results = {\n",
    "            'medication': medication,\n",
    "            'dosage': dosage,\n",
    "            'patient_conditions': patient_conditions,\n",
    "            'checks': []\n",
    "        }\n",
    "        \n",
    "        # Check contraindication\n",
    "        contra_safe, contra_msg = self.check_contraindication(medication, patient_conditions)\n",
    "        results['checks'].append(('Contraindication', contra_safe, contra_msg))\n",
    "        \n",
    "        # Check dosage\n",
    "        dose_safe, dose_msg = self.check_dosage(medication, dosage)\n",
    "        results['checks'].append(('Dosage', dose_safe, dose_msg))\n",
    "        \n",
    "        # Overall safety\n",
    "        results['safe'] = contra_safe and dose_safe\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Demonstrate safety checking\n",
    "def demonstrate_safety_constraints():\n",
    "    print(\"Medical Safety Constraint Checking\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    checker = MedicalSafetyChecker()\n",
    "    \n",
    "    # Test cases\n",
    "    test_cases = [\n",
    "        ('aspirin', 100, ['hypertension']),\n",
    "        ('aspirin', 100, ['bleeding_disorder']),\n",
    "        ('aspirin', 500, ['hypertension']),\n",
    "        ('beta_blocker', 50, ['hypertension']),\n",
    "        ('beta_blocker', 50, ['asthma'])\n",
    "    ]\n",
    "    \n",
    "    for medication, dosage, conditions in test_cases:\n",
    "        result = checker.validate_recommendation(medication, dosage, conditions)\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Medication: {medication}\")\n",
    "        print(f\"Dosage: {dosage} mg\")\n",
    "        print(f\"Patient conditions: {', '.join(conditions)}\")\n",
    "        print(f\"\\nSafety Checks:\")\n",
    "        \n",
    "        for check_name, is_safe, message in result['checks']:\n",
    "            status = \"‚úÖ PASS\" if is_safe else \"‚ùå FAIL\"\n",
    "            print(f\"  {check_name}: {status} - {message}\")\n",
    "        \n",
    "        overall = \"‚úÖ SAFE TO RECOMMEND\" if result['safe'] else \"‚õî BLOCKED - UNSAFE\"\n",
    "        print(f\"\\nOverall: {overall}\")\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"\\nüí° Key insight: Hard constraints prevent harmful outputs regardless of reward\")\n",
    "\n",
    "demonstrate_safety_constraints()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Practice 6: Performance Monitoring Dashboard\n",
    "\n",
    "### üéØ Learning Objectives\n",
    "- Create a simple monitoring system\n",
    "- Track key performance indicators (KPIs)\n",
    "- Visualize performance trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.1 Simulate monitoring data\n",
    "def generate_monitoring_data(n_days=30):\n",
    "    \"\"\"Generate synthetic monitoring data\"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    data = []\n",
    "    for day in range(n_days):\n",
    "        # Simulate gradual improvement with some noise\n",
    "        base_accuracy = 0.75 + (day / n_days) * 0.1\n",
    "        accuracy = base_accuracy + np.random.normal(0, 0.02)\n",
    "        \n",
    "        base_safety = 0.95 + (day / n_days) * 0.03\n",
    "        safety_score = min(base_safety + np.random.normal(0, 0.01), 0.99)\n",
    "        \n",
    "        response_time = 0.5 + np.random.exponential(0.2)\n",
    "        \n",
    "        satisfaction = 0.7 + (day / n_days) * 0.15 + np.random.normal(0, 0.03)\n",
    "        \n",
    "        data.append({\n",
    "            'day': day + 1,\n",
    "            'accuracy': np.clip(accuracy, 0, 1),\n",
    "            'safety_score': np.clip(safety_score, 0, 1),\n",
    "            'response_time': response_time,\n",
    "            'user_satisfaction': np.clip(satisfaction, 0, 1),\n",
    "            'queries_processed': np.random.randint(800, 1200)\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "def create_monitoring_dashboard(df):\n",
    "    \"\"\"Create performance monitoring dashboard\"\"\"\n",
    "    print(\"Performance Monitoring Dashboard\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Current metrics\n",
    "    latest = df.iloc[-1]\n",
    "    print(f\"\\nüìä Current Metrics (Day {int(latest['day'])})\")\n",
    "    print(f\"  Accuracy: {latest['accuracy']:.2%}\")\n",
    "    print(f\"  Safety Score: {latest['safety_score']:.2%}\")\n",
    "    print(f\"  Avg Response Time: {latest['response_time']:.2f}s\")\n",
    "    print(f\"  User Satisfaction: {latest['user_satisfaction']:.2%}\")\n",
    "    print(f\"  Queries Processed: {int(latest['queries_processed'])}\")\n",
    "    \n",
    "    # Trends\n",
    "    print(f\"\\nüìà Trends (Last 7 days vs Previous 7 days)\")\n",
    "    recent_7 = df.iloc[-7:]\n",
    "    previous_7 = df.iloc[-14:-7]\n",
    "    \n",
    "    metrics = ['accuracy', 'safety_score', 'user_satisfaction']\n",
    "    for metric in metrics:\n",
    "        recent_avg = recent_7[metric].mean()\n",
    "        previous_avg = previous_7[metric].mean()\n",
    "        change = recent_avg - previous_avg\n",
    "        arrow = \"‚Üó\" if change > 0 else \"‚Üò\"\n",
    "        print(f\"  {metric.replace('_', ' ').title()}: {recent_avg:.2%} {arrow} ({change:+.2%})\")\n",
    "    \n",
    "    # Visualization\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    # Accuracy over time\n",
    "    axes[0, 0].plot(df['day'], df['accuracy'], 'b-', linewidth=2)\n",
    "    axes[0, 0].axhline(y=0.8, color='green', linestyle='--', alpha=0.5, label='Target')\n",
    "    axes[0, 0].set_xlabel('Day')\n",
    "    axes[0, 0].set_ylabel('Accuracy')\n",
    "    axes[0, 0].set_title('Model Accuracy Trend')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Safety score\n",
    "    axes[0, 1].plot(df['day'], df['safety_score'], 'g-', linewidth=2)\n",
    "    axes[0, 1].axhline(y=0.95, color='red', linestyle='--', alpha=0.5, label='Threshold')\n",
    "    axes[0, 1].set_xlabel('Day')\n",
    "    axes[0, 1].set_ylabel('Safety Score')\n",
    "    axes[0, 1].set_title('Safety Score Trend')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Response time distribution\n",
    "    axes[1, 0].hist(df['response_time'], bins=20, color='orange', alpha=0.7, edgecolor='black')\n",
    "    axes[1, 0].axvline(x=df['response_time'].mean(), color='red', linestyle='--', \n",
    "                       linewidth=2, label=f\"Mean: {df['response_time'].mean():.2f}s\")\n",
    "    axes[1, 0].set_xlabel('Response Time (s)')\n",
    "    axes[1, 0].set_ylabel('Frequency')\n",
    "    axes[1, 0].set_title('Response Time Distribution')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # User satisfaction\n",
    "    axes[1, 1].plot(df['day'], df['user_satisfaction'], 'purple', linewidth=2)\n",
    "    axes[1, 1].fill_between(df['day'], df['user_satisfaction'], alpha=0.3, color='purple')\n",
    "    axes[1, 1].set_xlabel('Day')\n",
    "    axes[1, 1].set_ylabel('Satisfaction Score')\n",
    "    axes[1, 1].set_title('User Satisfaction Trend')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n‚úÖ Dashboard generated successfully!\")\n",
    "\n",
    "monitoring_df = generate_monitoring_data()\n",
    "create_monitoring_dashboard(monitoring_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Practice 7: A/B Testing Simulation\n",
    "\n",
    "### üéØ Learning Objectives\n",
    "- Design and run A/B tests for model comparison\n",
    "- Perform statistical significance testing\n",
    "- Make data-driven deployment decisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.1 A/B testing simulation\n",
    "def simulate_ab_test(n_samples=1000, effect_size=0.05):\n",
    "    \"\"\"Simulate A/B test between two models\"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    print(\"A/B Testing: Model A (Control) vs Model B (Treatment)\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Model A (control - baseline)\n",
    "    accuracy_A = 0.80\n",
    "    results_A = np.random.binomial(1, accuracy_A, n_samples)\n",
    "    \n",
    "    # Model B (treatment - RLHF optimized)\n",
    "    accuracy_B = accuracy_A + effect_size\n",
    "    results_B = np.random.binomial(1, accuracy_B, n_samples)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    mean_A = results_A.mean()\n",
    "    mean_B = results_B.mean()\n",
    "    \n",
    "    print(f\"\\nüìä Results Summary:\")\n",
    "    print(f\"  Model A (Control):   {mean_A:.2%} accuracy ({results_A.sum()}/{n_samples})\")\n",
    "    print(f\"  Model B (Treatment): {mean_B:.2%} accuracy ({results_B.sum()}/{n_samples})\")\n",
    "    print(f\"  Absolute Difference: {mean_B - mean_A:+.2%}\")\n",
    "    print(f\"  Relative Improvement: {(mean_B - mean_A) / mean_A:+.2%}\")\n",
    "    \n",
    "    # Statistical test (two-proportion z-test)\n",
    "    from scipy.stats import chi2_contingency\n",
    "    \n",
    "    contingency_table = np.array([\n",
    "        [results_A.sum(), n_samples - results_A.sum()],\n",
    "        [results_B.sum(), n_samples - results_B.sum()]\n",
    "    ])\n",
    "    \n",
    "    chi2, p_value, dof, expected = chi2_contingency(contingency_table)\n",
    "    \n",
    "    print(f\"\\nüìà Statistical Test:\")\n",
    "    print(f\"  Chi-square statistic: {chi2:.4f}\")\n",
    "    print(f\"  P-value: {p_value:.4f}\")\n",
    "    \n",
    "    alpha = 0.05\n",
    "    if p_value < alpha:\n",
    "        print(f\"  ‚úÖ Result: STATISTICALLY SIGNIFICANT (p < {alpha})\")\n",
    "        print(f\"  Decision: LAUNCH Model B\")\n",
    "    else:\n",
    "        print(f\"  ‚ö†Ô∏è  Result: NOT statistically significant (p ‚â• {alpha})\")\n",
    "        print(f\"  Decision: Continue with Model A or collect more data\")\n",
    "    \n",
    "    # Visualization\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Bar comparison\n",
    "    models = ['Model A\\n(Control)', 'Model B\\n(RLHF)']\n",
    "    accuracies = [mean_A, mean_B]\n",
    "    colors = ['#1E64C8', '#6bcf7f']\n",
    "    \n",
    "    bars = axes[0].bar(models, accuracies, color=colors, alpha=0.7, edgecolor='black')\n",
    "    axes[0].set_ylabel('Accuracy')\n",
    "    axes[0].set_title('Model Comparison')\n",
    "    axes[0].set_ylim([0.75, 0.90])\n",
    "    axes[0].grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, acc in zip(bars, accuracies):\n",
    "        height = bar.get_height()\n",
    "        axes[0].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                    f'{acc:.2%}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # Confidence intervals (bootstrap)\n",
    "    n_bootstrap = 1000\n",
    "    bootstrap_A = [np.random.choice(results_A, size=n_samples, replace=True).mean() \n",
    "                   for _ in range(n_bootstrap)]\n",
    "    bootstrap_B = [np.random.choice(results_B, size=n_samples, replace=True).mean() \n",
    "                   for _ in range(n_bootstrap)]\n",
    "    \n",
    "    axes[1].hist(bootstrap_A, bins=30, alpha=0.5, label='Model A', color='#1E64C8')\n",
    "    axes[1].hist(bootstrap_B, bins=30, alpha=0.5, label='Model B', color='#6bcf7f')\n",
    "    axes[1].axvline(mean_A, color='#1E64C8', linestyle='--', linewidth=2)\n",
    "    axes[1].axvline(mean_B, color='#6bcf7f', linestyle='--', linewidth=2)\n",
    "    axes[1].set_xlabel('Accuracy')\n",
    "    axes[1].set_ylabel('Frequency')\n",
    "    axes[1].set_title('Bootstrap Distribution')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n‚úÖ A/B test complete!\")\n",
    "    return mean_A, mean_B, p_value\n",
    "\n",
    "# Run simulation\n",
    "acc_A, acc_B, p_val = simulate_ab_test(n_samples=1000, effect_size=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Practice 8: Complete Mini RLHF Pipeline\n",
    "\n",
    "### üéØ Learning Objectives\n",
    "- Integrate all components into a complete pipeline\n",
    "- Run end-to-end RLHF simulation\n",
    "- Understand the full workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.1 Complete RLHF Pipeline\n",
    "class MiniRLHFPipeline:\n",
    "    \"\"\"Complete mini RLHF pipeline for medical AI\"\"\"\n",
    "    \n",
    "    def __init__(self, beta=0.05):\n",
    "        self.beta = beta  # KL penalty coefficient\n",
    "        self.reward_model = None\n",
    "        self.safety_checker = MedicalSafetyChecker()\n",
    "        self.training_history = []\n",
    "    \n",
    "    def step1_collect_preferences(self, n_samples=200):\n",
    "        \"\"\"Step 1: Collect preference data\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"STEP 1: Collecting Preference Data\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        self.preference_data = create_preference_dataset(n_samples)\n",
    "        return self.preference_data\n",
    "    \n",
    "    def step2_train_reward_model(self):\n",
    "        \"\"\"Step 2: Train reward model\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"STEP 2: Training Reward Model\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        train_df, test_df = train_test_split(self.preference_data, test_size=0.2)\n",
    "        \n",
    "        self.reward_model = SimpleRewardModel()\n",
    "        self.reward_model.train(train_df)\n",
    "        accuracy = self.reward_model.evaluate(test_df)\n",
    "        \n",
    "        return accuracy\n",
    "    \n",
    "    def step3_policy_optimization(self, n_iterations=5):\n",
    "        \"\"\"Step 3: Optimize policy with PPO (simulated)\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"STEP 3: Policy Optimization (Simulated)\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        print(f\"Running {n_iterations} optimization iterations...\")\n",
    "        print(f\"KL penalty coefficient (Œ≤): {self.beta}\\n\")\n",
    "        \n",
    "        for iteration in range(n_iterations):\n",
    "            # Simulate reward and KL divergence\n",
    "            base_reward = 5.0 + iteration * 0.5  # Improving reward\n",
    "            kl_divergence = 0.1 + iteration * 0.05  # Increasing KL\n",
    "            \n",
    "            # Apply KL penalty\n",
    "            penalized_reward = base_reward - self.beta * kl_divergence\n",
    "            \n",
    "            # Simulate safety score (decreases if too much optimization)\n",
    "            safety_score = max(0.95 - iteration * 0.01, 0.90)\n",
    "            \n",
    "            self.training_history.append({\n",
    "                'iteration': iteration + 1,\n",
    "                'base_reward': base_reward,\n",
    "                'kl_divergence': kl_divergence,\n",
    "                'penalized_reward': penalized_reward,\n",
    "                'safety_score': safety_score\n",
    "            })\n",
    "            \n",
    "            print(f\"Iteration {iteration+1}: \"\n",
    "                  f\"Reward={base_reward:.2f}, \"\n",
    "                  f\"KL={kl_divergence:.3f}, \"\n",
    "                  f\"Penalized={penalized_reward:.2f}, \"\n",
    "                  f\"Safety={safety_score:.2%}\")\n",
    "        \n",
    "        print(\"\\n‚úÖ Policy optimization complete!\")\n",
    "    \n",
    "    def step4_safety_validation(self):\n",
    "        \"\"\"Step 4: Validate safety constraints\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"STEP 4: Safety Validation\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        test_cases = [\n",
    "            ('aspirin', 150, ['hypertension']),\n",
    "            ('beta_blocker', 75, ['diabetes'])\n",
    "        ]\n",
    "        \n",
    "        passed = 0\n",
    "        for medication, dosage, conditions in test_cases:\n",
    "            result = self.safety_checker.validate_recommendation(\n",
    "                medication, dosage, conditions\n",
    "            )\n",
    "            if result['safe']:\n",
    "                passed += 1\n",
    "                print(f\"‚úÖ {medication} ({dosage}mg): SAFE\")\n",
    "            else:\n",
    "                print(f\"‚ùå {medication} ({dosage}mg): UNSAFE\")\n",
    "        \n",
    "        print(f\"\\nSafety validation: {passed}/{len(test_cases)} passed\")\n",
    "    \n",
    "    def step5_visualize_results(self):\n",
    "        \"\"\"Step 5: Visualize training results\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"STEP 5: Results Visualization\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        history_df = pd.DataFrame(self.training_history)\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "        \n",
    "        # Reward over iterations\n",
    "        axes[0, 0].plot(history_df['iteration'], history_df['base_reward'], \n",
    "                       'b-o', label='Base Reward', linewidth=2)\n",
    "        axes[0, 0].plot(history_df['iteration'], history_df['penalized_reward'], \n",
    "                       'r-s', label='Penalized Reward', linewidth=2)\n",
    "        axes[0, 0].set_xlabel('Iteration')\n",
    "        axes[0, 0].set_ylabel('Reward')\n",
    "        axes[0, 0].set_title('Reward Optimization Progress')\n",
    "        axes[0, 0].legend()\n",
    "        axes[0, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # KL divergence\n",
    "        axes[0, 1].plot(history_df['iteration'], history_df['kl_divergence'], \n",
    "                       'g-^', linewidth=2)\n",
    "        axes[0, 1].axhline(y=0.1, color='orange', linestyle='--', \n",
    "                          label='Target KL', alpha=0.7)\n",
    "        axes[0, 1].set_xlabel('Iteration')\n",
    "        axes[0, 1].set_ylabel('KL Divergence')\n",
    "        axes[0, 1].set_title('KL Divergence from Base Model')\n",
    "        axes[0, 1].legend()\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Safety score\n",
    "        axes[1, 0].plot(history_df['iteration'], history_df['safety_score'], \n",
    "                       'purple', linewidth=2, marker='d')\n",
    "        axes[1, 0].axhline(y=0.95, color='red', linestyle='--', \n",
    "                          label='Safety Threshold', alpha=0.7)\n",
    "        axes[1, 0].set_xlabel('Iteration')\n",
    "        axes[1, 0].set_ylabel('Safety Score')\n",
    "        axes[1, 0].set_title('Safety Score Monitoring')\n",
    "        axes[1, 0].legend()\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Summary metrics\n",
    "        axes[1, 1].axis('off')\n",
    "        summary_text = f\"\"\"\n",
    "        RLHF Pipeline Summary\n",
    "        {'='*40}\n",
    "        \n",
    "        Final Metrics:\n",
    "          ‚Ä¢ Base Reward: {history_df['base_reward'].iloc[-1]:.2f}\n",
    "          ‚Ä¢ KL Divergence: {history_df['kl_divergence'].iloc[-1]:.3f}\n",
    "          ‚Ä¢ Penalized Reward: {history_df['penalized_reward'].iloc[-1]:.2f}\n",
    "          ‚Ä¢ Safety Score: {history_df['safety_score'].iloc[-1]:.2%}\n",
    "        \n",
    "        Improvements:\n",
    "          ‚Ä¢ Reward: +{history_df['base_reward'].iloc[-1] - history_df['base_reward'].iloc[0]:.2f}\n",
    "          ‚Ä¢ KL: +{history_df['kl_divergence'].iloc[-1] - history_df['kl_divergence'].iloc[0]:.3f}\n",
    "        \n",
    "        Œ≤ coefficient: {self.beta}\n",
    "        \n",
    "        Status: ‚úÖ Training Complete\n",
    "        \"\"\"\n",
    "        axes[1, 1].text(0.1, 0.5, summary_text, \n",
    "                       fontsize=11, family='monospace',\n",
    "                       verticalalignment='center')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"\\n‚úÖ All visualizations complete!\")\n",
    "    \n",
    "    def run_complete_pipeline(self):\n",
    "        \"\"\"Run the complete RLHF pipeline\"\"\"\n",
    "        print(\"\\n\" + \"#\"*60)\n",
    "        print(\"#\" + \" \"*58 + \"#\")\n",
    "        print(\"#\" + \" \"*10 + \"COMPLETE RLHF PIPELINE FOR MEDICAL AI\" + \" \"*9 + \"#\")\n",
    "        print(\"#\" + \" \"*58 + \"#\")\n",
    "        print(\"#\"*60)\n",
    "        \n",
    "        # Run all steps\n",
    "        self.step1_collect_preferences()\n",
    "        self.step2_train_reward_model()\n",
    "        self.step3_policy_optimization()\n",
    "        self.step4_safety_validation()\n",
    "        self.step5_visualize_results()\n",
    "        \n",
    "        print(\"\\n\" + \"#\"*60)\n",
    "        print(\"#\" + \" \"*58 + \"#\")\n",
    "        print(\"#\" + \" \"*15 + \"üéâ PIPELINE COMPLETE! üéâ\" + \" \"*16 + \"#\")\n",
    "        print(\"#\" + \" \"*58 + \"#\")\n",
    "        print(\"#\"*60)\n",
    "\n",
    "# Run the complete pipeline\n",
    "pipeline = MiniRLHFPipeline(beta=0.05)\n",
    "pipeline.run_complete_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üéØ Practice Complete!\n",
    "\n",
    "### Summary of What We Learned:\n",
    "\n",
    "1. **Preference Data Creation**: Understanding how expert preferences are structured and collected\n",
    "2. **Bradley-Terry Model**: Mathematical foundation for converting rewards to preference probabilities\n",
    "3. **Reward Model Training**: Building models that learn from expert feedback\n",
    "4. **KL Divergence Control**: Preventing excessive deviation from base model knowledge\n",
    "5. **Safety Constraints**: Implementing hard constraints to prevent harmful outputs\n",
    "6. **Performance Monitoring**: Real-time tracking of model performance in deployment\n",
    "7. **A/B Testing**: Statistical validation before deploying new models\n",
    "8. **Complete RLHF Pipeline**: End-to-end integration of all components\n",
    "\n",
    "### Key Insights:\n",
    "\n",
    "- **RLHF is iterative**: Continuous improvement through feedback loops\n",
    "- **Safety first**: Medical AI requires multiple layers of safety constraints\n",
    "- **Balance is crucial**: Trade-off between optimization and preserving base knowledge\n",
    "- **Monitoring is essential**: Continuous performance tracking prevents degradation\n",
    "- **Statistical rigor**: A/B testing ensures decisions are data-driven\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. Implement with real medical datasets\n",
    "2. Use transformer-based reward models\n",
    "3. Integrate with production LLMs\n",
    "4. Explore DPO as alternative to PPO\n",
    "5. Add more sophisticated safety layers\n",
    "6. Implement multi-objective optimization\n",
    "\n",
    "### üìö Additional Resources:\n",
    "\n",
    "- **Papers**: \"Training Language Models to Follow Instructions with Human Feedback\" (OpenAI, 2022)\n",
    "- **Libraries**: HuggingFace TRL, DeepSpeed\n",
    "- **Frameworks**: PyTorch, TensorFlow\n",
    "\n",
    "---\n",
    "\n",
    "**üéì Congratulations on completing the RLHF hands-on practice!**\n",
    "\n",
    "**Contact:**\n",
    "- Ho-min Park\n",
    "- homin.park@ghent.ac.kr\n",
    "- powersimmani@gmail.com"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
