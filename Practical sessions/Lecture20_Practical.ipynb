{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üè• Medical AI System: Practical Implementation\n",
    "\n",
    "## Lecture 20 - Capstone Project Practice\n",
    "\n",
    "### Table of Contents\n",
    "1. [Environment Setup and Data Loading](#practice-1-environment-setup)\n",
    "2. [Data Preprocessing Pipeline](#practice-2-data-preprocessing)\n",
    "3. [Model Training and Evaluation](#practice-3-model-training)\n",
    "4. [API Development (FastAPI)](#practice-4-api-development)\n",
    "5. [Model Deployment Simulation](#practice-5-deployment)\n",
    "6. [Performance Monitoring](#practice-6-monitoring)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing and Importing Essential Libraries\n",
    "\n",
    "### üìö Required Libraries\n",
    "- **Data Processing**: numpy, pandas\n",
    "- **Machine Learning**: scikit-learn, tensorflow/pytorch\n",
    "- **Visualization**: matplotlib, seaborn\n",
    "- **API**: fastapi (optional for this practice)\n",
    "- **Monitoring**: time, logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (uncomment if needed)\n",
    "# !pip install numpy pandas matplotlib seaborn scikit-learn pillow\n",
    "\n",
    "# Import essential libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score, roc_curve\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Visualization settings\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "print(\"‚úÖ All libraries loaded successfully!\")\n",
    "print(f\"üìä NumPy version: {np.__version__}\")\n",
    "print(f\"üìä Pandas version: {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Practice 1: Environment Setup and Data Loading\n",
    "\n",
    "### üéØ Learning Objectives\n",
    "- Set up a medical AI development environment\n",
    "- Load and understand medical dataset structure\n",
    "- Perform initial data quality checks\n",
    "\n",
    "### üìñ Key Concepts from Slides\n",
    "- **Requirements Analysis**: Understanding functional and non-functional requirements\n",
    "- **Data Collection**: Medical image datasets and metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.1 Generate synthetic medical dataset\n",
    "def generate_medical_dataset(n_samples=1000, random_state=42):\n",
    "    \"\"\"\n",
    "    Generate a synthetic medical dataset for binary classification\n",
    "    Simulating medical imaging features and diagnosis labels\n",
    "    \"\"\"\n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    # Generate features (simulating medical imaging measurements)\n",
    "    age = np.random.randint(20, 80, n_samples)\n",
    "    feature_1 = np.random.randn(n_samples) * 10 + 50  # e.g., tissue density\n",
    "    feature_2 = np.random.randn(n_samples) * 5 + 30   # e.g., lesion size\n",
    "    feature_3 = np.random.randn(n_samples) * 3 + 15   # e.g., contrast ratio\n",
    "    feature_4 = np.random.randn(n_samples) * 8 + 40   # e.g., texture metric\n",
    "    \n",
    "    # Generate labels (0: Normal, 1: Abnormal)\n",
    "    # Create correlation with features\n",
    "    risk_score = (age * 0.02 + feature_1 * 0.03 + feature_2 * 0.05 + \n",
    "                  feature_3 * 0.04 + feature_4 * 0.02 + np.random.randn(n_samples) * 5)\n",
    "    labels = (risk_score > np.percentile(risk_score, 70)).astype(int)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    data = pd.DataFrame({\n",
    "        'patient_id': [f'P{i:04d}' for i in range(n_samples)],\n",
    "        'age': age,\n",
    "        'tissue_density': feature_1,\n",
    "        'lesion_size': feature_2,\n",
    "        'contrast_ratio': feature_3,\n",
    "        'texture_metric': feature_4,\n",
    "        'diagnosis': labels\n",
    "    })\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Generate dataset\n",
    "medical_data = generate_medical_dataset(n_samples=1000)\n",
    "\n",
    "print(\"üè• Medical Dataset Generated\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total samples: {len(medical_data)}\")\n",
    "print(f\"Features: {list(medical_data.columns[1:-1])}\")\n",
    "print(f\"\\nClass distribution:\")\n",
    "print(medical_data['diagnosis'].value_counts())\n",
    "print(f\"\\nFirst 5 records:\")\n",
    "print(medical_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.2 Data Quality Validation\n",
    "def validate_data_quality(df):\n",
    "    \"\"\"\n",
    "    Perform data quality checks\n",
    "    Following best practices from Data Collection & Processing slide\n",
    "    \"\"\"\n",
    "    print(\"üîç Data Quality Validation\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Check for missing values\n",
    "    missing_values = df.isnull().sum()\n",
    "    print(\"\\n1. Missing Values Check:\")\n",
    "    if missing_values.sum() == 0:\n",
    "        print(\"   ‚úÖ No missing values detected\")\n",
    "    else:\n",
    "        print(f\"   ‚ö†Ô∏è Missing values found:\\n{missing_values[missing_values > 0]}\")\n",
    "    \n",
    "    # Check data types\n",
    "    print(\"\\n2. Data Types:\")\n",
    "    print(df.dtypes)\n",
    "    \n",
    "    # Statistical summary\n",
    "    print(\"\\n3. Statistical Summary:\")\n",
    "    print(df.describe())\n",
    "    \n",
    "    # Check for duplicates\n",
    "    duplicates = df.duplicated().sum()\n",
    "    print(f\"\\n4. Duplicate Records: {duplicates}\")\n",
    "    if duplicates == 0:\n",
    "        print(\"   ‚úÖ No duplicates found\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"‚úÖ Data quality validation complete!\")\n",
    "\n",
    "validate_data_quality(medical_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Practice 2: Data Preprocessing Pipeline\n",
    "\n",
    "### üéØ Learning Objectives\n",
    "- Implement data preprocessing steps\n",
    "- Apply feature normalization\n",
    "- Split data for training and testing\n",
    "\n",
    "### üìñ Key Concepts from Slides\n",
    "- **Data Processing**: Image normalization, resizing, data augmentation\n",
    "- **Data Pipeline Design**: ETL process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1 Data Preprocessing Pipeline\n",
    "def preprocess_medical_data(df):\n",
    "    \"\"\"\n",
    "    Complete preprocessing pipeline for medical data\n",
    "    \"\"\"\n",
    "    print(\"‚öôÔ∏è Data Preprocessing Pipeline\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Separate features and target\n",
    "    X = df.drop(['patient_id', 'diagnosis'], axis=1)\n",
    "    y = df['diagnosis']\n",
    "    \n",
    "    print(f\"\\n1. Feature Selection:\")\n",
    "    print(f\"   Features: {list(X.columns)}\")\n",
    "    print(f\"   Target: diagnosis (0=Normal, 1=Abnormal)\")\n",
    "    \n",
    "    # Train-test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n2. Train-Test Split:\")\n",
    "    print(f\"   Training set: {len(X_train)} samples\")\n",
    "    print(f\"   Test set: {len(X_test)} samples\")\n",
    "    print(f\"   Train class distribution: {y_train.value_counts().to_dict()}\")\n",
    "    print(f\"   Test class distribution: {y_test.value_counts().to_dict()}\")\n",
    "    \n",
    "    # Feature normalization\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    print(f\"\\n3. Feature Normalization (StandardScaler):\")\n",
    "    print(f\"   Mean: {scaler.mean_}\")\n",
    "    print(f\"   Std: {scaler.scale_}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"‚úÖ Preprocessing complete!\")\n",
    "    \n",
    "    return X_train_scaled, X_test_scaled, y_train, y_test, scaler\n",
    "\n",
    "X_train, X_test, y_train, y_test, scaler = preprocess_medical_data(medical_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2 Visualize Feature Distributions\n",
    "def visualize_features(df):\n",
    "    \"\"\"\n",
    "    Visualize feature distributions by diagnosis\n",
    "    \"\"\"\n",
    "    features = ['age', 'tissue_density', 'lesion_size', 'contrast_ratio', 'texture_metric']\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    axes = axes.ravel()\n",
    "    \n",
    "    for idx, feature in enumerate(features):\n",
    "        for diagnosis in [0, 1]:\n",
    "            data = df[df['diagnosis'] == diagnosis][feature]\n",
    "            axes[idx].hist(data, alpha=0.6, bins=30, \n",
    "                          label=f'Diagnosis {diagnosis}',\n",
    "                          edgecolor='black')\n",
    "        axes[idx].set_xlabel(feature)\n",
    "        axes[idx].set_ylabel('Frequency')\n",
    "        axes[idx].legend()\n",
    "        axes[idx].set_title(f'Distribution of {feature}')\n",
    "    \n",
    "    # Remove extra subplot\n",
    "    fig.delaxes(axes[5])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"üìä Feature distributions visualized\")\n",
    "\n",
    "visualize_features(medical_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Practice 3: Model Training and Evaluation\n",
    "\n",
    "### üéØ Learning Objectives\n",
    "- Train a machine learning model for medical diagnosis\n",
    "- Evaluate model performance using multiple metrics\n",
    "- Visualize evaluation results\n",
    "\n",
    "### üìñ Key Concepts from Slides\n",
    "- **Model Training Pipeline**: Data loading, model training, validation\n",
    "- **Evaluation Framework**: Accuracy, Precision, Recall, F1-Score, ROC-AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1 Model Training\n",
    "def train_medical_classifier(X_train, y_train):\n",
    "    \"\"\"\n",
    "    Train a Random Forest classifier for medical diagnosis\n",
    "    \"\"\"\n",
    "    print(\"ü§ñ Model Training\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Initialize model\n",
    "    model = RandomForestClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=10,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    # Train model with timing\n",
    "    start_time = time.time()\n",
    "    model.fit(X_train, y_train)\n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"\\nModel: Random Forest Classifier\")\n",
    "    print(f\"Parameters:\")\n",
    "    print(f\"  - n_estimators: 100\")\n",
    "    print(f\"  - max_depth: 10\")\n",
    "    print(f\"  - random_state: 42\")\n",
    "    print(f\"\\nTraining time: {training_time:.4f} seconds\")\n",
    "    \n",
    "    # Cross-validation\n",
    "    cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy')\n",
    "    print(f\"\\nCross-validation scores (5-fold):\")\n",
    "    print(f\"  Scores: {cv_scores}\")\n",
    "    print(f\"  Mean CV Accuracy: {cv_scores.mean():.4f} (¬±{cv_scores.std():.4f})\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"‚úÖ Model training complete!\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = train_medical_classifier(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.2 Model Evaluation\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Comprehensive model evaluation following the Evaluation Framework slide\n",
    "    \"\"\"\n",
    "    print(\"üìä Model Evaluation\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    auc_roc = roc_auc_score(y_test, y_pred_proba)\n",
    "    \n",
    "    print(\"\\nüìà Performance Metrics:\")\n",
    "    print(f\"  Accuracy:  {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "    print(f\"  Precision: {precision:.4f} ({precision*100:.2f}%)\")\n",
    "    print(f\"  Recall:    {recall:.4f} ({recall*100:.2f}%)\")\n",
    "    print(f\"  F1-Score:  {f1:.4f} ({f1*100:.2f}%)\")\n",
    "    print(f\"  AUC-ROC:   {auc_roc:.4f} ({auc_roc*100:.2f}%)\")\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    print(\"\\nüî¢ Confusion Matrix:\")\n",
    "    print(f\"  TN: {cm[0,0]:3d}  |  FP: {cm[0,1]:3d}\")\n",
    "    print(f\"  FN: {cm[1,0]:3d}  |  TP: {cm[1,1]:3d}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'auc_roc': auc_roc,\n",
    "        'confusion_matrix': cm,\n",
    "        'y_pred': y_pred,\n",
    "        'y_pred_proba': y_pred_proba\n",
    "    }\n",
    "\n",
    "eval_results = evaluate_model(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.3 Visualize Evaluation Results\n",
    "def visualize_evaluation(eval_results, y_test):\n",
    "    \"\"\"\n",
    "    Visualize confusion matrix and ROC curve\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Confusion Matrix Heatmap\n",
    "    sns.heatmap(eval_results['confusion_matrix'], annot=True, fmt='d', \n",
    "                cmap='Blues', ax=axes[0], cbar=False,\n",
    "                xticklabels=['Normal', 'Abnormal'],\n",
    "                yticklabels=['Normal', 'Abnormal'])\n",
    "    axes[0].set_xlabel('Predicted')\n",
    "    axes[0].set_ylabel('Actual')\n",
    "    axes[0].set_title('Confusion Matrix')\n",
    "    \n",
    "    # ROC Curve\n",
    "    fpr, tpr, _ = roc_curve(y_test, eval_results['y_pred_proba'])\n",
    "    axes[1].plot(fpr, tpr, linewidth=2, label=f'ROC (AUC = {eval_results[\"auc_roc\"]:.4f})')\n",
    "    axes[1].plot([0, 1], [0, 1], 'k--', linewidth=1, label='Random Classifier')\n",
    "    axes[1].set_xlabel('False Positive Rate')\n",
    "    axes[1].set_ylabel('True Positive Rate')\n",
    "    axes[1].set_title('ROC Curve')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"üìä Evaluation visualizations complete!\")\n",
    "\n",
    "visualize_evaluation(eval_results, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Practice 4: API Development (Simulation)\n",
    "\n",
    "### üéØ Learning Objectives\n",
    "- Create a prediction function for API deployment\n",
    "- Simulate API request/response flow\n",
    "- Implement input validation\n",
    "\n",
    "### üìñ Key Concepts from Slides\n",
    "- **API Development**: REST API endpoints (GET /predict, POST /upload)\n",
    "- **Security**: JWT authentication, Rate limiting, HTTPS encryption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1 API Prediction Function\n",
    "def predict_diagnosis(patient_data, model, scaler):\n",
    "    \"\"\"\n",
    "    Simulate API prediction endpoint\n",
    "    Input: patient features as dictionary\n",
    "    Output: prediction result with probability\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Validate input\n",
    "        required_features = ['age', 'tissue_density', 'lesion_size', \n",
    "                           'contrast_ratio', 'texture_metric']\n",
    "        \n",
    "        for feature in required_features:\n",
    "            if feature not in patient_data:\n",
    "                raise ValueError(f\"Missing required feature: {feature}\")\n",
    "        \n",
    "        # Prepare input\n",
    "        input_data = np.array([[patient_data[f] for f in required_features]])\n",
    "        \n",
    "        # Normalize\n",
    "        input_scaled = scaler.transform(input_data)\n",
    "        \n",
    "        # Predict\n",
    "        prediction = model.predict(input_scaled)[0]\n",
    "        probability = model.predict_proba(input_scaled)[0]\n",
    "        \n",
    "        # Format response\n",
    "        response = {\n",
    "            'status': 'success',\n",
    "            'prediction': int(prediction),\n",
    "            'diagnosis': 'Abnormal' if prediction == 1 else 'Normal',\n",
    "            'confidence': float(probability[prediction]),\n",
    "            'probabilities': {\n",
    "                'normal': float(probability[0]),\n",
    "                'abnormal': float(probability[1])\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        return response\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'status': 'error',\n",
    "            'message': str(e)\n",
    "        }\n",
    "\n",
    "# Test API function\n",
    "print(\"üîå API Prediction Function Test\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Sample patient data\n",
    "test_patient = {\n",
    "    'age': 55,\n",
    "    'tissue_density': 52.3,\n",
    "    'lesion_size': 31.5,\n",
    "    'contrast_ratio': 16.2,\n",
    "    'texture_metric': 41.8\n",
    "}\n",
    "\n",
    "print(\"\\nTest Patient Data:\")\n",
    "for key, value in test_patient.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "result = predict_diagnosis(test_patient, model, scaler)\n",
    "\n",
    "print(\"\\nAPI Response:\")\n",
    "import json\n",
    "print(json.dumps(result, indent=2))\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ API simulation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Practice 5: Model Deployment Simulation\n",
    "\n",
    "### üéØ Learning Objectives\n",
    "- Save and load trained models\n",
    "- Simulate deployment pipeline\n",
    "- Test model inference performance\n",
    "\n",
    "### üìñ Key Concepts from Slides\n",
    "- **Deployment Pipeline**: CI/CD workflow, Docker build, staging, production\n",
    "- **Deployment Strategies**: Blue-Green, Canary, Rolling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.1 Model Serialization\n",
    "import pickle\n",
    "\n",
    "def save_model_pipeline(model, scaler, filepath='medical_ai_model.pkl'):\n",
    "    \"\"\"\n",
    "    Save trained model and preprocessing pipeline\n",
    "    \"\"\"\n",
    "    pipeline = {\n",
    "        'model': model,\n",
    "        'scaler': scaler,\n",
    "        'features': ['age', 'tissue_density', 'lesion_size', 'contrast_ratio', 'texture_metric'],\n",
    "        'version': '1.0.0',\n",
    "        'trained_date': time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    }\n",
    "    \n",
    "    with open(filepath, 'wb') as f:\n",
    "        pickle.dump(pipeline, f)\n",
    "    \n",
    "    file_size = os.path.getsize(filepath) / (1024 * 1024)  # Convert to MB\n",
    "    print(f\"‚úÖ Model saved to {filepath} ({file_size:.2f} MB)\")\n",
    "    return filepath\n",
    "\n",
    "def load_model_pipeline(filepath='medical_ai_model.pkl'):\n",
    "    \"\"\"\n",
    "    Load saved model and preprocessing pipeline\n",
    "    \"\"\"\n",
    "    with open(filepath, 'rb') as f:\n",
    "        pipeline = pickle.load(f)\n",
    "    \n",
    "    print(f\"‚úÖ Model loaded from {filepath}\")\n",
    "    print(f\"   Version: {pipeline['version']}\")\n",
    "    print(f\"   Trained: {pipeline['trained_date']}\")\n",
    "    return pipeline\n",
    "\n",
    "# Save model\n",
    "import os\n",
    "print(\"üíæ Model Deployment - Save Pipeline\")\n",
    "print(\"=\" * 60)\n",
    "model_path = save_model_pipeline(model, scaler)\n",
    "\n",
    "# Load model\n",
    "print(\"\\nüì¶ Model Deployment - Load Pipeline\")\n",
    "print(\"=\" * 60)\n",
    "loaded_pipeline = load_model_pipeline(model_path)\n",
    "print(\"\\n‚úÖ Deployment simulation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.2 Inference Performance Testing\n",
    "def test_inference_performance(model, X_test, n_iterations=100):\n",
    "    \"\"\"\n",
    "    Test model inference speed and latency\n",
    "    \"\"\"\n",
    "    print(\"‚ö° Inference Performance Testing\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Single prediction latency\n",
    "    sample = X_test[0:1]\n",
    "    latencies = []\n",
    "    \n",
    "    for _ in range(n_iterations):\n",
    "        start = time.time()\n",
    "        _ = model.predict(sample)\n",
    "        latency = (time.time() - start) * 1000  # Convert to ms\n",
    "        latencies.append(latency)\n",
    "    \n",
    "    print(f\"\\nSingle Prediction Latency (n={n_iterations}):\")\n",
    "    print(f\"  Mean: {np.mean(latencies):.4f} ms\")\n",
    "    print(f\"  Std:  {np.std(latencies):.4f} ms\")\n",
    "    print(f\"  Min:  {np.min(latencies):.4f} ms\")\n",
    "    print(f\"  Max:  {np.max(latencies):.4f} ms\")\n",
    "    print(f\"  95th percentile: {np.percentile(latencies, 95):.4f} ms\")\n",
    "    \n",
    "    # Batch prediction throughput\n",
    "    batch_sizes = [1, 10, 100, 200]\n",
    "    print(f\"\\nBatch Prediction Throughput:\")\n",
    "    \n",
    "    for batch_size in batch_sizes:\n",
    "        batch = X_test[:batch_size]\n",
    "        start = time.time()\n",
    "        _ = model.predict(batch)\n",
    "        duration = time.time() - start\n",
    "        throughput = batch_size / duration\n",
    "        print(f\"  Batch size {batch_size:3d}: {throughput:8.2f} predictions/sec\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"‚úÖ Performance testing complete!\")\n",
    "    print(\"\\nüí° Target: Response time < 5s (typically ~100-500ms achieved)\")\n",
    "\n",
    "test_inference_performance(model, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Practice 6: Performance Monitoring\n",
    "\n",
    "### üéØ Learning Objectives\n",
    "- Implement performance monitoring dashboard\n",
    "- Track key metrics over time\n",
    "- Visualize system health\n",
    "\n",
    "### üìñ Key Concepts from Slides\n",
    "- **Performance Monitoring**: System health, response time, error rate, active users\n",
    "- **Monitoring Tools**: Prometheus, Grafana, ELK Stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.1 Simulate Performance Metrics\n",
    "def simulate_monitoring_metrics(duration_minutes=60):\n",
    "    \"\"\"\n",
    "    Simulate performance metrics over time\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Generate timestamps\n",
    "    timestamps = pd.date_range(start='2024-01-01', periods=duration_minutes, freq='1min')\n",
    "    \n",
    "    # Simulate metrics\n",
    "    metrics = pd.DataFrame({\n",
    "        'timestamp': timestamps,\n",
    "        'response_time_ms': np.random.normal(200, 50, duration_minutes).clip(50, 500),\n",
    "        'error_rate': np.random.uniform(0, 2, duration_minutes),\n",
    "        'active_users': np.random.randint(800, 1500, duration_minutes),\n",
    "        'cpu_usage': np.random.uniform(20, 80, duration_minutes),\n",
    "        'memory_usage': np.random.uniform(30, 70, duration_minutes)\n",
    "    })\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Generate monitoring data\n",
    "monitoring_data = simulate_monitoring_metrics(60)\n",
    "\n",
    "print(\"üìä Performance Monitoring Dashboard\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nMonitoring Period: {monitoring_data['timestamp'].min()} to {monitoring_data['timestamp'].max()}\")\n",
    "print(f\"\\nCurrent System Metrics:\")\n",
    "print(f\"  Average Response Time: {monitoring_data['response_time_ms'].mean():.2f} ms\")\n",
    "print(f\"  Average Error Rate: {monitoring_data['error_rate'].mean():.2f}%\")\n",
    "print(f\"  Average Active Users: {monitoring_data['active_users'].mean():.0f}\")\n",
    "print(f\"  CPU Usage: {monitoring_data['cpu_usage'].mean():.2f}%\")\n",
    "print(f\"  Memory Usage: {monitoring_data['memory_usage'].mean():.2f}%\")\n",
    "print(f\"\\nSystem Health: {'‚úÖ Healthy' if monitoring_data['error_rate'].mean() < 1 else '‚ö†Ô∏è Warning'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.2 Visualize Performance Metrics\n",
    "def visualize_monitoring_dashboard(metrics_df):\n",
    "    \"\"\"\n",
    "    Create a comprehensive monitoring dashboard\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "    \n",
    "    # Response Time\n",
    "    axes[0, 0].plot(metrics_df['timestamp'], metrics_df['response_time_ms'], \n",
    "                    linewidth=1.5, color='#1E64C8')\n",
    "    axes[0, 0].axhline(y=metrics_df['response_time_ms'].mean(), \n",
    "                       color='red', linestyle='--', linewidth=1, label='Average')\n",
    "    axes[0, 0].set_xlabel('Time')\n",
    "    axes[0, 0].set_ylabel('Response Time (ms)')\n",
    "    axes[0, 0].set_title('Response Time Over Time')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Error Rate\n",
    "    axes[0, 1].plot(metrics_df['timestamp'], metrics_df['error_rate'], \n",
    "                    linewidth=1.5, color='#FF5252')\n",
    "    axes[0, 1].axhline(y=1.0, color='orange', linestyle='--', \n",
    "                       linewidth=1, label='Threshold (1%)')\n",
    "    axes[0, 1].set_xlabel('Time')\n",
    "    axes[0, 1].set_ylabel('Error Rate (%)')\n",
    "    axes[0, 1].set_title('Error Rate Over Time')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Active Users\n",
    "    axes[1, 0].plot(metrics_df['timestamp'], metrics_df['active_users'], \n",
    "                    linewidth=1.5, color='#4CAF50')\n",
    "    axes[1, 0].fill_between(metrics_df['timestamp'], metrics_df['active_users'], \n",
    "                            alpha=0.3, color='#4CAF50')\n",
    "    axes[1, 0].set_xlabel('Time')\n",
    "    axes[1, 0].set_ylabel('Active Users')\n",
    "    axes[1, 0].set_title('Active Users Over Time')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Resource Usage\n",
    "    axes[1, 1].plot(metrics_df['timestamp'], metrics_df['cpu_usage'], \n",
    "                    linewidth=1.5, label='CPU Usage (%)', color='#FF9800')\n",
    "    axes[1, 1].plot(metrics_df['timestamp'], metrics_df['memory_usage'], \n",
    "                    linewidth=1.5, label='Memory Usage (%)', color='#9C27B0')\n",
    "    axes[1, 1].set_xlabel('Time')\n",
    "    axes[1, 1].set_ylabel('Usage (%)')\n",
    "    axes[1, 1].set_title('System Resource Usage')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Rotate x-axis labels\n",
    "    for ax in axes.flat:\n",
    "        ax.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nüìä Monitoring dashboard visualized!\")\n",
    "\n",
    "visualize_monitoring_dashboard(monitoring_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üéØ Practice Complete!\n",
    "\n",
    "### Summary of What We Learned:\n",
    "\n",
    "#### 1. **System Design & Planning** ‚úÖ\n",
    "- Requirements analysis and validation\n",
    "- Data quality checks\n",
    "- Preprocessing pipeline design\n",
    "\n",
    "#### 2. **Implementation** ‚úÖ\n",
    "- Model training with Random Forest\n",
    "- Comprehensive evaluation metrics\n",
    "- API function implementation\n",
    "\n",
    "#### 3. **Deployment & Validation** ‚úÖ\n",
    "- Model serialization and loading\n",
    "- Performance testing (latency & throughput)\n",
    "- Monitoring dashboard simulation\n",
    "\n",
    "### Key Achievements:\n",
    "\n",
    "| Metric | Target | Achieved |\n",
    "|--------|--------|----------|\n",
    "| Accuracy | > 90% | ‚úÖ Check your results |\n",
    "| Response Time | < 5s | ‚úÖ ~200-500ms |\n",
    "| Error Rate | < 1% | ‚úÖ ~0.5% |\n",
    "| System Uptime | > 99% | ‚úÖ Monitored |\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. **Advanced Features**:\n",
    "   - Deep learning models (CNN for medical images)\n",
    "   - Transfer learning with pre-trained models\n",
    "   - Ensemble methods\n",
    "\n",
    "2. **Production Deployment**:\n",
    "   - Docker containerization\n",
    "   - Kubernetes orchestration\n",
    "   - CI/CD pipeline setup\n",
    "\n",
    "3. **Clinical Validation**:\n",
    "   - Real medical dataset testing\n",
    "   - Expert physician comparison\n",
    "   - Regulatory compliance (FDA/CE)\n",
    "\n",
    "4. **User Interface**:\n",
    "   - React.js frontend development\n",
    "   - Real-time prediction visualization\n",
    "   - User feedback integration\n",
    "\n",
    "### üéì Congratulations!\n",
    "\n",
    "You've successfully completed a hands-on implementation of a medical AI system, covering the entire pipeline from data preprocessing to deployment and monitoring!\n",
    "\n",
    "---\n",
    "\n",
    "### üìö Additional Resources:\n",
    "\n",
    "- **Documentation**: scikit-learn, TensorFlow, PyTorch\n",
    "- **Medical AI**: NIH Medical Imaging datasets\n",
    "- **Deployment**: FastAPI, Docker, Kubernetes documentation\n",
    "- **Monitoring**: Prometheus, Grafana tutorials\n",
    "\n",
    "---\n",
    "\n",
    "**Author**: Lecture 20 - Capstone Project  \n",
    "**Contact**: homin.park@ghent.ac.kr, powersimmani@gmail.com  \n",
    "**Course**: Introduction to Biomedical Datascience"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
