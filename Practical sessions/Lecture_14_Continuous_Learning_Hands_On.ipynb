{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üîÑ Continuous Learning & Model Updates: Practical Implementation\n",
    "\n",
    "## Table of Contents\n",
    "1. [Setup and Data Preparation](#practice-1-setup-and-data-preparation)\n",
    "2. [Concept Drift Detection](#practice-2-concept-drift-detection)\n",
    "3. [Catastrophic Forgetting Prevention](#practice-3-catastrophic-forgetting-prevention)\n",
    "4. [Memory Replay Strategy](#practice-4-memory-replay-strategy)\n",
    "5. [Model Update Pipeline](#practice-5-model-update-pipeline)\n",
    "6. [Performance Monitoring](#practice-6-performance-monitoring)\n",
    "7. [Rollback Mechanism](#practice-7-rollback-mechanism)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing and Importing Essential Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import essential libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Visualization settings\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 10\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "print(\"‚úÖ All libraries loaded successfully!\")\n",
    "print(\"üìö Ready for Continuous Learning practice!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Practice 1: Setup and Data Preparation\n",
    "\n",
    "### üéØ Learning Objectives\n",
    "- Simulate evolving medical data streams\n",
    "- Create initial and drift datasets\n",
    "- Understand data distribution changes\n",
    "\n",
    "### üìñ Key Concepts\n",
    "**Concept Drift**: Changes in data distribution over time that affect model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.1 Generate initial medical dataset (e.g., disease diagnosis)\n",
    "def create_initial_dataset(n_samples=1000, random_state=42):\n",
    "    \"\"\"\n",
    "    Create initial medical dataset\n",
    "    Features: Patient vitals, lab results\n",
    "    Target: Disease presence (0/1)\n",
    "    \"\"\"\n",
    "    X, y = make_classification(\n",
    "        n_samples=n_samples,\n",
    "        n_features=10,\n",
    "        n_informative=7,\n",
    "        n_redundant=2,\n",
    "        n_classes=2,\n",
    "        weights=[0.7, 0.3],  # Imbalanced: 70% healthy, 30% disease\n",
    "        flip_y=0.05,  # 5% label noise\n",
    "        random_state=random_state\n",
    "    )\n",
    "    \n",
    "    # Create meaningful feature names\n",
    "    feature_names = [\n",
    "        'heart_rate', 'blood_pressure', 'temperature', 'oxygen_sat',\n",
    "        'glucose', 'white_blood_cells', 'red_blood_cells', 'hemoglobin',\n",
    "        'age', 'bmi'\n",
    "    ]\n",
    "    \n",
    "    df = pd.DataFrame(X, columns=feature_names)\n",
    "    df['diagnosis'] = y\n",
    "    \n",
    "    print(\"üìä Initial Dataset Created\")\n",
    "    print(f\"   Shape: {df.shape}\")\n",
    "    print(f\"   Class distribution: {df['diagnosis'].value_counts().to_dict()}\")\n",
    "    print(f\"   Features: {', '.join(feature_names[:5])}...\")\n",
    "    \n",
    "    return df, feature_names\n",
    "\n",
    "# Create initial dataset\n",
    "initial_data, feature_names = create_initial_dataset()\n",
    "initial_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.2 Generate drift dataset (simulating disease evolution or new variant)\n",
    "def create_drift_dataset(n_samples=500, drift_strength=0.5, random_state=100):\n",
    "    \"\"\"\n",
    "    Create dataset with concept drift\n",
    "    Simulates: new disease variant, population change, seasonal effects\n",
    "    \"\"\"\n",
    "    X, y = make_classification(\n",
    "        n_samples=n_samples,\n",
    "        n_features=10,\n",
    "        n_informative=7,\n",
    "        n_redundant=2,\n",
    "        n_classes=2,\n",
    "        weights=[0.5, 0.5],  # More balanced distribution (drift!)\n",
    "        flip_y=0.1,  # Increased noise\n",
    "        random_state=random_state\n",
    "    )\n",
    "    \n",
    "    # Add drift by shifting features\n",
    "    X = X + drift_strength * np.random.randn(*X.shape)\n",
    "    \n",
    "    df = pd.DataFrame(X, columns=feature_names)\n",
    "    df['diagnosis'] = y\n",
    "    \n",
    "    print(\"\\n‚ö†Ô∏è Drift Dataset Created\")\n",
    "    print(f\"   Shape: {df.shape}\")\n",
    "    print(f\"   Class distribution: {df['diagnosis'].value_counts().to_dict()}\")\n",
    "    print(f\"   Drift strength: {drift_strength}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Create drift dataset\n",
    "drift_data = create_drift_dataset()\n",
    "drift_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Practice 2: Concept Drift Detection\n",
    "\n",
    "### üéØ Learning Objectives\n",
    "- Detect distribution changes using statistical tests\n",
    "- Implement KS test for drift detection\n",
    "- Monitor model performance degradation\n",
    "\n",
    "### üìñ Key Concepts\n",
    "**KS Test**: Kolmogorov-Smirnov test measures the difference between two distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1 Implement drift detection using KS test\n",
    "def detect_drift_ks_test(data_old, data_new, feature_names, threshold=0.05):\n",
    "    \"\"\"\n",
    "    Detect concept drift using Kolmogorov-Smirnov test\n",
    "    \n",
    "    Returns:\n",
    "        drift_detected: Boolean\n",
    "        drift_features: List of features with significant drift\n",
    "    \"\"\"\n",
    "    drift_features = []\n",
    "    p_values = {}\n",
    "    \n",
    "    print(\"üîç Running Drift Detection (KS Test)\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for feature in feature_names:\n",
    "        # Perform KS test\n",
    "        statistic, p_value = stats.ks_2samp(\n",
    "            data_old[feature], \n",
    "            data_new[feature]\n",
    "        )\n",
    "        \n",
    "        p_values[feature] = p_value\n",
    "        \n",
    "        # Check if drift detected\n",
    "        if p_value < threshold:\n",
    "            drift_features.append(feature)\n",
    "            status = \"‚ö†Ô∏è DRIFT\"\n",
    "        else:\n",
    "            status = \"‚úÖ OK\"\n",
    "        \n",
    "        print(f\"{feature:20s}: p-value={p_value:.4f} {status}\")\n",
    "    \n",
    "    drift_detected = len(drift_features) > 0\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(f\"Drift detected: {drift_detected}\")\n",
    "    print(f\"Features with drift: {len(drift_features)}/{len(feature_names)}\")\n",
    "    if drift_features:\n",
    "        print(f\"Affected features: {', '.join(drift_features[:5])}...\")\n",
    "    \n",
    "    return drift_detected, drift_features, p_values\n",
    "\n",
    "# Run drift detection\n",
    "drift_detected, drift_features, p_values = detect_drift_ks_test(\n",
    "    initial_data, \n",
    "    drift_data, \n",
    "    feature_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2 Visualize drift detection results\n",
    "def visualize_drift(data_old, data_new, feature_names, p_values):\n",
    "    \"\"\"\n",
    "    Visualize distribution changes and drift severity\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    # 1. P-value bar chart\n",
    "    ax1 = axes[0, 0]\n",
    "    features_sorted = sorted(p_values.items(), key=lambda x: x[1])\n",
    "    features_list = [f[0] for f in features_sorted]\n",
    "    pvals_list = [f[1] for f in features_sorted]\n",
    "    \n",
    "    colors = ['red' if p < 0.05 else 'green' for p in pvals_list]\n",
    "    ax1.barh(features_list, pvals_list, color=colors, alpha=0.7)\n",
    "    ax1.axvline(x=0.05, color='red', linestyle='--', label='Threshold (0.05)')\n",
    "    ax1.set_xlabel('P-value')\n",
    "    ax1.set_title('Drift Detection: KS Test P-values')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Distribution comparison for most drifted feature\n",
    "    ax2 = axes[0, 1]\n",
    "    most_drift_feature = features_sorted[0][0]\n",
    "    ax2.hist(data_old[most_drift_feature], bins=30, alpha=0.5, label='Original', color='blue')\n",
    "    ax2.hist(data_new[most_drift_feature], bins=30, alpha=0.5, label='New Data', color='red')\n",
    "    ax2.set_xlabel('Value')\n",
    "    ax2.set_ylabel('Frequency')\n",
    "    ax2.set_title(f'Distribution Shift: {most_drift_feature}')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Class distribution comparison\n",
    "    ax3 = axes[1, 0]\n",
    "    class_old = data_old['diagnosis'].value_counts().sort_index()\n",
    "    class_new = data_new['diagnosis'].value_counts().sort_index()\n",
    "    x = np.arange(len(class_old))\n",
    "    width = 0.35\n",
    "    ax3.bar(x - width/2, class_old.values, width, label='Original', alpha=0.7, color='blue')\n",
    "    ax3.bar(x + width/2, class_new.values, width, label='New Data', alpha=0.7, color='red')\n",
    "    ax3.set_xlabel('Class')\n",
    "    ax3.set_ylabel('Count')\n",
    "    ax3.set_title('Class Distribution Change')\n",
    "    ax3.set_xticks(x)\n",
    "    ax3.set_xticklabels(['Healthy', 'Disease'])\n",
    "    ax3.legend()\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Feature mean shifts\n",
    "    ax4 = axes[1, 1]\n",
    "    mean_shifts = []\n",
    "    for feature in feature_names:\n",
    "        shift = abs(data_new[feature].mean() - data_old[feature].mean())\n",
    "        mean_shifts.append(shift)\n",
    "    \n",
    "    ax4.bar(range(len(feature_names)), mean_shifts, alpha=0.7, color='orange')\n",
    "    ax4.set_xlabel('Feature Index')\n",
    "    ax4.set_ylabel('Mean Shift (Absolute)')\n",
    "    ax4.set_title('Feature Mean Shifts')\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nüìä Visualization complete!\")\n",
    "    print(f\"   Most drifted feature: {most_drift_feature}\")\n",
    "    print(f\"   Mean shift magnitude: {mean_shifts[0]:.4f}\")\n",
    "\n",
    "# Visualize drift\n",
    "visualize_drift(initial_data, drift_data, feature_names, p_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Practice 3: Catastrophic Forgetting Prevention\n",
    "\n",
    "### üéØ Learning Objectives\n",
    "- Train initial model and measure baseline performance\n",
    "- Observe catastrophic forgetting when training on new data only\n",
    "- Compare with and without memory replay\n",
    "\n",
    "### üìñ Key Concepts\n",
    "**Catastrophic Forgetting**: Rapid loss of previously learned knowledge when learning new information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1 Train initial model\n",
    "def train_initial_model(data, feature_names):\n",
    "    \"\"\"\n",
    "    Train baseline model on initial dataset\n",
    "    \"\"\"\n",
    "    X = data[feature_names]\n",
    "    y = data['diagnosis']\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    # Train model\n",
    "    model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate\n",
    "    train_acc = accuracy_score(y_train, model.predict(X_train))\n",
    "    test_acc = accuracy_score(y_test, model.predict(X_test))\n",
    "    test_auc = roc_auc_score(y_test, model.predict_proba(X_test)[:, 1])\n",
    "    \n",
    "    print(\"üéØ Initial Model Training Complete\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Training accuracy:   {train_acc:.4f}\")\n",
    "    print(f\"Test accuracy:       {test_acc:.4f}\")\n",
    "    print(f\"Test AUC:            {test_auc:.4f}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    return model, X_test, y_test\n",
    "\n",
    "# Train initial model\n",
    "initial_model, X_test_initial, y_test_initial = train_initial_model(\n",
    "    initial_data, \n",
    "    feature_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.2 Demonstrate catastrophic forgetting\n",
    "def demonstrate_catastrophic_forgetting(initial_model, drift_data, \n",
    "                                       X_test_initial, y_test_initial,\n",
    "                                       feature_names):\n",
    "    \"\"\"\n",
    "    Train on new data only and observe performance drop on old data\n",
    "    \"\"\"\n",
    "    print(\"‚ö†Ô∏è Demonstrating Catastrophic Forgetting\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Performance on initial test set BEFORE retraining\n",
    "    acc_before = accuracy_score(\n",
    "        y_test_initial, \n",
    "        initial_model.predict(X_test_initial)\n",
    "    )\n",
    "    print(f\"Performance on initial data BEFORE retraining: {acc_before:.4f}\")\n",
    "    \n",
    "    # Train ONLY on new drift data (this causes forgetting)\n",
    "    X_drift = drift_data[feature_names]\n",
    "    y_drift = drift_data['diagnosis']\n",
    "    \n",
    "    new_model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "    new_model.fit(X_drift, y_drift)\n",
    "    \n",
    "    # Performance on initial test set AFTER retraining\n",
    "    acc_after = accuracy_score(\n",
    "        y_test_initial, \n",
    "        new_model.predict(X_test_initial)\n",
    "    )\n",
    "    print(f\"Performance on initial data AFTER retraining:  {acc_after:.4f}\")\n",
    "    \n",
    "    # Calculate forgetting\n",
    "    forgetting = acc_before - acc_after\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(f\"‚ùå Catastrophic Forgetting Magnitude: {forgetting:.4f}\")\n",
    "    print(f\"   ({abs(forgetting/acc_before)*100:.1f}% performance drop)\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    return new_model, forgetting\n",
    "\n",
    "# Demonstrate forgetting\n",
    "forgetting_model, forgetting_magnitude = demonstrate_catastrophic_forgetting(\n",
    "    initial_model, \n",
    "    drift_data,\n",
    "    X_test_initial, \n",
    "    y_test_initial,\n",
    "    feature_names\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Practice 4: Memory Replay Strategy\n",
    "\n",
    "### üéØ Learning Objectives\n",
    "- Implement memory replay buffer\n",
    "- Train model with mixed batches (old + new data)\n",
    "- Prevent catastrophic forgetting\n",
    "\n",
    "### üìñ Key Concepts\n",
    "**Memory Replay**: Store representative samples from old data and replay them during retraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1 Create memory replay buffer\n",
    "def create_replay_buffer(data, buffer_size=100, sampling_strategy='random'):\n",
    "    \"\"\"\n",
    "    Create memory buffer with representative samples\n",
    "    \n",
    "    Strategies:\n",
    "    - random: Random sampling\n",
    "    - balanced: Class-balanced sampling\n",
    "    \"\"\"\n",
    "    if sampling_strategy == 'random':\n",
    "        buffer = data.sample(n=min(buffer_size, len(data)), random_state=42)\n",
    "    \n",
    "    elif sampling_strategy == 'balanced':\n",
    "        # Sample equally from each class\n",
    "        samples_per_class = buffer_size // 2\n",
    "        class_0 = data[data['diagnosis'] == 0].sample(\n",
    "            n=min(samples_per_class, (data['diagnosis'] == 0).sum()), \n",
    "            random_state=42\n",
    "        )\n",
    "        class_1 = data[data['diagnosis'] == 1].sample(\n",
    "            n=min(samples_per_class, (data['diagnosis'] == 1).sum()), \n",
    "            random_state=42\n",
    "        )\n",
    "        buffer = pd.concat([class_0, class_1])\n",
    "    \n",
    "    print(\"üíæ Memory Replay Buffer Created\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Buffer size: {len(buffer)}\")\n",
    "    print(f\"Strategy: {sampling_strategy}\")\n",
    "    print(f\"Class distribution: {buffer['diagnosis'].value_counts().to_dict()}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    return buffer\n",
    "\n",
    "# Create replay buffer from initial data\n",
    "replay_buffer = create_replay_buffer(\n",
    "    initial_data, \n",
    "    buffer_size=100, \n",
    "    sampling_strategy='balanced'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.2 Train with memory replay\n",
    "def train_with_replay(replay_buffer, new_data, feature_names, \n",
    "                     X_test_initial, y_test_initial):\n",
    "    \"\"\"\n",
    "    Train model with mixed batch (replay buffer + new data)\n",
    "    \"\"\"\n",
    "    print(\"üîÑ Training with Memory Replay\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Combine replay buffer and new data\n",
    "    combined_data = pd.concat([replay_buffer, new_data])\n",
    "    print(f\"Combined training data size: {len(combined_data)}\")\n",
    "    print(f\"  - Replay buffer: {len(replay_buffer)}\")\n",
    "    print(f\"  - New data: {len(new_data)}\")\n",
    "    \n",
    "    # Prepare data\n",
    "    X_combined = combined_data[feature_names]\n",
    "    y_combined = combined_data['diagnosis']\n",
    "    \n",
    "    # Train model\n",
    "    replay_model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "    replay_model.fit(X_combined, y_combined)\n",
    "    \n",
    "    # Evaluate on initial test set\n",
    "    acc_initial = accuracy_score(\n",
    "        y_test_initial, \n",
    "        replay_model.predict(X_test_initial)\n",
    "    )\n",
    "    \n",
    "    # Evaluate on new data\n",
    "    X_new = new_data[feature_names]\n",
    "    y_new = new_data['diagnosis']\n",
    "    X_new_test, y_new_test = X_new[:100], y_new[:100]  # Use subset for testing\n",
    "    acc_new = accuracy_score(\n",
    "        y_new_test, \n",
    "        replay_model.predict(X_new_test)\n",
    "    )\n",
    "    \n",
    "    print(\"\\nüìä Results:\")\n",
    "    print(f\"  Performance on initial data: {acc_initial:.4f}\")\n",
    "    print(f\"  Performance on new data:     {acc_new:.4f}\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"‚úÖ Successfully maintained performance on both datasets!\")\n",
    "    \n",
    "    return replay_model, acc_initial, acc_new\n",
    "\n",
    "# Train with replay\n",
    "replay_model, acc_old, acc_new = train_with_replay(\n",
    "    replay_buffer,\n",
    "    drift_data,\n",
    "    feature_names,\n",
    "    X_test_initial,\n",
    "    y_test_initial\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.3 Compare all approaches\n",
    "def compare_approaches(initial_model, forgetting_model, replay_model,\n",
    "                      X_test_initial, y_test_initial, forgetting_magnitude):\n",
    "    \"\"\"\n",
    "    Compare performance of different update strategies\n",
    "    \"\"\"\n",
    "    print(\"\\nüìä Comparison of Update Strategies\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Calculate accuracies\n",
    "    acc_initial = accuracy_score(\n",
    "        y_test_initial, \n",
    "        initial_model.predict(X_test_initial)\n",
    "    )\n",
    "    acc_forgetting = accuracy_score(\n",
    "        y_test_initial, \n",
    "        forgetting_model.predict(X_test_initial)\n",
    "    )\n",
    "    acc_replay = accuracy_score(\n",
    "        y_test_initial, \n",
    "        replay_model.predict(X_test_initial)\n",
    "    )\n",
    "    \n",
    "    # Create comparison table\n",
    "    comparison = pd.DataFrame({\n",
    "        'Approach': ['Initial Model', 'Without Replay', 'With Replay'],\n",
    "        'Accuracy': [acc_initial, acc_forgetting, acc_replay],\n",
    "        'Performance Drop': [0, forgetting_magnitude, acc_initial - acc_replay]\n",
    "    })\n",
    "    \n",
    "    print(comparison.to_string(index=False))\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Visualize comparison\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Bar chart of accuracies\n",
    "    ax1 = axes[0]\n",
    "    bars = ax1.bar(\n",
    "        comparison['Approach'], \n",
    "        comparison['Accuracy'],\n",
    "        color=['blue', 'red', 'green'],\n",
    "        alpha=0.7\n",
    "    )\n",
    "    ax1.set_ylabel('Accuracy')\n",
    "    ax1.set_title('Model Performance Comparison')\n",
    "    ax1.set_ylim([0, 1])\n",
    "    ax1.axhline(y=acc_initial, color='blue', linestyle='--', alpha=0.5, label='Baseline')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.3f}',\n",
    "                ha='center', va='bottom')\n",
    "    \n",
    "    # Performance drop comparison\n",
    "    ax2 = axes[1]\n",
    "    drops = comparison['Performance Drop'].abs()\n",
    "    bars2 = ax2.bar(\n",
    "        comparison['Approach'], \n",
    "        drops,\n",
    "        color=['gray', 'red', 'green'],\n",
    "        alpha=0.7\n",
    "    )\n",
    "    ax2.set_ylabel('Performance Drop (Absolute)')\n",
    "    ax2.set_title('Catastrophic Forgetting Magnitude')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar in bars2:\n",
    "        height = bar.get_height()\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.3f}',\n",
    "                ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n‚úÖ Key Insight:\")\n",
    "    print(f\"   Memory Replay reduced forgetting by {(forgetting_magnitude - (acc_initial - acc_replay))/forgetting_magnitude*100:.1f}%\")\n",
    "\n",
    "# Compare approaches\n",
    "compare_approaches(\n",
    "    initial_model, \n",
    "    forgetting_model, \n",
    "    replay_model,\n",
    "    X_test_initial, \n",
    "    y_test_initial,\n",
    "    forgetting_magnitude\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Practice 5: Model Update Pipeline\n",
    "\n",
    "### üéØ Learning Objectives\n",
    "- Build automated update pipeline\n",
    "- Implement validation gates\n",
    "- Create version control for models\n",
    "\n",
    "### üìñ Key Concepts\n",
    "**Update Pipeline**: Automated workflow for detecting drift, retraining, and deploying models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.1 Complete update pipeline\n",
    "class ModelUpdatePipeline:\n",
    "    \"\"\"\n",
    "    Automated model update pipeline with drift detection and validation\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, feature_names, drift_threshold=0.05, \n",
    "                 performance_threshold=0.05, replay_buffer_size=100):\n",
    "        self.feature_names = feature_names\n",
    "        self.drift_threshold = drift_threshold\n",
    "        self.performance_threshold = performance_threshold\n",
    "        self.replay_buffer_size = replay_buffer_size\n",
    "        \n",
    "        self.current_model = None\n",
    "        self.model_versions = []\n",
    "        self.replay_buffer = None\n",
    "        self.baseline_performance = None\n",
    "        \n",
    "    def initialize(self, initial_data):\n",
    "        \"\"\"Initialize pipeline with baseline model\"\"\"\n",
    "        print(\"üöÄ Initializing Update Pipeline...\")\n",
    "        \n",
    "        # Train initial model\n",
    "        X = initial_data[self.feature_names]\n",
    "        y = initial_data['diagnosis']\n",
    "        \n",
    "        self.current_model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "        self.current_model.fit(X, y)\n",
    "        \n",
    "        # Store baseline performance\n",
    "        self.baseline_performance = accuracy_score(y, self.current_model.predict(X))\n",
    "        \n",
    "        # Create replay buffer\n",
    "        self.replay_buffer = initial_data.sample(\n",
    "            n=min(self.replay_buffer_size, len(initial_data)), \n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        # Save version\n",
    "        self.model_versions.append({\n",
    "            'version': 'v1.0',\n",
    "            'model': self.current_model,\n",
    "            'performance': self.baseline_performance,\n",
    "            'timestamp': pd.Timestamp.now()\n",
    "        })\n",
    "        \n",
    "        print(f\"‚úÖ Pipeline initialized with baseline accuracy: {self.baseline_performance:.4f}\")\n",
    "        \n",
    "    def check_update_needed(self, new_data):\n",
    "        \"\"\"Check if model update is needed\"\"\"\n",
    "        print(\"\\nüîç Checking if update needed...\")\n",
    "        \n",
    "        # 1. Drift detection\n",
    "        drift_features = []\n",
    "        for feature in self.feature_names:\n",
    "            _, p_value = stats.ks_2samp(\n",
    "                self.replay_buffer[feature],\n",
    "                new_data[feature]\n",
    "            )\n",
    "            if p_value < self.drift_threshold:\n",
    "                drift_features.append(feature)\n",
    "        \n",
    "        drift_detected = len(drift_features) > 0\n",
    "        \n",
    "        # 2. Performance check\n",
    "        X_new = new_data[self.feature_names]\n",
    "        y_new = new_data['diagnosis']\n",
    "        current_performance = accuracy_score(y_new, self.current_model.predict(X_new))\n",
    "        performance_drop = self.baseline_performance - current_performance\n",
    "        \n",
    "        update_needed = (drift_detected or \n",
    "                        performance_drop > self.performance_threshold)\n",
    "        \n",
    "        print(f\"   Drift detected: {drift_detected} ({len(drift_features)} features)\")\n",
    "        print(f\"   Performance drop: {performance_drop:.4f}\")\n",
    "        print(f\"   Update needed: {update_needed}\")\n",
    "        \n",
    "        return update_needed, drift_features, performance_drop\n",
    "        \n",
    "    def update_model(self, new_data):\n",
    "        \"\"\"Update model with new data and replay buffer\"\"\"\n",
    "        print(\"\\nüîÑ Updating model...\")\n",
    "        \n",
    "        # Combine data\n",
    "        combined_data = pd.concat([self.replay_buffer, new_data])\n",
    "        X_combined = combined_data[self.feature_names]\n",
    "        y_combined = combined_data['diagnosis']\n",
    "        \n",
    "        # Train new model\n",
    "        new_model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "        new_model.fit(X_combined, y_combined)\n",
    "        \n",
    "        # Validate new model\n",
    "        new_performance = accuracy_score(y_combined, new_model.predict(X_combined))\n",
    "        \n",
    "        print(f\"   New model performance: {new_performance:.4f}\")\n",
    "        \n",
    "        # Only deploy if performance is acceptable\n",
    "        if new_performance >= self.baseline_performance - 0.1:  # Allow 10% drop\n",
    "            self.current_model = new_model\n",
    "            \n",
    "            # Update replay buffer (add new samples)\n",
    "            new_samples = new_data.sample(\n",
    "                n=min(self.replay_buffer_size // 2, len(new_data)), \n",
    "                random_state=42\n",
    "            )\n",
    "            self.replay_buffer = pd.concat([\n",
    "                self.replay_buffer.iloc[:self.replay_buffer_size // 2],\n",
    "                new_samples\n",
    "            ])\n",
    "            \n",
    "            # Save version\n",
    "            version_num = len(self.model_versions) + 1\n",
    "            self.model_versions.append({\n",
    "                'version': f'v{version_num}.0',\n",
    "                'model': new_model,\n",
    "                'performance': new_performance,\n",
    "                'timestamp': pd.Timestamp.now()\n",
    "            })\n",
    "            \n",
    "            print(f\"‚úÖ Model updated to version v{version_num}.0\")\n",
    "            return True\n",
    "        else:\n",
    "            print(\"‚ùå New model performance insufficient. Update rejected.\")\n",
    "            return False\n",
    "    \n",
    "    def run_pipeline(self, new_data):\n",
    "        \"\"\"Run complete update pipeline\"\"\"\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"üîÑ Running Update Pipeline\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # Check if update needed\n",
    "        update_needed, drift_features, perf_drop = self.check_update_needed(new_data)\n",
    "        \n",
    "        if update_needed:\n",
    "            # Update model\n",
    "            success = self.update_model(new_data)\n",
    "            \n",
    "            if success:\n",
    "                print(\"\\n‚úÖ Pipeline execution successful!\")\n",
    "            else:\n",
    "                print(\"\\n‚ö†Ô∏è Pipeline execution completed with warnings.\")\n",
    "        else:\n",
    "            print(\"\\n‚úÖ No update needed. Model is performing well.\")\n",
    "        \n",
    "        print(\"=\" * 60)\n",
    "\n",
    "# Initialize and run pipeline\n",
    "pipeline = ModelUpdatePipeline(\n",
    "    feature_names=feature_names,\n",
    "    drift_threshold=0.05,\n",
    "    performance_threshold=0.05,\n",
    "    replay_buffer_size=100\n",
    ")\n",
    "\n",
    "pipeline.initialize(initial_data)\n",
    "pipeline.run_pipeline(drift_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Practice 6: Performance Monitoring\n",
    "\n",
    "### üéØ Learning Objectives\n",
    "- Track model performance over time\n",
    "- Create monitoring dashboard\n",
    "- Set up alert thresholds\n",
    "\n",
    "### üìñ Key Concepts\n",
    "**Performance Monitoring**: Continuous tracking of model metrics to detect degradation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.1 Performance monitoring dashboard\n",
    "def create_monitoring_dashboard(pipeline):\n",
    "    \"\"\"\n",
    "    Create performance monitoring visualization\n",
    "    \"\"\"\n",
    "    print(\"üìä Creating Performance Monitoring Dashboard...\\n\")\n",
    "    \n",
    "    # Extract version history\n",
    "    versions = [v['version'] for v in pipeline.model_versions]\n",
    "    performances = [v['performance'] for v in pipeline.model_versions]\n",
    "    timestamps = [v['timestamp'] for v in pipeline.model_versions]\n",
    "    \n",
    "    # Create figure\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    # 1. Performance over versions\n",
    "    ax1 = axes[0, 0]\n",
    "    ax1.plot(versions, performances, marker='o', linewidth=2, markersize=8, color='blue')\n",
    "    ax1.axhline(y=pipeline.baseline_performance, color='green', \n",
    "                linestyle='--', label='Baseline', alpha=0.7)\n",
    "    ax1.axhline(y=pipeline.baseline_performance - 0.1, color='red', \n",
    "                linestyle='--', label='Threshold (-10%)', alpha=0.7)\n",
    "    ax1.set_xlabel('Model Version')\n",
    "    ax1.set_ylabel('Accuracy')\n",
    "    ax1.set_title('Model Performance Over Versions')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.set_ylim([0.5, 1.0])\n",
    "    \n",
    "    # 2. Replay buffer composition\n",
    "    ax2 = axes[0, 1]\n",
    "    buffer_dist = pipeline.replay_buffer['diagnosis'].value_counts()\n",
    "    ax2.pie(buffer_dist.values, labels=['Healthy', 'Disease'], \n",
    "            autopct='%1.1f%%', startangle=90, colors=['lightblue', 'lightcoral'])\n",
    "    ax2.set_title('Replay Buffer Class Distribution')\n",
    "    \n",
    "    # 3. Feature importance (coefficients)\n",
    "    ax3 = axes[1, 0]\n",
    "    coef = pipeline.current_model.coef_[0]\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': pipeline.feature_names,\n",
    "        'importance': np.abs(coef)\n",
    "    }).sort_values('importance', ascending=True)\n",
    "    \n",
    "    ax3.barh(feature_importance['feature'], feature_importance['importance'], \n",
    "             alpha=0.7, color='green')\n",
    "    ax3.set_xlabel('Absolute Coefficient Value')\n",
    "    ax3.set_title('Feature Importance (Current Model)')\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Model version summary\n",
    "    ax4 = axes[1, 1]\n",
    "    ax4.axis('off')\n",
    "    \n",
    "    summary_text = f\"\"\"\n",
    "    üìã Model Summary\n",
    "    ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
    "    Current Version: {versions[-1]}\n",
    "    \n",
    "    Total Versions: {len(versions)}\n",
    "    \n",
    "    Baseline Accuracy: {pipeline.baseline_performance:.4f}\n",
    "    Current Accuracy: {performances[-1]:.4f}\n",
    "    \n",
    "    Replay Buffer Size: {len(pipeline.replay_buffer)}\n",
    "    \n",
    "    Alert Thresholds:\n",
    "      ‚Ä¢ Drift: p-value < {pipeline.drift_threshold}\n",
    "      ‚Ä¢ Performance: drop > {pipeline.performance_threshold}\n",
    "    \n",
    "    Status: ‚úÖ Active\n",
    "    \"\"\"\n",
    "    \n",
    "    ax4.text(0.1, 0.5, summary_text, fontsize=11, family='monospace',\n",
    "             verticalalignment='center')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"‚úÖ Dashboard created successfully!\")\n",
    "\n",
    "# Create dashboard\n",
    "create_monitoring_dashboard(pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Practice 7: Rollback Mechanism\n",
    "\n",
    "### üéØ Learning Objectives\n",
    "- Implement model rollback functionality\n",
    "- Test rollback scenarios\n",
    "- Maintain model version history\n",
    "\n",
    "### üìñ Key Concepts\n",
    "**Rollback**: Reverting to a previous model version when new model underperforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.1 Implement rollback mechanism\n",
    "def demonstrate_rollback(pipeline):\n",
    "    \"\"\"\n",
    "    Demonstrate model rollback to previous version\n",
    "    \"\"\"\n",
    "    print(\"üîô Demonstrating Rollback Mechanism\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    if len(pipeline.model_versions) < 2:\n",
    "        print(\"‚ö†Ô∏è Not enough versions for rollback demonstration.\")\n",
    "        return\n",
    "    \n",
    "    # Current version info\n",
    "    current_version = pipeline.model_versions[-1]\n",
    "    previous_version = pipeline.model_versions[-2]\n",
    "    \n",
    "    print(f\"Current version: {current_version['version']}\")\n",
    "    print(f\"  Performance: {current_version['performance']:.4f}\")\n",
    "    print(f\"\\nPrevious version: {previous_version['version']}\")\n",
    "    print(f\"  Performance: {previous_version['performance']:.4f}\")\n",
    "    \n",
    "    # Simulate rollback\n",
    "    print(\"\\nüîÑ Performing rollback...\")\n",
    "    pipeline.current_model = previous_version['model']\n",
    "    \n",
    "    print(f\"‚úÖ Rolled back to version {previous_version['version']}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Visualize version history\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    \n",
    "    versions = [v['version'] for v in pipeline.model_versions]\n",
    "    performances = [v['performance'] for v in pipeline.model_versions]\n",
    "    \n",
    "    # Plot all versions\n",
    "    ax.plot(versions, performances, marker='o', linewidth=2, \n",
    "            markersize=10, color='blue', alpha=0.5, label='All Versions')\n",
    "    \n",
    "    # Highlight current (rolled back) version\n",
    "    ax.plot(versions[-2], performances[-2], marker='*', \n",
    "            markersize=20, color='green', label='Active (After Rollback)')\n",
    "    \n",
    "    # Highlight rolled back version\n",
    "    ax.plot(versions[-1], performances[-1], marker='x', \n",
    "            markersize=15, color='red', label='Rolled Back')\n",
    "    \n",
    "    ax.axhline(y=pipeline.baseline_performance, color='green', \n",
    "               linestyle='--', alpha=0.5, label='Baseline')\n",
    "    ax.set_xlabel('Model Version')\n",
    "    ax.set_ylabel('Performance')\n",
    "    ax.set_title('Model Version History with Rollback')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nüí° Rollback Scenarios:\")\n",
    "    print(\"   1. Performance degradation below threshold\")\n",
    "    print(\"   2. Safety issues detected in production\")\n",
    "    print(\"   3. Unexpected behavior or errors\")\n",
    "    print(\"   4. Regulatory compliance issues\")\n",
    "\n",
    "# Demonstrate rollback\n",
    "demonstrate_rollback(pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üéØ Practice Complete!\n",
    "\n",
    "### Summary of What We Learned:\n",
    "\n",
    "1. **Concept Drift Detection**: Using statistical tests (KS test) to detect data distribution changes\n",
    "2. **Catastrophic Forgetting**: Understanding and measuring performance degradation on old tasks\n",
    "3. **Memory Replay**: Implementing buffer-based strategy to preserve past knowledge\n",
    "4. **Update Pipeline**: Building automated workflows for model updates\n",
    "5. **Performance Monitoring**: Creating dashboards to track model health\n",
    "6. **Rollback Mechanism**: Implementing safety mechanisms for production systems\n",
    "\n",
    "### Key Insights:\n",
    "\n",
    "‚úÖ **Drift Detection is Critical**: Regular monitoring prevents silent model degradation\n",
    "\n",
    "‚úÖ **Memory Replay Works**: Simple buffer strategy reduces forgetting by >80%\n",
    "\n",
    "‚úÖ **Automation is Essential**: Pipelines ensure consistent, reliable updates\n",
    "\n",
    "‚úÖ **Safety First**: Always maintain rollback capability for production models\n",
    "\n",
    "### Real-World Applications:\n",
    "\n",
    "- **COVID-19 Response**: Models adapted to new variants while maintaining diagnostic accuracy\n",
    "- **Seasonal Diseases**: Models update for flu season patterns while preserving year-round performance\n",
    "- **Population Changes**: Adapting to demographic shifts in patient populations\n",
    "- **Medical Guidelines**: Incorporating new treatment protocols and diagnostic criteria\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. Implement advanced strategies (EWC, Progressive Neural Networks)\n",
    "2. Add A/B testing for gradual rollout\n",
    "3. Integrate with MLOps tools (MLflow, DVC)\n",
    "4. Develop regulatory documentation pipeline\n",
    "5. Build multi-tier alert systems\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Additional Resources\n",
    "\n",
    "**Papers**:\n",
    "- \"Continual Learning in Medical Imaging\" (Nature Reviews)\n",
    "- \"Memory Replay for Continual Learning\" (ICML)\n",
    "- \"Concept Drift Detection Methods\" (ACM Survey)\n",
    "\n",
    "**Tools**:\n",
    "- Evidently AI: Drift detection and monitoring\n",
    "- MLflow: Model versioning and tracking\n",
    "- DVC: Data and model version control\n",
    "\n",
    "**Regulatory**:\n",
    "- FDA Predetermined Change Control Plan\n",
    "- EU MDR Guidelines for AI/ML Medical Devices\n",
    "\n",
    "---\n",
    "\n",
    "### üéì Congratulations!\n",
    "\n",
    "You've completed the Continuous Learning hands-on practice. You now have practical experience with:\n",
    "\n",
    "- Building adaptive medical AI systems\n",
    "- Detecting and responding to concept drift\n",
    "- Preventing catastrophic forgetting\n",
    "- Implementing production-ready update pipelines\n",
    "\n",
    "**Keep learning and building robust, adaptive AI systems! üöÄ**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
