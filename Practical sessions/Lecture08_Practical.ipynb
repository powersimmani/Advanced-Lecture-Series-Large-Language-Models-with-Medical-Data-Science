{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéØ Constitutional AI & Medical Ethics: Hands-On Practice\n",
    "\n",
    "## üìö Table of Contents\n",
    "1. [Bias Detection in Medical AI](#practice-1-bias-detection-in-medical-ai)\n",
    "2. [Fairness Metrics Calculation](#practice-2-fairness-metrics-calculation)\n",
    "3. [Risk-Benefit Analysis](#practice-3-risk-benefit-analysis)\n",
    "4. [Output Filtering Simulation](#practice-4-output-filtering-simulation)\n",
    "\n",
    "**‚è±Ô∏è Estimated Time: 15 minutes**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing and Importing Essential Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import essential libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Visualization settings\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "print(\"‚úÖ All libraries loaded successfully!\")\n",
    "print(\"üìä Ready for Constitutional AI Ethics Practice\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Practice 1: Bias Detection in Medical AI\n",
    "\n",
    "### üéØ Learning Objectives\n",
    "- Detect performance disparities across demographic groups\n",
    "- Calculate group-specific accuracy metrics\n",
    "- Identify potential bias in AI predictions\n",
    "\n",
    "### üìñ Key Concepts\n",
    "**Demographic Parity:** Equal positive prediction rates across all groups  \n",
    "**Equalized Odds:** Equal TPR and FPR across all groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.1 Generate simulated medical AI predictions\n",
    "def generate_medical_predictions():\n",
    "    \"\"\"Simulate medical AI predictions with potential bias\"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Create dataset with two demographic groups (A and B)\n",
    "    n_samples = 500\n",
    "    \n",
    "    # Group A (e.g., majority population)\n",
    "    group_a_true = np.random.binomial(1, 0.3, n_samples//2)  # 30% disease rate\n",
    "    group_a_pred = np.where(group_a_true == 1, \n",
    "                            np.random.binomial(1, 0.90, n_samples//2),  # 90% sensitivity\n",
    "                            np.random.binomial(1, 0.10, n_samples//2))  # 10% false positive\n",
    "    \n",
    "    # Group B (e.g., minority population) - with bias\n",
    "    group_b_true = np.random.binomial(1, 0.3, n_samples//2)  # Same 30% disease rate\n",
    "    group_b_pred = np.where(group_b_true == 1,\n",
    "                            np.random.binomial(1, 0.70, n_samples//2),  # 70% sensitivity (LOWER!)\n",
    "                            np.random.binomial(1, 0.15, n_samples//2))  # 15% false positive\n",
    "    \n",
    "    # Combine data\n",
    "    df = pd.DataFrame({\n",
    "        'group': ['A']*len(group_a_true) + ['B']*len(group_b_true),\n",
    "        'true_label': np.concatenate([group_a_true, group_b_true]),\n",
    "        'predicted': np.concatenate([group_a_pred, group_b_pred])\n",
    "    })\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Generate data\n",
    "medical_data = generate_medical_predictions()\n",
    "\n",
    "print(\"üìä Medical AI Prediction Data Generated\")\n",
    "print(f\"Total samples: {len(medical_data)}\")\n",
    "print(f\"\\nGroup distribution:\")\n",
    "print(medical_data['group'].value_counts())\n",
    "print(f\"\\nDisease prevalence by group:\")\n",
    "print(medical_data.groupby('group')['true_label'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.2 Calculate performance metrics by group\n",
    "def evaluate_fairness(df):\n",
    "    \"\"\"Evaluate AI fairness across demographic groups\"\"\"\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for group in df['group'].unique():\n",
    "        group_data = df[df['group'] == group]\n",
    "        y_true = group_data['true_label']\n",
    "        y_pred = group_data['predicted']\n",
    "        \n",
    "        # Confusion matrix\n",
    "        tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "        \n",
    "        # Calculate metrics\n",
    "        results[group] = {\n",
    "            'Accuracy': (tp + tn) / (tp + tn + fp + fn),\n",
    "            'TPR (Sensitivity)': tp / (tp + fn) if (tp + fn) > 0 else 0,\n",
    "            'FPR': fp / (fp + tn) if (fp + tn) > 0 else 0,\n",
    "            'PPV (Precision)': tp / (tp + fp) if (tp + fp) > 0 else 0,\n",
    "            'Positive Rate': (tp + fp) / len(group_data)\n",
    "        }\n",
    "    \n",
    "    return pd.DataFrame(results).T\n",
    "\n",
    "# Evaluate fairness\n",
    "fairness_metrics = evaluate_fairness(medical_data)\n",
    "\n",
    "print(\"‚öñÔ∏è Fairness Evaluation Results\")\n",
    "print(\"=\" * 60)\n",
    "print(fairness_metrics.round(3))\n",
    "print(\"\\nüìå Key Observations:\")\n",
    "print(f\"  ‚Ä¢ TPR difference: {abs(fairness_metrics.loc['A', 'TPR (Sensitivity)'] - fairness_metrics.loc['B', 'TPR (Sensitivity)']):.3f}\")\n",
    "print(f\"  ‚Ä¢ Accuracy difference: {abs(fairness_metrics.loc['A', 'Accuracy'] - fairness_metrics.loc['B', 'Accuracy']):.3f}\")\n",
    "\n",
    "if abs(fairness_metrics.loc['A', 'TPR (Sensitivity)'] - fairness_metrics.loc['B', 'TPR (Sensitivity)']) > 0.1:\n",
    "    print(\"\\n‚ö†Ô∏è WARNING: Significant bias detected! Group B has lower sensitivity.\")\n",
    "else:\n",
    "    print(\"\\n‚úÖ No significant bias detected.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.3 Visualize bias\n",
    "def visualize_bias(metrics_df):\n",
    "    \"\"\"Visualize performance disparities across groups\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Plot 1: Bar chart comparison\n",
    "    metrics_to_plot = ['Accuracy', 'TPR (Sensitivity)', 'PPV (Precision)']\n",
    "    metrics_df[metrics_to_plot].plot(kind='bar', ax=axes[0], rot=0, width=0.7)\n",
    "    axes[0].set_title('Performance Metrics by Group', fontsize=14, fontweight='bold')\n",
    "    axes[0].set_ylabel('Score')\n",
    "    axes[0].set_ylim(0, 1)\n",
    "    axes[0].legend(loc='lower right')\n",
    "    axes[0].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Gap visualization\n",
    "    gaps = metrics_df.loc['A'] - metrics_df.loc['B']\n",
    "    colors = ['red' if x > 0.05 else 'green' for x in gaps]\n",
    "    axes[1].barh(gaps.index, gaps.values, color=colors, alpha=0.7)\n",
    "    axes[1].axvline(x=0, color='black', linestyle='--', linewidth=1)\n",
    "    axes[1].axvline(x=0.05, color='orange', linestyle=':', linewidth=1, label='5% threshold')\n",
    "    axes[1].axvline(x=-0.05, color='orange', linestyle=':', linewidth=1)\n",
    "    axes[1].set_title('Performance Gap (Group A - Group B)', fontsize=14, fontweight='bold')\n",
    "    axes[1].set_xlabel('Difference')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_bias(fairness_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Practice 2: Fairness Metrics Calculation\n",
    "\n",
    "### üéØ Learning Objectives\n",
    "- Calculate Disparate Impact Ratio (4/5ths rule)\n",
    "- Understand different fairness definitions\n",
    "- Apply fairness thresholds\n",
    "\n",
    "### üìñ Key Concepts\n",
    "**Disparate Impact Ratio:** P(≈∂=1|Group=B) / P(≈∂=1|Group=A)  \n",
    "**4/5ths Rule:** Ratio should be ‚â• 0.8 to be considered fair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1 Calculate Disparate Impact Ratio\n",
    "def calculate_disparate_impact(df):\n",
    "    \"\"\"Calculate disparate impact ratio and apply 4/5ths rule\"\"\"\n",
    "    \n",
    "    # Positive prediction rates by group\n",
    "    positive_rates = df.groupby('group')['predicted'].mean()\n",
    "    \n",
    "    print(\"üìä Disparate Impact Analysis\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"\\nPositive Prediction Rates:\")\n",
    "    for group in positive_rates.index:\n",
    "        print(f\"  Group {group}: {positive_rates[group]:.3f} ({positive_rates[group]*100:.1f}%)\")\n",
    "    \n",
    "    # Calculate Disparate Impact Ratio\n",
    "    dir_ratio = positive_rates['B'] / positive_rates['A']\n",
    "    \n",
    "    print(f\"\\n‚öñÔ∏è Disparate Impact Ratio: {dir_ratio:.3f}\")\n",
    "    print(f\"   Formula: P(≈∂=1|B) / P(≈∂=1|A) = {positive_rates['B']:.3f} / {positive_rates['A']:.3f}\")\n",
    "    \n",
    "    # Apply 4/5ths rule\n",
    "    threshold = 0.8\n",
    "    print(f\"\\nüìè 4/5ths Rule Threshold: {threshold}\")\n",
    "    \n",
    "    if dir_ratio >= threshold:\n",
    "        print(f\"‚úÖ PASS: Ratio {dir_ratio:.3f} ‚â• {threshold} - No disparate impact detected\")\n",
    "    else:\n",
    "        print(f\"‚ùå FAIL: Ratio {dir_ratio:.3f} < {threshold} - Disparate impact detected!\")\n",
    "        print(f\"   ‚ö†Ô∏è Group B receives {(1-dir_ratio)*100:.1f}% fewer positive predictions than Group A\")\n",
    "    \n",
    "    return dir_ratio, positive_rates\n",
    "\n",
    "di_ratio, pos_rates = calculate_disparate_impact(medical_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2 Compare multiple fairness definitions\n",
    "def compare_fairness_definitions(metrics_df):\n",
    "    \"\"\"Compare different fairness criteria\"\"\"\n",
    "    \n",
    "    print(\"‚öñÔ∏è Fairness Criteria Comparison\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # 1. Demographic Parity\n",
    "    pos_rate_diff = abs(metrics_df.loc['A', 'Positive Rate'] - metrics_df.loc['B', 'Positive Rate'])\n",
    "    print(f\"\\n1Ô∏è‚É£ Demographic Parity\")\n",
    "    print(f\"   Positive rate difference: {pos_rate_diff:.3f}\")\n",
    "    print(f\"   Status: {'‚úÖ PASS' if pos_rate_diff < 0.05 else '‚ùå FAIL'} (threshold: 0.05)\")\n",
    "    \n",
    "    # 2. Equalized Odds\n",
    "    tpr_diff = abs(metrics_df.loc['A', 'TPR (Sensitivity)'] - metrics_df.loc['B', 'TPR (Sensitivity)'])\n",
    "    fpr_diff = abs(metrics_df.loc['A', 'FPR'] - metrics_df.loc['B', 'FPR'])\n",
    "    print(f\"\\n2Ô∏è‚É£ Equalized Odds\")\n",
    "    print(f\"   TPR difference: {tpr_diff:.3f}\")\n",
    "    print(f\"   FPR difference: {fpr_diff:.3f}\")\n",
    "    print(f\"   Status: {'‚úÖ PASS' if (tpr_diff < 0.1 and fpr_diff < 0.1) else '‚ùå FAIL'} (threshold: 0.1 each)\")\n",
    "    \n",
    "    # 3. Predictive Parity\n",
    "    ppv_diff = abs(metrics_df.loc['A', 'PPV (Precision)'] - metrics_df.loc['B', 'PPV (Precision)'])\n",
    "    print(f\"\\n3Ô∏è‚É£ Predictive Parity\")\n",
    "    print(f\"   PPV difference: {ppv_diff:.3f}\")\n",
    "    print(f\"   Status: {'‚úÖ PASS' if ppv_diff < 0.1 else '‚ùå FAIL'} (threshold: 0.1)\")\n",
    "    \n",
    "    # Summary\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"üí° Key Insight: It's mathematically impossible to satisfy all\")\n",
    "    print(\"   fairness criteria simultaneously (Fairness Impossibility Theorem)\")\n",
    "\n",
    "compare_fairness_definitions(fairness_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Practice 3: Risk-Benefit Analysis\n",
    "\n",
    "### üéØ Learning Objectives\n",
    "- Assess AI system risks and benefits\n",
    "- Create risk matrices\n",
    "- Make deployment decisions\n",
    "\n",
    "### üìñ Key Concepts\n",
    "**Risk Matrix:** Maps likelihood vs impact to categorize risks  \n",
    "**Benefit-Risk Ratio:** Weighs potential benefits against potential harms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1 Risk-Benefit Assessment\n",
    "def assess_risk_benefit():\n",
    "    \"\"\"Perform systematic risk-benefit analysis\"\"\"\n",
    "    \n",
    "    # Define scenarios\n",
    "    scenarios = {\n",
    "        'Misdiagnosis': {'likelihood': 'Medium', 'impact': 'High', 'score': 6},\n",
    "        'Privacy Breach': {'likelihood': 'Low', 'impact': 'High', 'score': 3},\n",
    "        'Biased Recommendation': {'likelihood': 'High', 'impact': 'Medium', 'score': 6},\n",
    "        'System Downtime': {'likelihood': 'Low', 'impact': 'Low', 'score': 1}\n",
    "    }\n",
    "    \n",
    "    benefits = {\n",
    "        'Early Detection': {'magnitude': 'High', 'score': 9},\n",
    "        'Cost Reduction': {'magnitude': 'Medium', 'score': 6},\n",
    "        'Accessibility': {'magnitude': 'High', 'score': 8},\n",
    "        'Efficiency': {'magnitude': 'Medium', 'score': 5}\n",
    "    }\n",
    "    \n",
    "    print(\"‚öñÔ∏è Risk-Benefit Analysis Matrix\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Calculate scores\n",
    "    total_risk = sum(s['score'] for s in scenarios.values())\n",
    "    total_benefit = sum(b['score'] for b in benefits.values())\n",
    "    benefit_risk_ratio = total_benefit / total_risk\n",
    "    \n",
    "    print(\"\\nüìä Risk Assessment:\")\n",
    "    for risk, details in scenarios.items():\n",
    "        print(f\"  ‚Ä¢ {risk:25s} | Likelihood: {details['likelihood']:6s} | Impact: {details['impact']:6s} | Score: {details['score']}\")\n",
    "    print(f\"\\n  Total Risk Score: {total_risk}\")\n",
    "    \n",
    "    print(\"\\nüíö Benefit Assessment:\")\n",
    "    for benefit, details in benefits.items():\n",
    "        print(f\"  ‚Ä¢ {benefit:25s} | Magnitude: {details['magnitude']:6s} | Score: {details['score']}\")\n",
    "    print(f\"\\n  Total Benefit Score: {total_benefit}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(f\"üìà Benefit-Risk Ratio: {benefit_risk_ratio:.2f}\")\n",
    "    \n",
    "    # Decision\n",
    "    if benefit_risk_ratio > 2.0:\n",
    "        decision = \"‚úÖ RECOMMEND DEPLOYMENT\"\n",
    "        note = \"Benefits significantly outweigh risks\"\n",
    "    elif benefit_risk_ratio > 1.5:\n",
    "        decision = \"‚ö†Ô∏è CONDITIONAL APPROVAL\"\n",
    "        note = \"Deploy with strict monitoring and safeguards\"\n",
    "    elif benefit_risk_ratio > 1.0:\n",
    "        decision = \"üîç REQUIRE REVIEW\"\n",
    "        note = \"Additional risk mitigation needed\"\n",
    "    else:\n",
    "        decision = \"‚ùå DO NOT DEPLOY\"\n",
    "        note = \"Risks outweigh benefits\"\n",
    "    \n",
    "    print(f\"\\nüéØ Decision: {decision}\")\n",
    "    print(f\"   Rationale: {note}\")\n",
    "    \n",
    "    return scenarios, benefits, benefit_risk_ratio\n",
    "\n",
    "risks, benefits, ratio = assess_risk_benefit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.2 Visualize Risk Matrix\n",
    "def plot_risk_matrix(scenarios):\n",
    "    \"\"\"Visualize risks on a likelihood-impact matrix\"\"\"\n",
    "    \n",
    "    # Map categories to numerical values\n",
    "    likelihood_map = {'Low': 1, 'Medium': 2, 'High': 3}\n",
    "    impact_map = {'Low': 1, 'Medium': 2, 'High': 3}\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    \n",
    "    # Plot background zones\n",
    "    # Low risk (green)\n",
    "    ax.add_patch(plt.Rectangle((0.5, 0.5), 1, 1, color='green', alpha=0.2))\n",
    "    # Medium risk (yellow)\n",
    "    ax.add_patch(plt.Rectangle((1.5, 0.5), 1, 1, color='yellow', alpha=0.2))\n",
    "    ax.add_patch(plt.Rectangle((0.5, 1.5), 1, 1, color='yellow', alpha=0.2))\n",
    "    ax.add_patch(plt.Rectangle((1.5, 1.5), 1, 1, color='orange', alpha=0.2))\n",
    "    # High risk (red)\n",
    "    ax.add_patch(plt.Rectangle((2.5, 0.5), 1, 2, color='orange', alpha=0.2))\n",
    "    ax.add_patch(plt.Rectangle((0.5, 2.5), 2, 1, color='red', alpha=0.2))\n",
    "    ax.add_patch(plt.Rectangle((2.5, 2.5), 1, 1, color='red', alpha=0.3))\n",
    "    \n",
    "    # Plot risk points\n",
    "    for risk_name, details in scenarios.items():\n",
    "        x = likelihood_map[details['likelihood']]\n",
    "        y = impact_map[details['impact']]\n",
    "        ax.plot(x, y, 'ko', markersize=15)\n",
    "        ax.text(x, y, risk_name, ha='center', va='center', \n",
    "                fontsize=9, fontweight='bold', color='white',\n",
    "                bbox=dict(boxstyle='round,pad=0.3', facecolor='black', alpha=0.7))\n",
    "    \n",
    "    ax.set_xlim(0.5, 3.5)\n",
    "    ax.set_ylim(0.5, 3.5)\n",
    "    ax.set_xticks([1, 2, 3])\n",
    "    ax.set_yticks([1, 2, 3])\n",
    "    ax.set_xticklabels(['Low', 'Medium', 'High'])\n",
    "    ax.set_yticklabels(['Low', 'Medium', 'High'])\n",
    "    ax.set_xlabel('Likelihood (Î∞úÏÉù Í∞ÄÎä•ÏÑ±)', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel('Impact (ÏòÅÌñ•ÎèÑ)', fontsize=12, fontweight='bold')\n",
    "    ax.set_title('‚ö†Ô∏è Risk Assessment Matrix', fontsize=14, fontweight='bold', pad=20)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add legend\n",
    "    from matplotlib.patches import Patch\n",
    "    legend_elements = [\n",
    "        Patch(facecolor='green', alpha=0.3, label='Low Risk'),\n",
    "        Patch(facecolor='yellow', alpha=0.3, label='Medium Risk'),\n",
    "        Patch(facecolor='orange', alpha=0.3, label='High Risk'),\n",
    "        Patch(facecolor='red', alpha=0.3, label='Critical Risk')\n",
    "    ]\n",
    "    ax.legend(handles=legend_elements, loc='upper left')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_risk_matrix(risks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Practice 4: Output Filtering Simulation\n",
    "\n",
    "### üéØ Learning Objectives\n",
    "- Implement basic content safety filters\n",
    "- Understand multi-layer filtering\n",
    "- Measure filter effectiveness\n",
    "\n",
    "### üìñ Key Concepts\n",
    "**Output Filtering:** Multi-stage validation before AI response delivery  \n",
    "**Safety Guardrails:** Automated checks to prevent harmful outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1 Implement output filtering system\n",
    "def output_filtering_system():\n",
    "    \"\"\"Simulate a multi-layer output filtering pipeline\"\"\"\n",
    "    \n",
    "    # Sample AI responses to filter\n",
    "    test_responses = [\n",
    "        \"Based on your symptoms, I recommend taking aspirin and resting.\",\n",
    "        \"You definitely have cancer. Start chemotherapy immediately.\",\n",
    "        \"Patient John Doe (SSN: 123-45-6789) should increase dosage.\",\n",
    "        \"Your condition seems mild. Consider consulting a doctor if it persists.\",\n",
    "        \"Stop all medications now! They are poisoning you.\",\n",
    "        \"Studies show that treatment A has 85% efficacy for this condition.\",\n",
    "        \"Women are naturally worse at handling this type of condition.\",\n",
    "        \"I suggest scheduling a follow-up appointment with your physician.\"\n",
    "    ]\n",
    "    \n",
    "    print(\"üõ°Ô∏è Multi-Layer Output Filtering System\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for idx, response in enumerate(test_responses, 1):\n",
    "        print(f\"\\nüìù Response {idx}: {response[:60]}...\")\n",
    "        \n",
    "        passed_filters = True\n",
    "        flags = []\n",
    "        \n",
    "        # Filter 1: Content Safety\n",
    "        harmful_keywords = ['definitely', 'cancer', 'immediately', 'stop all', 'poisoning']\n",
    "        if any(keyword in response.lower() for keyword in harmful_keywords):\n",
    "            flags.append(\"‚ö†Ô∏è Filter 1 FAIL: Potentially harmful content\")\n",
    "            passed_filters = False\n",
    "        else:\n",
    "            flags.append(\"‚úÖ Filter 1 PASS: Content safety\")\n",
    "        \n",
    "        # Filter 2: Privacy Protection\n",
    "        import re\n",
    "        if re.search(r'\\b\\d{3}-\\d{2}-\\d{4}\\b', response):  # SSN pattern\n",
    "            flags.append(\"‚ö†Ô∏è Filter 2 FAIL: Privacy violation (PII detected)\")\n",
    "            passed_filters = False\n",
    "        else:\n",
    "            flags.append(\"‚úÖ Filter 2 PASS: Privacy protection\")\n",
    "        \n",
    "        # Filter 3: Bias Detection\n",
    "        biased_terms = ['women are', 'men are', 'naturally worse', 'naturally better']\n",
    "        if any(term in response.lower() for term in biased_terms):\n",
    "            flags.append(\"‚ö†Ô∏è Filter 3 FAIL: Biased language detected\")\n",
    "            passed_filters = False\n",
    "        else:\n",
    "            flags.append(\"‚úÖ Filter 3 PASS: Bias detection\")\n",
    "        \n",
    "        # Filter 4: Medical Accuracy (simplified)\n",
    "        requires_disclaimer = any(word in response.lower() for word in ['recommend', 'should', 'treatment'])\n",
    "        has_disclaimer = 'consult' in response.lower() or 'doctor' in response.lower() or 'physician' in response.lower()\n",
    "        \n",
    "        if requires_disclaimer and not has_disclaimer:\n",
    "            flags.append(\"‚ö†Ô∏è Filter 4 FAIL: Medical advice without disclaimer\")\n",
    "            passed_filters = False\n",
    "        else:\n",
    "            flags.append(\"‚úÖ Filter 4 PASS: Medical accuracy & disclaimer\")\n",
    "        \n",
    "        # Print results\n",
    "        for flag in flags:\n",
    "            print(f\"   {flag}\")\n",
    "        \n",
    "        if passed_filters:\n",
    "            print(\"   üéØ DECISION: ‚úÖ APPROVED for output\")\n",
    "            results.append('PASS')\n",
    "        else:\n",
    "            print(\"   üéØ DECISION: ‚ùå BLOCKED from output\")\n",
    "            results.append('FAIL')\n",
    "    \n",
    "    # Summary\n",
    "    pass_rate = (results.count('PASS') / len(results)) * 100\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(f\"üìä Filtering Summary:\")\n",
    "    print(f\"   Total responses: {len(results)}\")\n",
    "    print(f\"   Approved: {results.count('PASS')} ({results.count('PASS')/len(results)*100:.1f}%)\")\n",
    "    print(f\"   Blocked: {results.count('FAIL')} ({results.count('FAIL')/len(results)*100:.1f}%)\")\n",
    "    print(f\"\\n   üéØ System effectiveness: {100 - pass_rate:.1f}% harmful content blocked\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "filter_results = output_filtering_system()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üéØ Practice Complete!\n",
    "\n",
    "### Summary of What We Learned:\n",
    "\n",
    "1. **Bias Detection** üë•\n",
    "   - Measured performance disparities across demographic groups\n",
    "   - Calculated TPR, FPR, and PPV by group\n",
    "   - Identified potential AI bias in medical predictions\n",
    "\n",
    "2. **Fairness Metrics** ‚öñÔ∏è\n",
    "   - Applied the 4/5ths rule (Disparate Impact Ratio)\n",
    "   - Compared multiple fairness definitions\n",
    "   - Understood fairness-accuracy tradeoffs\n",
    "\n",
    "3. **Risk-Benefit Analysis** üìä\n",
    "   - Created risk assessment matrices\n",
    "   - Calculated benefit-risk ratios\n",
    "   - Made evidence-based deployment decisions\n",
    "\n",
    "4. **Output Filtering** üõ°Ô∏è\n",
    "   - Implemented multi-layer safety filters\n",
    "   - Detected harmful content, privacy violations, and bias\n",
    "   - Measured filter effectiveness\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "‚úÖ **Constitutional AI requires continuous monitoring** - Bias and safety issues must be actively detected and mitigated\n",
    "\n",
    "‚úÖ **No single fairness metric is perfect** - Different contexts require different fairness definitions\n",
    "\n",
    "‚úÖ **Multi-layer defenses are essential** - Safety cannot rely on a single filtering mechanism\n",
    "\n",
    "‚úÖ **Ethics must be built into the system** - Not added as an afterthought\n",
    "\n",
    "### Next Steps:\n",
    "- Implement more sophisticated bias mitigation techniques (reweighting, adversarial debiasing)\n",
    "- Explore Red Teaming methodologies\n",
    "- Study real-world case studies of AI failures and successes\n",
    "- Practice with actual medical datasets (with proper ethical approval)\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Additional Resources:\n",
    "\n",
    "**Tools for Fairness Testing:**\n",
    "- Fairlearn (Microsoft): https://fairlearn.org/\n",
    "- AI Fairness 360 (IBM): https://aif360.mybluemix.net/\n",
    "- What-If Tool (Google): https://pair-code.github.io/what-if-tool/\n",
    "\n",
    "**Further Reading:**\n",
    "- Beauchamp & Childress - Principles of Biomedical Ethics\n",
    "- Anthropic's Constitutional AI paper\n",
    "- FDA guidance on AI/ML in medical devices\n",
    "\n",
    "---\n",
    "\n",
    "**‚ú® Thank you for completing this hands-on practice! ‚ú®**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
