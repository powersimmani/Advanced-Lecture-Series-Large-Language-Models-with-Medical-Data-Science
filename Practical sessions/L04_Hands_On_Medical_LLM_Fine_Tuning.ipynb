{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üè• Hands-On: Medical LLM Fine-Tuning Practice\n",
    "\n",
    "## üìã Table of Contents\n",
    "1. [Environment Setup and Installation](#practice-1-environment-setup-and-installation)\n",
    "2. [Loading Pre-trained Models](#practice-2-loading-pre-trained-models)\n",
    "3. [Preparing Medical Datasets](#practice-3-preparing-medical-datasets)\n",
    "4. [Configuring LoRA for PEFT](#practice-4-configuring-lora-for-peft)\n",
    "5. [Training with Medical Instructions](#practice-5-training-with-medical-instructions)\n",
    "6. [Model Evaluation and Testing](#practice-6-model-evaluation-and-testing)\n",
    "7. [Inference and Deployment](#practice-7-inference-and-deployment)\n",
    "8. [Monitoring and Optimization](#practice-8-monitoring-and-optimization)\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "- Implement LoRA-based parameter-efficient fine-tuning\n",
    "- Prepare and format medical instruction datasets\n",
    "- Train models with medical domain knowledge\n",
    "- Evaluate model performance on clinical tasks\n",
    "- Deploy fine-tuned models for inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Practice 1: Environment Setup and Installation\n",
    "\n",
    "### üéØ Learning Objectives\n",
    "- Install essential libraries for medical LLM fine-tuning\n",
    "- Set up GPU environment and verify resources\n",
    "- Configure logging and monitoring tools\n",
    "\n",
    "### üìñ Key Concepts\n",
    "**Required Libraries:**\n",
    "- `transformers`: Hugging Face model library\n",
    "- `peft`: Parameter-Efficient Fine-Tuning\n",
    "- `datasets`: Dataset loading and processing\n",
    "- `bitsandbytes`: Quantization support (QLoRA)\n",
    "- `accelerate`: Distributed training utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.1 Install required packages\n",
    "# Note: Run this cell only once or in a fresh environment\n",
    "\n",
    "!pip install -q transformers==4.36.0\n",
    "!pip install -q peft==0.7.1\n",
    "!pip install -q datasets==2.16.0\n",
    "!pip install -q bitsandbytes==0.41.3\n",
    "!pip install -q accelerate==0.25.0\n",
    "!pip install -q scipy\n",
    "\n",
    "print(\"‚úÖ All packages installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.2 Import libraries and check GPU availability\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    prepare_model_for_kbit_training,\n",
    "    TaskType\n",
    ")\n",
    "from datasets import Dataset, load_dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Check GPU availability\n",
    "print(\"=\" * 60)\n",
    "print(\"üñ•Ô∏è  System Information\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Transformers version: {transformers.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "    print(f\"Current GPU memory allocated: {torch.cuda.memory_allocated(0) / 1e9:.2f} GB\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No GPU available. Training will be slow on CPU.\")\n",
    "\n",
    "print(\"\\n‚úÖ Environment setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Practice 2: Loading Pre-trained Models\n",
    "\n",
    "### üéØ Learning Objectives\n",
    "- Load pre-trained language models from Hugging Face\n",
    "- Understand model architecture and tokenization\n",
    "- Prepare models for fine-tuning\n",
    "\n",
    "### üìñ Key Concepts\n",
    "**Model Selection:**\n",
    "- Small models (7B): Faster training, less memory\n",
    "- We'll use a small GPT-2 model for quick demonstration\n",
    "- In production: Use LLaMA-7B, Mistral-7B, or similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1 Load base model and tokenizer\n",
    "def load_base_model(model_name=\"gpt2\"):\n",
    "    \"\"\"\n",
    "    Load pre-trained model and tokenizer\n",
    "    \n",
    "    For production use:\n",
    "    - model_name = \"meta-llama/Llama-2-7b-hf\" (requires access token)\n",
    "    - model_name = \"mistralai/Mistral-7B-v0.1\"\n",
    "    \"\"\"\n",
    "    print(f\"Loading model: {model_name}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    # Add padding token if not present\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        print(\"‚úÖ Added pad_token = eos_token\")\n",
    "    \n",
    "    # Load model\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "    )\n",
    "    \n",
    "    # Model information\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "    print(f\"\\nüìä Model Statistics:\")\n",
    "    print(f\"  Total parameters: {total_params:,}\")\n",
    "    print(f\"  Trainable parameters: {trainable_params:,}\")\n",
    "    print(f\"  Vocabulary size: {len(tokenizer)}\")\n",
    "    print(f\"  Max sequence length: {tokenizer.model_max_length}\")\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "# Load the model\n",
    "base_model, tokenizer = load_base_model()\n",
    "print(\"\\n‚úÖ Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2 Test base model inference\n",
    "def test_base_model(model, tokenizer, prompt=\"Describe the symptoms of diabetes:\"):\n",
    "    \"\"\"Test the base model before fine-tuning\"\"\"\n",
    "    print(\"Testing base model...\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(\"\\nGenerated response:\")\n",
    "    \n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "    \n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=100,\n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.pad_token_id\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    print(response)\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    \n",
    "    return response\n",
    "\n",
    "# Test the model\n",
    "base_response = test_base_model(base_model, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Practice 3: Preparing Medical Datasets\n",
    "\n",
    "### üéØ Learning Objectives\n",
    "- Create medical instruction datasets\n",
    "- Format data for instruction tuning\n",
    "- Implement data preprocessing pipelines\n",
    "\n",
    "### üìñ Key Concepts\n",
    "**Instruction Format:**\n",
    "```\n",
    "### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Input:\n",
    "{input}\n",
    "\n",
    "### Response:\n",
    "{output}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1 Create sample medical instruction dataset\n",
    "def create_medical_dataset():\n",
    "    \"\"\"Create a sample medical instruction dataset\"\"\"\n",
    "    \n",
    "    # Sample medical QA pairs\n",
    "    data = [\n",
    "        {\n",
    "            \"instruction\": \"Explain the symptoms of the given condition.\",\n",
    "            \"input\": \"Type 2 Diabetes\",\n",
    "            \"output\": \"Type 2 diabetes symptoms include increased thirst, frequent urination, increased hunger, unintended weight loss, fatigue, blurred vision, slow-healing sores, and frequent infections. These symptoms develop slowly over time and may not be immediately noticeable.\"\n",
    "        },\n",
    "        {\n",
    "            \"instruction\": \"Provide treatment recommendations for the following condition.\",\n",
    "            \"input\": \"Hypertension\",\n",
    "            \"output\": \"Treatment for hypertension includes lifestyle modifications (reduced sodium intake, regular exercise, weight management, stress reduction) and medications such as ACE inhibitors, ARBs, diuretics, beta-blockers, or calcium channel blockers. Regular monitoring of blood pressure is essential.\"\n",
    "        },\n",
    "        {\n",
    "            \"instruction\": \"Describe the diagnostic criteria for the following disease.\",\n",
    "            \"input\": \"Pneumonia\",\n",
    "            \"output\": \"Pneumonia diagnosis involves clinical symptoms (fever, cough, difficulty breathing), physical examination (abnormal breath sounds), and diagnostic tests including chest X-ray showing infiltrates, complete blood count, and possibly sputum culture to identify the causative organism.\"\n",
    "        },\n",
    "        {\n",
    "            \"instruction\": \"Explain the mechanism of action for the medication.\",\n",
    "            \"input\": \"Metformin\",\n",
    "            \"output\": \"Metformin is a biguanide that works by decreasing hepatic glucose production, decreasing intestinal absorption of glucose, and improving insulin sensitivity by increasing peripheral glucose uptake and utilization. It does not cause hypoglycemia when used as monotherapy.\"\n",
    "        },\n",
    "        {\n",
    "            \"instruction\": \"List the risk factors for the given condition.\",\n",
    "            \"input\": \"Coronary Artery Disease\",\n",
    "            \"output\": \"Risk factors for coronary artery disease include hypertension, high cholesterol, smoking, diabetes, obesity, physical inactivity, family history, age (men >45, women >55), and chronic stress. Modifiable risk factors should be addressed through lifestyle changes and medical management.\"\n",
    "        },\n",
    "        {\n",
    "            \"instruction\": \"Explain the pathophysiology of the disease.\",\n",
    "            \"input\": \"Asthma\",\n",
    "            \"output\": \"Asthma is characterized by chronic airway inflammation leading to bronchial hyperresponsiveness and reversible airflow obstruction. Triggers cause mast cell degranulation, release of inflammatory mediators, smooth muscle contraction, mucus hypersecretion, and airway edema, resulting in wheezing, coughing, and difficulty breathing.\"\n",
    "        },\n",
    "        {\n",
    "            \"instruction\": \"Describe preventive measures for the condition.\",\n",
    "            \"input\": \"Stroke\",\n",
    "            \"output\": \"Stroke prevention includes controlling hypertension, managing diabetes, maintaining healthy cholesterol levels, not smoking, limiting alcohol intake, regular exercise, maintaining healthy weight, eating a balanced diet low in saturated fats, and taking prescribed anticoagulants or antiplatelet medications when indicated.\"\n",
    "        },\n",
    "        {\n",
    "            \"instruction\": \"Explain the differential diagnosis for the symptoms.\",\n",
    "            \"input\": \"Chest pain\",\n",
    "            \"output\": \"Differential diagnosis for chest pain includes cardiac causes (myocardial infarction, angina, pericarditis), pulmonary causes (pneumonia, pulmonary embolism, pneumothorax), gastrointestinal causes (GERD, esophageal spasm), musculoskeletal causes (costochondritis), and anxiety/panic attacks. Immediate evaluation is needed to rule out life-threatening conditions.\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    print(\"üìä Medical Dataset Created\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Total samples: {len(df)}\")\n",
    "    print(f\"\\nSample entry:\")\n",
    "    print(f\"Instruction: {df.iloc[0]['instruction']}\")\n",
    "    print(f\"Input: {df.iloc[0]['input']}\")\n",
    "    print(f\"Output: {df.iloc[0]['output'][:100]}...\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Create dataset\n",
    "medical_df = create_medical_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.2 Format dataset for instruction tuning\n",
    "def format_instruction(sample):\n",
    "    \"\"\"Format sample into instruction-following format\"\"\"\n",
    "    instruction = sample['instruction']\n",
    "    input_text = sample['input']\n",
    "    output = sample['output']\n",
    "    \n",
    "    # Create formatted prompt\n",
    "    if input_text:\n",
    "        prompt = f\"\"\"### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Input:\n",
    "{input_text}\n",
    "\n",
    "### Response:\n",
    "{output}\"\"\"\n",
    "    else:\n",
    "        prompt = f\"\"\"### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Response:\n",
    "{output}\"\"\"\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "# Apply formatting\n",
    "medical_df['text'] = medical_df.apply(format_instruction, axis=1)\n",
    "\n",
    "print(\"Formatted sample:\")\n",
    "print(\"=\" * 60)\n",
    "print(medical_df['text'].iloc[0])\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.3 Tokenize dataset\n",
    "def tokenize_function(examples, tokenizer, max_length=512):\n",
    "    \"\"\"Tokenize the dataset\"\"\"\n",
    "    return tokenizer(\n",
    "        examples['text'],\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        padding='max_length',\n",
    "        return_tensors=None\n",
    "    )\n",
    "\n",
    "# Convert to Hugging Face Dataset\n",
    "dataset = Dataset.from_pandas(medical_df[['text']])\n",
    "\n",
    "# Tokenize\n",
    "tokenized_dataset = dataset.map(\n",
    "    lambda x: tokenize_function(x, tokenizer),\n",
    "    batched=True,\n",
    "    remove_columns=['text']\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Dataset tokenized successfully!\")\n",
    "print(f\"Number of samples: {len(tokenized_dataset)}\")\n",
    "print(f\"Features: {tokenized_dataset.features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Practice 4: Configuring LoRA for PEFT\n",
    "\n",
    "### üéØ Learning Objectives\n",
    "- Understand LoRA configuration parameters\n",
    "- Apply LoRA to the base model\n",
    "- Verify trainable parameter reduction\n",
    "\n",
    "### üìñ Key Concepts\n",
    "**LoRA Configuration:**\n",
    "- `r`: Rank (typically 8-16 for medical tasks)\n",
    "- `lora_alpha`: Scaling factor (typically 2 √ó r)\n",
    "- `target_modules`: Which layers to adapt (q_proj, v_proj)\n",
    "- `lora_dropout`: Regularization (0.05-0.1)\n",
    "\n",
    "**Memory Saving:** W = W‚ÇÄ + BA where rank(B) = rank(A) = r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1 Configure LoRA\n",
    "def configure_lora(rank=8, alpha=16, dropout=0.05):\n",
    "    \"\"\"\n",
    "    Configure LoRA for parameter-efficient fine-tuning\n",
    "    \n",
    "    Parameters:\n",
    "    - rank (r): Low-rank dimension (4-64)\n",
    "    - alpha: Scaling parameter (typically 2*r)\n",
    "    - dropout: Dropout probability for LoRA layers\n",
    "    \"\"\"\n",
    "    print(\"üîß Configuring LoRA\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    lora_config = LoraConfig(\n",
    "        r=rank,\n",
    "        lora_alpha=alpha,\n",
    "        target_modules=[\"c_attn\"],  # For GPT-2; use [\"q_proj\", \"v_proj\"] for LLaMA\n",
    "        lora_dropout=dropout,\n",
    "        bias=\"none\",\n",
    "        task_type=TaskType.CAUSAL_LM\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ LoRA Configuration:\")\n",
    "    print(f\"  Rank (r): {rank}\")\n",
    "    print(f\"  Alpha: {alpha}\")\n",
    "    print(f\"  Dropout: {dropout}\")\n",
    "    print(f\"  Target modules: {lora_config.target_modules}\")\n",
    "    \n",
    "    return lora_config\n",
    "\n",
    "# Create LoRA configuration\n",
    "lora_config = configure_lora(rank=8, alpha=16, dropout=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.2 Apply LoRA to the model\n",
    "def apply_lora(model, lora_config):\n",
    "    \"\"\"Apply LoRA configuration to the model\"\"\"\n",
    "    print(\"\\nüîÑ Applying LoRA to model...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Get original parameter count\n",
    "    original_params = sum(p.numel() for p in model.parameters())\n",
    "    original_trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "    # Apply LoRA\n",
    "    model = get_peft_model(model, lora_config)\n",
    "    \n",
    "    # Get new parameter count\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "    # Calculate percentages\n",
    "    trainable_percent = 100 * trainable_params / total_params\n",
    "    memory_reduction = 100 * (1 - trainable_params / original_params)\n",
    "    \n",
    "    print(\"üìä Parameter Statistics:\")\n",
    "    print(f\"\\n  Before LoRA:\")\n",
    "    print(f\"    Total parameters: {original_params:,}\")\n",
    "    print(f\"    Trainable parameters: {original_trainable:,}\")\n",
    "    print(f\"\\n  After LoRA:\")\n",
    "    print(f\"    Total parameters: {total_params:,}\")\n",
    "    print(f\"    Trainable parameters: {trainable_params:,}\")\n",
    "    print(f\"    Trainable %: {trainable_percent:.2f}%\")\n",
    "    print(f\"    Memory reduction: {memory_reduction:.2f}%\")\n",
    "    \n",
    "    print(\"\\n‚úÖ LoRA applied successfully!\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Apply LoRA\n",
    "peft_model = apply_lora(base_model, lora_config)\n",
    "\n",
    "# Print trainable parameters\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Trainable Parameters:\")\n",
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Practice 5: Training with Medical Instructions\n",
    "\n",
    "### üéØ Learning Objectives\n",
    "- Configure training arguments for medical LLM fine-tuning\n",
    "- Implement training loop with proper monitoring\n",
    "- Save and manage checkpoints\n",
    "\n",
    "### üìñ Key Concepts\n",
    "**Training Configuration:**\n",
    "- Learning rate: 3e-4 (higher than full fine-tuning)\n",
    "- Batch size: 4-8 (with gradient accumulation)\n",
    "- Epochs: 3-5 for small datasets\n",
    "- Warmup: 5-10% of total steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.1 Configure training arguments\n",
    "def setup_training_args(output_dir=\"./medical_llm_finetuned\"):\n",
    "    \"\"\"Setup training arguments for fine-tuning\"\"\"\n",
    "    \n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        \n",
    "        # Training hyperparameters\n",
    "        num_train_epochs=3,\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=4,\n",
    "        learning_rate=3e-4,\n",
    "        \n",
    "        # Optimization\n",
    "        warmup_steps=10,\n",
    "        weight_decay=0.01,\n",
    "        \n",
    "        # Logging and saving\n",
    "        logging_steps=10,\n",
    "        save_steps=50,\n",
    "        save_total_limit=2,\n",
    "        \n",
    "        # Mixed precision\n",
    "        fp16=torch.cuda.is_available(),\n",
    "        \n",
    "        # Other settings\n",
    "        remove_unused_columns=False,\n",
    "        report_to=\"none\",  # Disable wandb/tensorboard for this demo\n",
    "    )\n",
    "    \n",
    "    print(\"‚öôÔ∏è  Training Configuration\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Output directory: {output_dir}\")\n",
    "    print(f\"Number of epochs: {training_args.num_train_epochs}\")\n",
    "    print(f\"Batch size per device: {training_args.per_device_train_batch_size}\")\n",
    "    print(f\"Gradient accumulation steps: {training_args.gradient_accumulation_steps}\")\n",
    "    print(f\"Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
    "    print(f\"Learning rate: {training_args.learning_rate}\")\n",
    "    print(f\"Warmup steps: {training_args.warmup_steps}\")\n",
    "    print(f\"Mixed precision (fp16): {training_args.fp16}\")\n",
    "    \n",
    "    return training_args\n",
    "\n",
    "# Setup training arguments\n",
    "training_args = setup_training_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.2 Create data collator and trainer\n",
    "def setup_trainer(model, tokenizer, train_dataset, training_args):\n",
    "    \"\"\"Setup the Trainer for fine-tuning\"\"\"\n",
    "    \n",
    "    # Data collator for language modeling\n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer,\n",
    "        mlm=False  # Causal language modeling (not masked)\n",
    "    )\n",
    "    \n",
    "    # Create trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        data_collator=data_collator,\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ Trainer initialized successfully!\")\n",
    "    print(f\"Training samples: {len(train_dataset)}\")\n",
    "    \n",
    "    return trainer\n",
    "\n",
    "# Setup trainer\n",
    "trainer = setup_trainer(peft_model, tokenizer, tokenized_dataset, training_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.3 Train the model\n",
    "print(\"üöÄ Starting training...\")\n",
    "print(\"=\" * 60)\n",
    "print(\"This may take a few minutes depending on your hardware.\")\n",
    "print(\"\")\n",
    "\n",
    "# Train\n",
    "train_result = trainer.train()\n",
    "\n",
    "# Print training results\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üìä Training Complete!\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Training loss: {train_result.training_loss:.4f}\")\n",
    "print(f\"Training time: {train_result.metrics['train_runtime']:.2f} seconds\")\n",
    "print(f\"Samples per second: {train_result.metrics['train_samples_per_second']:.2f}\")\n",
    "\n",
    "# Save the model\n",
    "trainer.save_model()\n",
    "print(\"\\n‚úÖ Model saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Practice 6: Model Evaluation and Testing\n",
    "\n",
    "### üéØ Learning Objectives\n",
    "- Test the fine-tuned model on medical queries\n",
    "- Compare responses before and after fine-tuning\n",
    "- Evaluate model performance qualitatively\n",
    "\n",
    "### üìñ Key Concepts\n",
    "**Evaluation Metrics:**\n",
    "- Qualitative: Response relevance and accuracy\n",
    "- Quantitative: Perplexity, BLEU score\n",
    "- Medical-specific: Clinical accuracy, safety"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.1 Generate responses with fine-tuned model\n",
    "def generate_response(model, tokenizer, prompt, max_length=150):\n",
    "    \"\"\"Generate response using the fine-tuned model\"\"\"\n",
    "    \n",
    "    # Format prompt\n",
    "    formatted_prompt = f\"\"\"### Instruction:\n",
    "{prompt}\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "    \n",
    "    # Tokenize\n",
    "    inputs = tokenizer(formatted_prompt, return_tensors=\"pt\")\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "    \n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_length=max_length,\n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "            top_p=0.9,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Extract only the response part\n",
    "    if \"### Response:\" in response:\n",
    "        response = response.split(\"### Response:\")[1].strip()\n",
    "    \n",
    "    return response\n",
    "\n",
    "# Test prompts\n",
    "test_prompts = [\n",
    "    \"Explain the symptoms of Type 2 Diabetes.\",\n",
    "    \"What are the treatment options for hypertension?\",\n",
    "    \"Describe the mechanism of action of aspirin.\"\n",
    "]\n",
    "\n",
    "print(\"üß™ Testing Fine-Tuned Model\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, prompt in enumerate(test_prompts, 1):\n",
    "    print(f\"\\nTest {i}:\")\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(\"\\nResponse:\")\n",
    "    response = generate_response(peft_model, tokenizer, prompt)\n",
    "    print(response)\n",
    "    print(\"\\n\" + \"-\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.2 Compare base model vs fine-tuned model\n",
    "def compare_models(base_model, finetuned_model, tokenizer, prompt):\n",
    "    \"\"\"Compare responses from base and fine-tuned models\"\"\"\n",
    "    \n",
    "    print(\"üîç Model Comparison\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    \n",
    "    # Base model response\n",
    "    print(\"\\nüìò Base Model Response:\")\n",
    "    base_response = test_base_model(base_model, tokenizer, prompt)\n",
    "    \n",
    "    # Fine-tuned model response\n",
    "    print(\"\\nüìó Fine-Tuned Model Response:\")\n",
    "    ft_response = generate_response(finetuned_model, tokenizer, prompt)\n",
    "    print(ft_response)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Observation: The fine-tuned model should provide more\")\n",
    "    print(\"structured and medically accurate responses.\")\n",
    "    \n",
    "    return base_response, ft_response\n",
    "\n",
    "# Compare on a medical query\n",
    "comparison_prompt = \"What are the symptoms of pneumonia?\"\n",
    "base_resp, ft_resp = compare_models(base_model, peft_model, tokenizer, comparison_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Practice 7: Inference and Deployment\n",
    "\n",
    "### üéØ Learning Objectives\n",
    "- Load saved models for inference\n",
    "- Merge LoRA weights with base model\n",
    "- Prepare model for production deployment\n",
    "\n",
    "### üìñ Key Concepts\n",
    "**Deployment Options:**\n",
    "1. Keep LoRA adapters separate (smaller storage)\n",
    "2. Merge adapters into base model (faster inference)\n",
    "3. Quantize for edge deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.1 Load the fine-tuned model\n",
    "def load_finetuned_model(model_path=\"./medical_llm_finetuned\"):\n",
    "    \"\"\"Load the fine-tuned model from checkpoint\"\"\"\n",
    "    \n",
    "    print(\"üìÇ Loading fine-tuned model...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    from peft import PeftModel\n",
    "    \n",
    "    # Load base model\n",
    "    base = AutoModelForCausalLM.from_pretrained(\n",
    "        \"gpt2\",\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "    )\n",
    "    \n",
    "    # Load PEFT model\n",
    "    model = PeftModel.from_pretrained(base, model_path)\n",
    "    \n",
    "    print(f\"‚úÖ Model loaded from: {model_path}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Load the model\n",
    "loaded_model = load_finetuned_model()\n",
    "print(\"\\n‚úÖ Model ready for inference!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.2 Merge LoRA weights (optional)\n",
    "def merge_lora_weights(peft_model):\n",
    "    \"\"\"Merge LoRA adapters into the base model\"\"\"\n",
    "    \n",
    "    print(\"üîÑ Merging LoRA weights...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Merge and unload\n",
    "    merged_model = peft_model.merge_and_unload()\n",
    "    \n",
    "    print(\"‚úÖ LoRA weights merged successfully!\")\n",
    "    print(\"   The model is now a standard model without adapters.\")\n",
    "    print(\"   This is useful for:\")\n",
    "    print(\"   - Faster inference (no adapter overhead)\")\n",
    "    print(\"   - Easier deployment\")\n",
    "    print(\"   - Compatibility with standard inference pipelines\")\n",
    "    \n",
    "    return merged_model\n",
    "\n",
    "# Merge weights\n",
    "merged_model = merge_lora_weights(loaded_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.3 Save merged model for deployment\n",
    "def save_for_deployment(model, tokenizer, output_path=\"./medical_llm_deployed\"):\n",
    "    \"\"\"Save the final model for deployment\"\"\"\n",
    "    \n",
    "    print(\"üíæ Saving model for deployment...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Save model\n",
    "    model.save_pretrained(output_path)\n",
    "    tokenizer.save_pretrained(output_path)\n",
    "    \n",
    "    print(f\"‚úÖ Model saved to: {output_path}\")\n",
    "    print(\"\\nDeployment files:\")\n",
    "    print(\"  - pytorch_model.bin (model weights)\")\n",
    "    print(\"  - config.json (model configuration)\")\n",
    "    print(\"  - tokenizer files\")\n",
    "    \n",
    "    return output_path\n",
    "\n",
    "# Save for deployment\n",
    "deployment_path = save_for_deployment(merged_model, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Practice 8: Monitoring and Optimization\n",
    "\n",
    "### üéØ Learning Objectives\n",
    "- Monitor training metrics and model performance\n",
    "- Identify common issues and solutions\n",
    "- Optimize hyperparameters for better results\n",
    "\n",
    "### üìñ Key Concepts\n",
    "**Common Issues:**\n",
    "- Overfitting: Model memorizes training data\n",
    "- Catastrophic forgetting: Loss of general knowledge\n",
    "- Poor convergence: Learning rate too high/low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.1 Best practices summary\n",
    "def print_best_practices():\n",
    "    \"\"\"Print best practices for medical LLM fine-tuning\"\"\"\n",
    "    \n",
    "    print(\"üìã Best Practices for Medical LLM Fine-Tuning\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    practices = {\n",
    "        \"‚úÖ DO's\": [\n",
    "            \"Start with small LoRA rank (r=8) and increase if needed\",\n",
    "            \"Use higher learning rates (3e-4 to 1e-3) for PEFT\",\n",
    "            \"Monitor both training loss and validation performance\",\n",
    "            \"Save checkpoints regularly\",\n",
    "            \"Test on diverse medical queries\",\n",
    "            \"Use mixed precision training (FP16) for efficiency\",\n",
    "            \"Validate with medical experts before deployment\"\n",
    "        ],\n",
    "        \"‚ùå DON'Ts\": [\n",
    "            \"Don't overtrain - stop when validation loss plateaus\",\n",
    "            \"Don't use too large batch sizes (causes memory issues)\",\n",
    "            \"Don't skip data quality checks\",\n",
    "            \"Don't deploy without safety testing\",\n",
    "            \"Don't ignore catastrophic forgetting\",\n",
    "            \"Don't use medical models for final diagnosis\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    for category, items in practices.items():\n",
    "        print(f\"\\n{category}:\")\n",
    "        for item in items:\n",
    "            print(f\"  ‚Ä¢ {item}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"‚ö†Ô∏è  Important Reminder:\")\n",
    "    print(\"Medical LLMs are assistive tools and should not replace\")\n",
    "    print(\"professional medical judgment. Always require expert validation.\")\n",
    "\n",
    "print_best_practices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.2 Hyperparameter recommendations\n",
    "def print_hyperparameter_guide():\n",
    "    \"\"\"Print hyperparameter tuning guide\"\"\"\n",
    "    \n",
    "    print(\"\\n‚öôÔ∏è  Hyperparameter Tuning Guide\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    guide = {\n",
    "        \"LoRA Rank (r)\": {\n",
    "            \"r=4\": \"Simple tasks, very limited data\",\n",
    "            \"r=8\": \"Most medical NLP tasks (recommended start)\",\n",
    "            \"r=16\": \"Complex medical reasoning\",\n",
    "            \"r=32-64\": \"Multi-task learning, large datasets\"\n",
    "        },\n",
    "        \"Learning Rate\": {\n",
    "            \"1e-3 to 5e-3\": \"PEFT methods (LoRA, Adapters)\",\n",
    "            \"1e-5 to 5e-5\": \"Full fine-tuning\",\n",
    "            \"Higher\": \"Small datasets, quick adaptation\",\n",
    "            \"Lower\": \"Large datasets, careful tuning\"\n",
    "        },\n",
    "        \"Batch Size\": {\n",
    "            \"4-8\": \"Most scenarios with gradient accumulation\",\n",
    "            \"16-32\": \"Large GPU memory available\",\n",
    "            \"1-2\": \"Very large models (70B+)\"\n",
    "        },\n",
    "        \"Epochs\": {\n",
    "            \"3-5\": \"Small datasets (< 1K samples)\",\n",
    "            \"2-3\": \"Medium datasets (1K-10K samples)\",\n",
    "            \"1-2\": \"Large datasets (> 10K samples)\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    for param, values in guide.items():\n",
    "        print(f\"\\nüìä {param}:\")\n",
    "        for setting, description in values.items():\n",
    "            print(f\"  ‚Ä¢ {setting}: {description}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"üí° Tip: Start with conservative settings and adjust based on\")\n",
    "    print(\"   validation performance. Monitor for overfitting!\")\n",
    "\n",
    "print_hyperparameter_guide()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üéØ Practice Complete!\n",
    "\n",
    "### üéì Summary of What We Learned:\n",
    "\n",
    "1. **Environment Setup**: Installing libraries and checking GPU resources\n",
    "2. **Model Loading**: Loading pre-trained models from Hugging Face\n",
    "3. **Data Preparation**: Creating and formatting medical instruction datasets\n",
    "4. **LoRA Configuration**: Applying parameter-efficient fine-tuning\n",
    "5. **Training**: Fine-tuning with medical instructions\n",
    "6. **Evaluation**: Testing and comparing model responses\n",
    "7. **Deployment**: Saving and preparing models for production\n",
    "8. **Best Practices**: Guidelines for successful medical LLM fine-tuning\n",
    "\n",
    "### üîë Key Takeaways:\n",
    "\n",
    "- **LoRA reduces trainable parameters by 99%+** while maintaining performance\n",
    "- **Medical instruction tuning** improves domain-specific responses\n",
    "- **Proper dataset formatting** is crucial for effective fine-tuning\n",
    "- **Safety and validation** are paramount in medical applications\n",
    "\n",
    "### üìà Performance Gains:\n",
    "\n",
    "| Metric | Base Model | Fine-Tuned Model | Improvement |\n",
    "|--------|------------|------------------|-------------|\n",
    "| Medical Accuracy | Low | High | ‚¨ÜÔ∏è ‚¨ÜÔ∏è ‚¨ÜÔ∏è |\n",
    "| Response Structure | Poor | Good | ‚¨ÜÔ∏è ‚¨ÜÔ∏è |\n",
    "| Domain Knowledge | Generic | Specialized | ‚¨ÜÔ∏è ‚¨ÜÔ∏è ‚¨ÜÔ∏è |\n",
    "| Memory Usage | - | 99% less | ‚¨áÔ∏è ‚¨áÔ∏è ‚¨áÔ∏è |\n",
    "\n",
    "### üöÄ Next Steps:\n",
    "\n",
    "1. **Experiment with larger models**: Try LLaMA-7B, Mistral-7B\n",
    "2. **Use real medical datasets**: MedInstruct, HealthCareMagic, MedQA\n",
    "3. **Implement QLoRA**: 4-bit quantization for even better efficiency\n",
    "4. **Add evaluation metrics**: BLEU, ROUGE, clinical accuracy\n",
    "5. **Deploy with safety checks**: Implement guardrails and monitoring\n",
    "\n",
    "### üìö Additional Resources:\n",
    "\n",
    "- [Hugging Face PEFT Documentation](https://huggingface.co/docs/peft)\n",
    "- [LoRA Paper](https://arxiv.org/abs/2106.09685)\n",
    "- [QLoRA Paper](https://arxiv.org/abs/2305.14314)\n",
    "- [Medical Instruction Datasets](https://github.com/Kent0n-Li/ChatDoctor)\n",
    "\n",
    "### ‚ö†Ô∏è Important Reminders:\n",
    "\n",
    "1. **Never deploy medical models without expert validation**\n",
    "2. **Always include appropriate disclaimers**\n",
    "3. **Comply with healthcare regulations (HIPAA, GDPR, etc.)**\n",
    "4. **Monitor for bias and ensure fairness**\n",
    "5. **Keep models updated with latest medical knowledge**\n",
    "\n",
    "---\n",
    "\n",
    "## üôè Thank You!\n",
    "\n",
    "**Questions?**\n",
    "- Email: homin.park@ghent.ac.kr | powersimmani@gmail.com\n",
    "- GitHub: [Your Repository]\n",
    "- Community: Join Medical AI forums\n",
    "\n",
    "**Good luck with your medical LLM fine-tuning journey! üè•ü§ñ**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
