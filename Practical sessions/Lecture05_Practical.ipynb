{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üè• Medical Model Evaluation: Hands-On Practice\n",
    "\n",
    "## Table of Contents\n",
    "1. [Medical Benchmark Dataset Loading](#practice-1-medical-benchmark-dataset-loading)\n",
    "2. [Accuracy and Basic Metrics](#practice-2-accuracy-and-basic-metrics)\n",
    "3. [Hallucination Detection](#practice-3-hallucination-detection)\n",
    "4. [Factuality Scoring](#practice-4-factuality-scoring)\n",
    "5. [Consistency Measures](#practice-5-consistency-measures)\n",
    "6. [Uncertainty Quantification](#practice-6-uncertainty-quantification)\n",
    "7. [Calibration Metrics](#practice-7-calibration-metrics)\n",
    "8. [Clinical Relevance Scoring](#practice-8-clinical-relevance-scoring)\n",
    "9. [Safety Assessment](#practice-9-safety-assessment)\n",
    "10. [Performance Dashboard Creation](#practice-10-performance-dashboard-creation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing and Importing Essential Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import essential libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
    "from sklearn.calibration import calibration_curve\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Visualization settings\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "sns.set_style('whitegrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print(\"‚úÖ All libraries loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Practice 1: Medical Benchmark Dataset Loading\n",
    "\n",
    "### üéØ Learning Objectives\n",
    "- Load and explore medical Q&A benchmark data\n",
    "- Understand the structure of MedQA-style datasets\n",
    "- Prepare data for evaluation\n",
    "\n",
    "### üìñ Key Concepts\n",
    "**Medical Benchmarks:** MedQA (11,450 questions), USMLE-style, 4-choice format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.1 Create synthetic medical Q&A dataset\n",
    "def create_medical_qa_dataset(n_samples=100):\n",
    "    \"\"\"\n",
    "    Create a synthetic medical Q&A dataset for evaluation practice\n",
    "    Simulates MedQA-style benchmark data\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Simulate model predictions with varying confidence\n",
    "    true_labels = np.random.choice([0, 1, 2, 3], size=n_samples, p=[0.25, 0.25, 0.25, 0.25])\n",
    "    \n",
    "    # Model predictions (with some errors)\n",
    "    predicted_labels = true_labels.copy()\n",
    "    error_indices = np.random.choice(n_samples, size=int(n_samples * 0.15), replace=False)\n",
    "    predicted_labels[error_indices] = np.random.choice([0, 1, 2, 3], size=len(error_indices))\n",
    "    \n",
    "    # Generate confidence scores (higher for correct predictions)\n",
    "    confidence_scores = np.zeros(n_samples)\n",
    "    correct_mask = (true_labels == predicted_labels)\n",
    "    confidence_scores[correct_mask] = np.random.beta(8, 2, size=np.sum(correct_mask))\n",
    "    confidence_scores[~correct_mask] = np.random.beta(3, 5, size=np.sum(~correct_mask))\n",
    "    \n",
    "    # Create difficulty levels\n",
    "    difficulty = np.random.choice(['Easy', 'Moderate', 'Hard', 'Expert'], \n",
    "                                   size=n_samples, \n",
    "                                   p=[0.3, 0.3, 0.25, 0.15])\n",
    "    \n",
    "    # Create medical specialties\n",
    "    specialties = np.random.choice(['Cardiology', 'Neurology', 'Pediatrics', 'Surgery', 'Internal Medicine'],\n",
    "                                    size=n_samples,\n",
    "                                    p=[0.2, 0.2, 0.2, 0.2, 0.2])\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "        'question_id': range(1, n_samples + 1),\n",
    "        'true_answer': true_labels,\n",
    "        'predicted_answer': predicted_labels,\n",
    "        'confidence': confidence_scores,\n",
    "        'difficulty': difficulty,\n",
    "        'specialty': specialties,\n",
    "        'is_correct': true_labels == predicted_labels\n",
    "    })\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Load dataset\n",
    "medical_data = create_medical_qa_dataset(n_samples=200)\n",
    "\n",
    "print(\"üìä Medical Q&A Dataset Overview\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total samples: {len(medical_data)}\")\n",
    "print(f\"\\nFirst 5 rows:\")\n",
    "print(medical_data.head())\n",
    "print(f\"\\nDataset Info:\")\n",
    "print(medical_data.info())\n",
    "print(f\"\\nBasic Statistics:\")\n",
    "print(medical_data.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Practice 2: Accuracy and Basic Metrics\n",
    "\n",
    "### üéØ Learning Objectives\n",
    "- Calculate accuracy, precision, recall, F1-score\n",
    "- Understand the limitations of accuracy alone\n",
    "- Visualize performance metrics\n",
    "\n",
    "### üìñ Key Concepts\n",
    "**Clinical Utility vs Accuracy:** High accuracy doesn't always mean clinical usefulness!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1 Calculate basic evaluation metrics\n",
    "def calculate_basic_metrics(df):\n",
    "    \"\"\"\n",
    "    Calculate and display basic evaluation metrics\n",
    "    \"\"\"\n",
    "    y_true = df['true_answer']\n",
    "    y_pred = df['predicted_answer']\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    f1_macro = f1_score(y_true, y_pred, average='macro')\n",
    "    f1_weighted = f1_score(y_true, y_pred, average='weighted')\n",
    "    \n",
    "    print(\"üìà Basic Evaluation Metrics\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "    print(f\"F1-Score (Macro): {f1_macro:.4f}\")\n",
    "    print(f\"F1-Score (Weighted): {f1_weighted:.4f}\")\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    # Visualization\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0])\n",
    "    axes[0].set_title('Confusion Matrix', fontsize=14, fontweight='bold')\n",
    "    axes[0].set_xlabel('Predicted Label')\n",
    "    axes[0].set_ylabel('True Label')\n",
    "    \n",
    "    # Performance by difficulty\n",
    "    difficulty_acc = df.groupby('difficulty')['is_correct'].mean()\n",
    "    difficulty_order = ['Easy', 'Moderate', 'Hard', 'Expert']\n",
    "    difficulty_acc = difficulty_acc.reindex(difficulty_order)\n",
    "    \n",
    "    axes[1].bar(difficulty_acc.index, difficulty_acc.values, color=['#2ecc71', '#f39c12', '#e74c3c', '#c0392b'])\n",
    "    axes[1].set_title('Accuracy by Difficulty Level', fontsize=14, fontweight='bold')\n",
    "    axes[1].set_xlabel('Difficulty')\n",
    "    axes[1].set_ylabel('Accuracy')\n",
    "    axes[1].set_ylim([0, 1])\n",
    "    axes[1].axhline(y=accuracy, color='red', linestyle='--', label=f'Overall: {accuracy:.3f}')\n",
    "    axes[1].legend()\n",
    "    \n",
    "    # Add percentage labels on bars\n",
    "    for i, v in enumerate(difficulty_acc.values):\n",
    "        axes[1].text(i, v + 0.02, f'{v*100:.1f}%', ha='center', fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return accuracy, f1_macro\n",
    "\n",
    "accuracy, f1 = calculate_basic_metrics(medical_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Practice 3: Hallucination Detection\n",
    "\n",
    "### üéØ Learning Objectives\n",
    "- Identify low-confidence predictions\n",
    "- Detect potential hallucinations\n",
    "- Set confidence thresholds\n",
    "\n",
    "### üìñ Key Concepts\n",
    "**Hallucination Detection:** Models should know when they don't know!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1 Implement hallucination detection\n",
    "def detect_hallucinations(df, confidence_threshold=0.7):\n",
    "    \"\"\"\n",
    "    Detect potential hallucinations based on confidence scores\n",
    "    \"\"\"\n",
    "    # Identify low confidence predictions\n",
    "    df['potential_hallucination'] = (df['confidence'] < confidence_threshold) & (~df['is_correct'])\n",
    "    \n",
    "    hallucination_rate = df['potential_hallucination'].mean()\n",
    "    \n",
    "    print(\"üîç Hallucination Detection Analysis\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Confidence Threshold: {confidence_threshold}\")\n",
    "    print(f\"Potential Hallucinations Detected: {df['potential_hallucination'].sum()}\")\n",
    "    print(f\"Hallucination Rate: {hallucination_rate*100:.2f}%\")\n",
    "    \n",
    "    # Analyze by confidence bins\n",
    "    df['confidence_bin'] = pd.cut(df['confidence'], bins=[0, 0.3, 0.5, 0.7, 0.9, 1.0],\n",
    "                                   labels=['Very Low', 'Low', 'Medium', 'High', 'Very High'])\n",
    "    \n",
    "    # Visualization\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Confidence distribution by correctness\n",
    "    axes[0].hist(df[df['is_correct']]['confidence'], bins=20, alpha=0.6, label='Correct', color='green')\n",
    "    axes[0].hist(df[~df['is_correct']]['confidence'], bins=20, alpha=0.6, label='Incorrect', color='red')\n",
    "    axes[0].axvline(x=confidence_threshold, color='black', linestyle='--', linewidth=2, label=f'Threshold={confidence_threshold}')\n",
    "    axes[0].set_xlabel('Confidence Score')\n",
    "    axes[0].set_ylabel('Frequency')\n",
    "    axes[0].set_title('Confidence Distribution by Correctness', fontsize=14, fontweight='bold')\n",
    "    axes[0].legend()\n",
    "    \n",
    "    # Accuracy by confidence bin\n",
    "    confidence_acc = df.groupby('confidence_bin', observed=True)['is_correct'].agg(['mean', 'count'])\n",
    "    \n",
    "    bars = axes[1].bar(range(len(confidence_acc)), confidence_acc['mean'], \n",
    "                       color=['#e74c3c', '#e67e22', '#f39c12', '#2ecc71', '#27ae60'])\n",
    "    axes[1].set_xticks(range(len(confidence_acc)))\n",
    "    axes[1].set_xticklabels(confidence_acc.index, rotation=45)\n",
    "    axes[1].set_xlabel('Confidence Bin')\n",
    "    axes[1].set_ylabel('Accuracy')\n",
    "    axes[1].set_title('Accuracy by Confidence Level', fontsize=14, fontweight='bold')\n",
    "    axes[1].set_ylim([0, 1])\n",
    "    \n",
    "    # Add count labels\n",
    "    for i, (acc, cnt) in enumerate(zip(confidence_acc['mean'], confidence_acc['count'])):\n",
    "        axes[1].text(i, acc + 0.02, f'{acc*100:.1f}%\\n(n={cnt})', ha='center', fontsize=9)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return hallucination_rate\n",
    "\n",
    "hallucination_rate = detect_hallucinations(medical_data, confidence_threshold=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Practice 4: Factuality Scoring\n",
    "\n",
    "### üéØ Learning Objectives\n",
    "- Implement evidence-based scoring\n",
    "- Weight predictions by confidence and correctness\n",
    "- Create a factuality score pyramid\n",
    "\n",
    "### üìñ Key Concepts\n",
    "**Evidence Levels:** Peer-reviewed research > Clinical guidelines > Medical textbooks > Expert opinion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1 Calculate factuality scores\n",
    "def calculate_factuality_score(df):\n",
    "    \"\"\"\n",
    "    Calculate factuality score based on correctness and confidence\n",
    "    Score: 0 (unverifiable) ‚Üí 10 (multiple high-quality sources)\n",
    "    \"\"\"\n",
    "    def assign_factuality(row):\n",
    "        if row['is_correct']:\n",
    "            # Correct predictions\n",
    "            if row['confidence'] >= 0.9:\n",
    "                return 10  # High confidence, correct\n",
    "            elif row['confidence'] >= 0.7:\n",
    "                return 8   # Medium-high confidence, correct\n",
    "            elif row['confidence'] >= 0.5:\n",
    "                return 6   # Medium confidence, correct\n",
    "            else:\n",
    "                return 4   # Low confidence, but correct\n",
    "        else:\n",
    "            # Incorrect predictions\n",
    "            if row['confidence'] >= 0.7:\n",
    "                return 1   # High confidence, wrong - dangerous!\n",
    "            else:\n",
    "                return 2   # Low confidence, wrong\n",
    "    \n",
    "    df['factuality_score'] = df.apply(assign_factuality, axis=1)\n",
    "    \n",
    "    # Calculate statistics\n",
    "    mean_factuality = df['factuality_score'].mean()\n",
    "    \n",
    "    print(\"üìä Factuality Scoring Analysis\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Mean Factuality Score: {mean_factuality:.2f} / 10\")\n",
    "    print(f\"\\nScore Distribution:\")\n",
    "    print(df['factuality_score'].value_counts().sort_index())\n",
    "    \n",
    "    # Visualization\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Score distribution\n",
    "    score_counts = df['factuality_score'].value_counts().sort_index()\n",
    "    colors = ['#e74c3c' if s <= 3 else '#f39c12' if s <= 6 else '#2ecc71' for s in score_counts.index]\n",
    "    \n",
    "    axes[0].bar(score_counts.index, score_counts.values, color=colors, edgecolor='black')\n",
    "    axes[0].set_xlabel('Factuality Score')\n",
    "    axes[0].set_ylabel('Frequency')\n",
    "    axes[0].set_title('Factuality Score Distribution', fontsize=14, fontweight='bold')\n",
    "    axes[0].axvline(x=mean_factuality, color='red', linestyle='--', linewidth=2, label=f'Mean={mean_factuality:.1f}')\n",
    "    axes[0].legend()\n",
    "    \n",
    "    # Score by specialty\n",
    "    specialty_scores = df.groupby('specialty')['factuality_score'].mean().sort_values()\n",
    "    axes[1].barh(range(len(specialty_scores)), specialty_scores.values, \n",
    "                 color=sns.color_palette('RdYlGn', len(specialty_scores)))\n",
    "    axes[1].set_yticks(range(len(specialty_scores)))\n",
    "    axes[1].set_yticklabels(specialty_scores.index)\n",
    "    axes[1].set_xlabel('Mean Factuality Score')\n",
    "    axes[1].set_title('Factuality by Medical Specialty', fontsize=14, fontweight='bold')\n",
    "    axes[1].axvline(x=mean_factuality, color='red', linestyle='--', linewidth=2, alpha=0.7)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return mean_factuality\n",
    "\n",
    "factuality = calculate_factuality_score(medical_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Practice 5: Consistency Measures\n",
    "\n",
    "### üéØ Learning Objectives\n",
    "- Test response consistency across multiple runs\n",
    "- Measure paraphrase robustness\n",
    "- Calculate agreement rates\n",
    "\n",
    "### üìñ Key Concepts\n",
    "**Consistency Target:** >90% consistency for critical clinical decisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.1 Simulate consistency testing\n",
    "def test_consistency(df, n_repeats=5):\n",
    "    \"\"\"\n",
    "    Simulate repeated model queries to test consistency\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Simulate multiple runs for a subset of questions\n",
    "    sample_size = 50\n",
    "    sample_df = df.sample(n=sample_size, random_state=42)\n",
    "    \n",
    "    consistency_results = []\n",
    "    \n",
    "    for idx, row in sample_df.iterrows():\n",
    "        # Simulate repeated predictions with some variation\n",
    "        base_pred = row['predicted_answer']\n",
    "        base_conf = row['confidence']\n",
    "        \n",
    "        # Higher confidence = more consistent\n",
    "        consistency_prob = 0.5 + (base_conf * 0.4)  # 50-90% consistency range\n",
    "        \n",
    "        repeated_preds = [base_pred]\n",
    "        for _ in range(n_repeats - 1):\n",
    "            if np.random.random() < consistency_prob:\n",
    "                repeated_preds.append(base_pred)\n",
    "            else:\n",
    "                # Random different answer\n",
    "                other_answers = [a for a in [0, 1, 2, 3] if a != base_pred]\n",
    "                repeated_preds.append(np.random.choice(other_answers))\n",
    "        \n",
    "        # Calculate consistency for this question\n",
    "        consistency = (np.array(repeated_preds) == base_pred).mean()\n",
    "        consistency_results.append({\n",
    "            'question_id': row['question_id'],\n",
    "            'confidence': base_conf,\n",
    "            'consistency': consistency,\n",
    "            'is_correct': row['is_correct']\n",
    "        })\n",
    "    \n",
    "    consistency_df = pd.DataFrame(consistency_results)\n",
    "    mean_consistency = consistency_df['consistency'].mean()\n",
    "    \n",
    "    print(\"üîÑ Consistency Testing Results\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Number of questions tested: {sample_size}\")\n",
    "    print(f\"Repeats per question: {n_repeats}\")\n",
    "    print(f\"Mean consistency: {mean_consistency*100:.2f}%\")\n",
    "    print(f\"Target: >90% for critical decisions\")\n",
    "    print(f\"\\nConsistency by correctness:\")\n",
    "    print(consistency_df.groupby('is_correct')['consistency'].agg(['mean', 'std', 'min', 'max']))\n",
    "    \n",
    "    # Visualization\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Consistency histogram\n",
    "    axes[0].hist(consistency_df['consistency'], bins=15, edgecolor='black', alpha=0.7, color='skyblue')\n",
    "    axes[0].axvline(x=0.9, color='red', linestyle='--', linewidth=2, label='Target (90%)')\n",
    "    axes[0].axvline(x=mean_consistency, color='green', linestyle='-', linewidth=2, label=f'Mean ({mean_consistency*100:.1f}%)')\n",
    "    axes[0].set_xlabel('Consistency Score')\n",
    "    axes[0].set_ylabel('Frequency')\n",
    "    axes[0].set_title('Consistency Score Distribution', fontsize=14, fontweight='bold')\n",
    "    axes[0].legend()\n",
    "    \n",
    "    # Confidence vs Consistency scatter\n",
    "    colors = ['green' if c else 'red' for c in consistency_df['is_correct']]\n",
    "    axes[1].scatter(consistency_df['confidence'], consistency_df['consistency'], \n",
    "                    c=colors, alpha=0.6, s=50, edgecolors='black')\n",
    "    axes[1].set_xlabel('Confidence')\n",
    "    axes[1].set_ylabel('Consistency')\n",
    "    axes[1].set_title('Confidence vs Consistency', fontsize=14, fontweight='bold')\n",
    "    axes[1].axhline(y=0.9, color='red', linestyle='--', alpha=0.5)\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add legend\n",
    "    from matplotlib.patches import Patch\n",
    "    legend_elements = [Patch(facecolor='green', alpha=0.6, label='Correct'),\n",
    "                       Patch(facecolor='red', alpha=0.6, label='Incorrect')]\n",
    "    axes[1].legend(handles=legend_elements)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return mean_consistency\n",
    "\n",
    "consistency_score = test_consistency(medical_data, n_repeats=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Practice 6: Uncertainty Quantification\n",
    "\n",
    "### üéØ Learning Objectives\n",
    "- Distinguish between aleatoric and epistemic uncertainty\n",
    "- Set clinical confidence thresholds\n",
    "- Implement \"I don't know\" responses\n",
    "\n",
    "### üìñ Key Concepts\n",
    "**Uncertainty Types:** Aleatoric (data randomness) vs Epistemic (model knowledge gaps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.1 Implement uncertainty quantification\n",
    "def quantify_uncertainty(df):\n",
    "    \"\"\"\n",
    "    Categorize predictions into confidence levels and recommend actions\n",
    "    \"\"\"\n",
    "    def categorize_confidence(conf):\n",
    "        if conf >= 0.9:\n",
    "            return 'High (>90%)', 'Proceed with recommendation'\n",
    "        elif conf >= 0.7:\n",
    "            return 'Medium (70-90%)', 'Flag for review'\n",
    "        elif conf >= 0.5:\n",
    "            return 'Low (50-70%)', 'Require expert consultation'\n",
    "        else:\n",
    "            return 'Very Low (<50%)', 'Decline to answer'\n",
    "    \n",
    "    df[['confidence_level', 'action']] = df['confidence'].apply(\n",
    "        lambda x: pd.Series(categorize_confidence(x))\n",
    "    )\n",
    "    \n",
    "    # Calculate statistics\n",
    "    level_counts = df['confidence_level'].value_counts()\n",
    "    \n",
    "    print(\"‚öñÔ∏è Uncertainty Quantification Results\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"\\nDistribution by Confidence Level:\")\n",
    "    print(level_counts)\n",
    "    print(\"\\nAction Recommendations:\")\n",
    "    print(df.groupby('confidence_level')['action'].first())\n",
    "    \n",
    "    # Calculate accuracy by confidence level\n",
    "    level_accuracy = df.groupby('confidence_level')['is_correct'].mean()\n",
    "    print(\"\\nAccuracy by Confidence Level:\")\n",
    "    print(level_accuracy)\n",
    "    \n",
    "    # Visualization\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    # Distribution by level\n",
    "    level_order = ['Very Low (<50%)', 'Low (50-70%)', 'Medium (70-90%)', 'High (>90%)']\n",
    "    level_counts_ordered = level_counts.reindex(level_order, fill_value=0)\n",
    "    colors = ['#e74c3c', '#e67e22', '#f39c12', '#2ecc71']\n",
    "    \n",
    "    axes[0, 0].bar(range(len(level_counts_ordered)), level_counts_ordered.values, color=colors)\n",
    "    axes[0, 0].set_xticks(range(len(level_counts_ordered)))\n",
    "    axes[0, 0].set_xticklabels(['Very Low', 'Low', 'Medium', 'High'], rotation=45)\n",
    "    axes[0, 0].set_ylabel('Count')\n",
    "    axes[0, 0].set_title('Distribution by Confidence Level', fontsize=13, fontweight='bold')\n",
    "    \n",
    "    # Accuracy by level\n",
    "    level_acc_ordered = level_accuracy.reindex(level_order, fill_value=0)\n",
    "    axes[0, 1].bar(range(len(level_acc_ordered)), level_acc_ordered.values, color=colors)\n",
    "    axes[0, 1].set_xticks(range(len(level_acc_ordered)))\n",
    "    axes[0, 1].set_xticklabels(['Very Low', 'Low', 'Medium', 'High'], rotation=45)\n",
    "    axes[0, 1].set_ylabel('Accuracy')\n",
    "    axes[0, 1].set_ylim([0, 1])\n",
    "    axes[0, 1].set_title('Accuracy by Confidence Level', fontsize=13, fontweight='bold')\n",
    "    axes[0, 1].axhline(y=0.9, color='red', linestyle='--', alpha=0.5, label='90% target')\n",
    "    axes[0, 1].legend()\n",
    "    \n",
    "    # Confidence distribution with thresholds\n",
    "    axes[1, 0].hist(df['confidence'], bins=30, edgecolor='black', alpha=0.7, color='steelblue')\n",
    "    axes[1, 0].axvline(x=0.9, color='green', linestyle='--', linewidth=2, label='High (90%)')\n",
    "    axes[1, 0].axvline(x=0.7, color='orange', linestyle='--', linewidth=2, label='Medium (70%)')\n",
    "    axes[1, 0].axvline(x=0.5, color='red', linestyle='--', linewidth=2, label='Low (50%)')\n",
    "    axes[1, 0].set_xlabel('Confidence Score')\n",
    "    axes[1, 0].set_ylabel('Frequency')\n",
    "    axes[1, 0].set_title('Confidence Distribution with Thresholds', fontsize=13, fontweight='bold')\n",
    "    axes[1, 0].legend()\n",
    "    \n",
    "    # Action recommendation pie chart\n",
    "    action_counts = df['action'].value_counts()\n",
    "    axes[1, 1].pie(action_counts.values, labels=action_counts.index, autopct='%1.1f%%',\n",
    "                   colors=colors, startangle=90)\n",
    "    axes[1, 1].set_title('Recommended Actions Distribution', fontsize=13, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return df\n",
    "\n",
    "medical_data_with_uncertainty = quantify_uncertainty(medical_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Practice 7: Calibration Metrics\n",
    "\n",
    "### üéØ Learning Objectives\n",
    "- Calculate Expected Calibration Error (ECE)\n",
    "- Create reliability diagrams\n",
    "- Understand model calibration\n",
    "\n",
    "### üìñ Key Concepts\n",
    "**Well-Calibrated:** When model says 80% confident, it's correct 80% of the time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.1 Calculate calibration metrics\n",
    "def calculate_calibration(df, n_bins=10):\n",
    "    \"\"\"\n",
    "    Calculate Expected Calibration Error (ECE) and create reliability diagram\n",
    "    \"\"\"\n",
    "    # Prepare data for binary classification (correct vs incorrect)\n",
    "    y_true = df['is_correct'].astype(int)\n",
    "    y_prob = df['confidence']\n",
    "    \n",
    "    # Calculate calibration curve\n",
    "    prob_true, prob_pred = calibration_curve(y_true, y_prob, n_bins=n_bins, strategy='uniform')\n",
    "    \n",
    "    # Calculate ECE\n",
    "    bin_edges = np.linspace(0, 1, n_bins + 1)\n",
    "    bin_indices = np.digitize(y_prob, bin_edges[1:-1])\n",
    "    \n",
    "    ece = 0\n",
    "    for i in range(n_bins):\n",
    "        mask = bin_indices == i\n",
    "        if mask.sum() > 0:\n",
    "            avg_confidence = y_prob[mask].mean()\n",
    "            avg_accuracy = y_true[mask].mean()\n",
    "            ece += (mask.sum() / len(y_prob)) * abs(avg_confidence - avg_accuracy)\n",
    "    \n",
    "    print(\"üéØ Calibration Analysis\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Expected Calibration Error (ECE): {ece:.4f}\")\n",
    "    print(f\"Lower is better (0 = perfect calibration)\")\n",
    "    \n",
    "    # Visualization\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Reliability diagram\n",
    "    axes[0].plot([0, 1], [0, 1], 'k--', label='Perfect Calibration', linewidth=2)\n",
    "    axes[0].plot(prob_pred, prob_true, 's-', label='Model', linewidth=2, markersize=8)\n",
    "    axes[0].set_xlabel('Predicted Probability (Confidence)')\n",
    "    axes[0].set_ylabel('True Probability (Accuracy)')\n",
    "    axes[0].set_title('Reliability Diagram', fontsize=14, fontweight='bold')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    axes[0].set_xlim([0, 1])\n",
    "    axes[0].set_ylim([0, 1])\n",
    "    \n",
    "    # Confidence histogram\n",
    "    axes[1].hist(df['confidence'], bins=20, edgecolor='black', alpha=0.7, color='lightcoral')\n",
    "    axes[1].set_xlabel('Confidence Score')\n",
    "    axes[1].set_ylabel('Frequency')\n",
    "    axes[1].set_title('Confidence Score Distribution', fontsize=14, fontweight='bold')\n",
    "    axes[1].axvline(x=df['confidence'].mean(), color='red', linestyle='--', \n",
    "                    linewidth=2, label=f'Mean={df[\"confidence\"].mean():.3f}')\n",
    "    axes[1].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return ece\n",
    "\n",
    "ece = calculate_calibration(medical_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Practice 8: Clinical Relevance Scoring\n",
    "\n",
    "### üéØ Learning Objectives\n",
    "- Score predictions based on clinical impact\n",
    "- Weight by urgency and patient outcomes\n",
    "- Identify high-risk errors\n",
    "\n",
    "### üìñ Key Concepts\n",
    "**Clinical Relevance:** Would this information improve patient care and outcomes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.1 Calculate clinical relevance scores\n",
    "def assess_clinical_relevance(df):\n",
    "    \"\"\"\n",
    "    Assess clinical relevance of predictions\n",
    "    Score: 1 (harmful) ‚Üí 10 (highly valuable)\n",
    "    \"\"\"\n",
    "    def calculate_relevance(row):\n",
    "        # Base score on correctness and confidence\n",
    "        if row['is_correct']:\n",
    "            base_score = 7 + (row['confidence'] * 3)  # 7-10 for correct\n",
    "        else:\n",
    "            if row['confidence'] > 0.7:\n",
    "                base_score = 1  # High confidence but wrong = harmful\n",
    "            else:\n",
    "                base_score = 3  # Low confidence, wrong = not helpful\n",
    "        \n",
    "        # Adjust by difficulty (harder questions are more valuable when correct)\n",
    "        difficulty_weights = {'Easy': 1.0, 'Moderate': 1.1, 'Hard': 1.2, 'Expert': 1.3}\n",
    "        weight = difficulty_weights.get(row['difficulty'], 1.0)\n",
    "        \n",
    "        score = min(base_score * weight, 10)  # Cap at 10\n",
    "        return score\n",
    "    \n",
    "    df['clinical_relevance'] = df.apply(calculate_relevance, axis=1)\n",
    "    \n",
    "    # Categorize relevance\n",
    "    def categorize_relevance(score):\n",
    "        if score <= 2:\n",
    "            return 'Harmful'\n",
    "        elif score <= 4:\n",
    "            return 'Not Helpful'\n",
    "        elif score <= 7:\n",
    "            return 'Somewhat Useful'\n",
    "        else:\n",
    "            return 'Highly Valuable'\n",
    "    \n",
    "    df['relevance_category'] = df['clinical_relevance'].apply(categorize_relevance)\n",
    "    \n",
    "    mean_relevance = df['clinical_relevance'].mean()\n",
    "    \n",
    "    print(\"üíä Clinical Relevance Assessment\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Mean Clinical Relevance Score: {mean_relevance:.2f} / 10\")\n",
    "    print(\"\\nDistribution by Category:\")\n",
    "    print(df['relevance_category'].value_counts())\n",
    "    print(\"\\nScore by Specialty:\")\n",
    "    print(df.groupby('specialty')['clinical_relevance'].mean().sort_values(ascending=False))\n",
    "    \n",
    "    # Visualization\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    # Score distribution\n",
    "    axes[0, 0].hist(df['clinical_relevance'], bins=20, edgecolor='black', alpha=0.7, color='mediumseagreen')\n",
    "    axes[0, 0].axvline(x=mean_relevance, color='red', linestyle='--', linewidth=2, \n",
    "                       label=f'Mean={mean_relevance:.2f}')\n",
    "    axes[0, 0].set_xlabel('Clinical Relevance Score')\n",
    "    axes[0, 0].set_ylabel('Frequency')\n",
    "    axes[0, 0].set_title('Clinical Relevance Score Distribution', fontsize=13, fontweight='bold')\n",
    "    axes[0, 0].legend()\n",
    "    \n",
    "    # Category distribution\n",
    "    category_counts = df['relevance_category'].value_counts()\n",
    "    category_order = ['Harmful', 'Not Helpful', 'Somewhat Useful', 'Highly Valuable']\n",
    "    category_counts = category_counts.reindex(category_order, fill_value=0)\n",
    "    colors_cat = ['#e74c3c', '#e67e22', '#f39c12', '#2ecc71']\n",
    "    \n",
    "    axes[0, 1].bar(range(len(category_counts)), category_counts.values, color=colors_cat)\n",
    "    axes[0, 1].set_xticks(range(len(category_counts)))\n",
    "    axes[0, 1].set_xticklabels(category_order, rotation=45, ha='right')\n",
    "    axes[0, 1].set_ylabel('Count')\n",
    "    axes[0, 1].set_title('Relevance Category Distribution', fontsize=13, fontweight='bold')\n",
    "    \n",
    "    # Score by specialty\n",
    "    specialty_scores = df.groupby('specialty')['clinical_relevance'].mean().sort_values()\n",
    "    axes[1, 0].barh(range(len(specialty_scores)), specialty_scores.values, \n",
    "                    color=sns.color_palette('viridis', len(specialty_scores)))\n",
    "    axes[1, 0].set_yticks(range(len(specialty_scores)))\n",
    "    axes[1, 0].set_yticklabels(specialty_scores.index)\n",
    "    axes[1, 0].set_xlabel('Mean Clinical Relevance')\n",
    "    axes[1, 0].set_title('Clinical Relevance by Specialty', fontsize=13, fontweight='bold')\n",
    "    axes[1, 0].axvline(x=mean_relevance, color='red', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Score by difficulty\n",
    "    difficulty_scores = df.groupby('difficulty')['clinical_relevance'].mean()\n",
    "    diff_order = ['Easy', 'Moderate', 'Hard', 'Expert']\n",
    "    difficulty_scores = difficulty_scores.reindex(diff_order)\n",
    "    \n",
    "    axes[1, 1].bar(range(len(difficulty_scores)), difficulty_scores.values, \n",
    "                   color=['#3498db', '#9b59b6', '#e74c3c', '#c0392b'])\n",
    "    axes[1, 1].set_xticks(range(len(difficulty_scores)))\n",
    "    axes[1, 1].set_xticklabels(diff_order)\n",
    "    axes[1, 1].set_ylabel('Mean Clinical Relevance')\n",
    "    axes[1, 1].set_title('Clinical Relevance by Difficulty', fontsize=13, fontweight='bold')\n",
    "    axes[1, 1].axhline(y=mean_relevance, color='red', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return mean_relevance\n",
    "\n",
    "clinical_relevance = assess_clinical_relevance(medical_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Practice 9: Safety Assessment\n",
    "\n",
    "### üéØ Learning Objectives\n",
    "- Identify safety-critical errors\n",
    "- Risk stratification (Critical, High, Medium, Low)\n",
    "- Flag dangerous predictions\n",
    "\n",
    "### üìñ Key Concepts\n",
    "**Safety-First:** Zero tolerance for critical safety issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9.1 Implement safety assessment\n",
    "def assess_safety(df):\n",
    "    \"\"\"\n",
    "    Assess safety risk of predictions\n",
    "    \"\"\"\n",
    "    def calculate_risk_level(row):\n",
    "        if not row['is_correct']:\n",
    "            # Incorrect predictions\n",
    "            if row['confidence'] >= 0.9:\n",
    "                return 'CRITICAL'  # High confidence but wrong\n",
    "            elif row['confidence'] >= 0.7:\n",
    "                return 'HIGH'      # Medium-high confidence but wrong\n",
    "            elif row['confidence'] >= 0.5:\n",
    "                return 'MEDIUM'    # Medium confidence but wrong\n",
    "            else:\n",
    "                return 'LOW'       # Low confidence, wrong (safer)\n",
    "        else:\n",
    "            # Correct predictions are generally safe\n",
    "            return 'LOW'\n",
    "    \n",
    "    df['safety_risk'] = df.apply(calculate_risk_level, axis=1)\n",
    "    \n",
    "    # Calculate statistics\n",
    "    risk_counts = df['safety_risk'].value_counts()\n",
    "    critical_rate = (df['safety_risk'] == 'CRITICAL').mean()\n",
    "    \n",
    "    print(\"‚ö†Ô∏è Safety Assessment Results\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Critical Safety Issues: {(df['safety_risk'] == 'CRITICAL').sum()}\")\n",
    "    print(f\"Critical Rate: {critical_rate*100:.2f}%\")\n",
    "    print(\"\\nRisk Level Distribution:\")\n",
    "    print(risk_counts)\n",
    "    \n",
    "    # Flag critical cases\n",
    "    critical_cases = df[df['safety_risk'] == 'CRITICAL']\n",
    "    if len(critical_cases) > 0:\n",
    "        print(\"\\nüö® CRITICAL SAFETY ALERTS:\")\n",
    "        print(critical_cases[['question_id', 'confidence', 'difficulty', 'specialty']].head())\n",
    "    \n",
    "    # Visualization\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Risk pyramid\n",
    "    risk_order = ['CRITICAL', 'HIGH', 'MEDIUM', 'LOW']\n",
    "    risk_counts_ordered = risk_counts.reindex(risk_order, fill_value=0)\n",
    "    colors_risk = ['#c0392b', '#e74c3c', '#f39c12', '#2ecc71']\n",
    "    \n",
    "    y_pos = np.arange(len(risk_order))\n",
    "    axes[0].barh(y_pos, risk_counts_ordered.values, color=colors_risk)\n",
    "    axes[0].set_yticks(y_pos)\n",
    "    axes[0].set_yticklabels(risk_order)\n",
    "    axes[0].set_xlabel('Count')\n",
    "    axes[0].set_title('Safety Risk Distribution (Pyramid)', fontsize=14, fontweight='bold')\n",
    "    axes[0].invert_yaxis()\n",
    "    \n",
    "    # Add counts on bars\n",
    "    for i, v in enumerate(risk_counts_ordered.values):\n",
    "        axes[0].text(v + 1, i, str(int(v)), va='center', fontweight='bold')\n",
    "    \n",
    "    # Risk by specialty\n",
    "    specialty_risk = df[df['safety_risk'].isin(['CRITICAL', 'HIGH'])].groupby('specialty').size()\n",
    "    if len(specialty_risk) > 0:\n",
    "        specialty_risk = specialty_risk.sort_values(ascending=True)\n",
    "        axes[1].barh(range(len(specialty_risk)), specialty_risk.values, \n",
    "                     color=sns.color_palette('Reds_r', len(specialty_risk)))\n",
    "        axes[1].set_yticks(range(len(specialty_risk)))\n",
    "        axes[1].set_yticklabels(specialty_risk.index)\n",
    "        axes[1].set_xlabel('Number of High-Risk Cases')\n",
    "        axes[1].set_title('High-Risk Cases by Specialty', fontsize=14, fontweight='bold')\n",
    "    else:\n",
    "        axes[1].text(0.5, 0.5, 'No high-risk cases', ha='center', va='center', \n",
    "                     transform=axes[1].transAxes, fontsize=14)\n",
    "        axes[1].set_title('High-Risk Cases by Specialty', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return critical_rate\n",
    "\n",
    "critical_rate = assess_safety(medical_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Practice 10: Performance Dashboard Creation\n",
    "\n",
    "### üéØ Learning Objectives\n",
    "- Create comprehensive evaluation dashboard\n",
    "- Aggregate all metrics\n",
    "- Generate executive summary\n",
    "\n",
    "### üìñ Key Concepts\n",
    "**Dashboard Features:** Real-time monitoring, alert system, trend analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10.1 Create comprehensive performance dashboard\n",
    "def create_performance_dashboard(df):\n",
    "    \"\"\"\n",
    "    Create a comprehensive performance dashboard\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\" \" * 15 + \"üìä MEDICAL MODEL EVALUATION DASHBOARD\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Key Performance Indicators\n",
    "    accuracy = df['is_correct'].mean()\n",
    "    mean_confidence = df['confidence'].mean()\n",
    "    mean_factuality = df['factuality_score'].mean()\n",
    "    mean_relevance = df['clinical_relevance'].mean()\n",
    "    critical_safety = (df['safety_risk'] == 'CRITICAL').sum()\n",
    "    hallucination_count = df['potential_hallucination'].sum()\n",
    "    \n",
    "    print(\"\\nüìà KEY PERFORMANCE INDICATORS\")\n",
    "    print(\"-\" * 70)\n",
    "    print(f\"  Overall Accuracy:          {accuracy*100:>6.2f}% \")\n",
    "    print(f\"  Mean Confidence:           {mean_confidence:>6.3f} \")\n",
    "    print(f\"  Factuality Score:          {mean_factuality:>6.2f} / 10\")\n",
    "    print(f\"  Clinical Relevance:        {mean_relevance:>6.2f} / 10\")\n",
    "    print(f\"  Critical Safety Issues:    {critical_safety:>6d} \")\n",
    "    print(f\"  Potential Hallucinations:  {hallucination_count:>6d} \")\n",
    "    \n",
    "    # Performance by Category\n",
    "    print(\"\\nüìä PERFORMANCE BY CATEGORY\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    # By difficulty\n",
    "    print(\"\\n  By Difficulty Level:\")\n",
    "    diff_stats = df.groupby('difficulty')['is_correct'].agg(['mean', 'count'])\n",
    "    for idx, row in diff_stats.iterrows():\n",
    "        print(f\"    {idx:12s}: {row['mean']*100:5.1f}%  (n={int(row['count'])})\")\n",
    "    \n",
    "    # By specialty\n",
    "    print(\"\\n  By Medical Specialty:\")\n",
    "    spec_stats = df.groupby('specialty')['is_correct'].agg(['mean', 'count'])\n",
    "    for idx, row in spec_stats.iterrows():\n",
    "        print(f\"    {idx:20s}: {row['mean']*100:5.1f}%  (n={int(row['count'])})\")\n",
    "    \n",
    "    # Safety breakdown\n",
    "    print(\"\\n‚ö†Ô∏è  SAFETY RISK BREAKDOWN\")\n",
    "    print(\"-\" * 70)\n",
    "    risk_counts = df['safety_risk'].value_counts()\n",
    "    for risk, count in risk_counts.items():\n",
    "        pct = count / len(df) * 100\n",
    "        print(f\"  {risk:10s}: {int(count):4d} ({pct:5.1f}%)\")\n",
    "    \n",
    "    # Recommendations\n",
    "    print(\"\\nüí° RECOMMENDATIONS\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    if accuracy < 0.8:\n",
    "        print(\"  ‚ö†Ô∏è  Accuracy below 80% - Consider model retraining\")\n",
    "    if critical_safety > 0:\n",
    "        print(f\"  üö® {critical_safety} critical safety issues detected - Immediate review required\")\n",
    "    if hallucination_count > len(df) * 0.1:\n",
    "        print(\"  ‚ö†Ô∏è  High hallucination rate - Review confidence calibration\")\n",
    "    if mean_relevance < 7:\n",
    "        print(\"  ‚ö†Ô∏è  Clinical relevance below target - Improve clinical utility\")\n",
    "    \n",
    "    if accuracy >= 0.85 and critical_safety == 0 and mean_relevance >= 7:\n",
    "        print(\"  ‚úÖ Model performance meets quality standards\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\" \" * 25 + \"End of Report\")\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "    \n",
    "    # Create comprehensive visualization\n",
    "    fig = plt.figure(figsize=(16, 10))\n",
    "    gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)\n",
    "    \n",
    "    # 1. KPI Summary\n",
    "    ax1 = fig.add_subplot(gs[0, :])\n",
    "    kpis = ['Accuracy', 'Confidence', 'Factuality', 'Relevance']\n",
    "    values = [accuracy*100, mean_confidence*100, mean_factuality*10, mean_relevance*10]\n",
    "    colors_kpi = ['#3498db', '#9b59b6', '#e67e22', '#2ecc71']\n",
    "    bars = ax1.bar(kpis, values, color=colors_kpi, edgecolor='black', linewidth=1.5)\n",
    "    ax1.set_ylim([0, 100])\n",
    "    ax1.set_ylabel('Score (%)')\n",
    "    ax1.set_title('Key Performance Indicators', fontsize=15, fontweight='bold')\n",
    "    ax1.axhline(y=80, color='red', linestyle='--', alpha=0.5, label='80% Target')\n",
    "    ax1.legend()\n",
    "    for bar, val in zip(bars, values):\n",
    "        height = bar.get_height()\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2., height + 2, f'{val:.1f}%',\n",
    "                ha='center', va='bottom', fontweight='bold', fontsize=11)\n",
    "    \n",
    "    # 2. Accuracy by Difficulty\n",
    "    ax2 = fig.add_subplot(gs[1, 0])\n",
    "    diff_acc = df.groupby('difficulty')['is_correct'].mean()\n",
    "    diff_order = ['Easy', 'Moderate', 'Hard', 'Expert']\n",
    "    diff_acc = diff_acc.reindex(diff_order)\n",
    "    ax2.bar(range(len(diff_acc)), diff_acc.values, color=['#2ecc71', '#f39c12', '#e74c3c', '#c0392b'])\n",
    "    ax2.set_xticks(range(len(diff_acc)))\n",
    "    ax2.set_xticklabels(diff_order, rotation=45)\n",
    "    ax2.set_ylabel('Accuracy')\n",
    "    ax2.set_ylim([0, 1])\n",
    "    ax2.set_title('Accuracy by Difficulty', fontweight='bold')\n",
    "    \n",
    "    # 3. Safety Risk Distribution\n",
    "    ax3 = fig.add_subplot(gs[1, 1])\n",
    "    risk_order = ['LOW', 'MEDIUM', 'HIGH', 'CRITICAL']\n",
    "    risk_counts_ordered = risk_counts.reindex(risk_order, fill_value=0)\n",
    "    colors_risk_dash = ['#2ecc71', '#f39c12', '#e74c3c', '#c0392b']\n",
    "    wedges, texts, autotexts = ax3.pie(risk_counts_ordered.values, labels=risk_order, autopct='%1.1f%%',\n",
    "                                        colors=colors_risk_dash, startangle=90)\n",
    "    ax3.set_title('Safety Risk Distribution', fontweight='bold')\n",
    "    \n",
    "    # 4. Confidence Distribution\n",
    "    ax4 = fig.add_subplot(gs[1, 2])\n",
    "    ax4.hist(df['confidence'], bins=20, edgecolor='black', alpha=0.7, color='steelblue')\n",
    "    ax4.axvline(x=mean_confidence, color='red', linestyle='--', linewidth=2, label=f'Mean={mean_confidence:.3f}')\n",
    "    ax4.set_xlabel('Confidence')\n",
    "    ax4.set_ylabel('Frequency')\n",
    "    ax4.set_title('Confidence Distribution', fontweight='bold')\n",
    "    ax4.legend()\n",
    "    \n",
    "    # 5. Specialty Performance\n",
    "    ax5 = fig.add_subplot(gs[2, :])\n",
    "    spec_acc = df.groupby('specialty')['is_correct'].mean().sort_values()\n",
    "    ax5.barh(range(len(spec_acc)), spec_acc.values, color=sns.color_palette('viridis', len(spec_acc)))\n",
    "    ax5.set_yticks(range(len(spec_acc)))\n",
    "    ax5.set_yticklabels(spec_acc.index)\n",
    "    ax5.set_xlabel('Accuracy')\n",
    "    ax5.set_xlim([0, 1])\n",
    "    ax5.set_title('Performance by Medical Specialty', fontsize=15, fontweight='bold')\n",
    "    ax5.axvline(x=accuracy, color='red', linestyle='--', linewidth=2, alpha=0.7, label=f'Overall={accuracy:.3f}')\n",
    "    ax5.legend()\n",
    "    \n",
    "    plt.suptitle('Medical Model Evaluation Dashboard', fontsize=18, fontweight='bold', y=0.98)\n",
    "    plt.show()\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'mean_confidence': mean_confidence,\n",
    "        'factuality': mean_factuality,\n",
    "        'relevance': mean_relevance,\n",
    "        'critical_safety': critical_safety,\n",
    "        'hallucinations': hallucination_count\n",
    "    }\n",
    "\n",
    "dashboard_metrics = create_performance_dashboard(medical_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üéØ Practice Complete!\n",
    "\n",
    "### Summary of What We Learned:\n",
    "\n",
    "1. **Medical Benchmark Evaluation**: Loading and exploring MedQA-style datasets\n",
    "2. **Accuracy Metrics**: Beyond simple accuracy - F1, precision, recall\n",
    "3. **Hallucination Detection**: Identifying unreliable predictions\n",
    "4. **Factuality Scoring**: Evidence-based assessment\n",
    "5. **Consistency Measures**: Testing response stability\n",
    "6. **Uncertainty Quantification**: When to say \"I don't know\"\n",
    "7. **Calibration Metrics**: ECE and reliability diagrams\n",
    "8. **Clinical Relevance**: Impact on patient care\n",
    "9. **Safety Assessment**: Risk stratification and critical error detection\n",
    "10. **Performance Dashboards**: Comprehensive monitoring\n",
    "\n",
    "### Key Insights:\n",
    "- **Accuracy ‚â† Clinical Utility**: High accuracy doesn't guarantee usefulness\n",
    "- **Safety First**: Zero tolerance for critical errors\n",
    "- **Calibration Matters**: Confidence should match accuracy\n",
    "- **Context is Critical**: Medical specialty and difficulty affect performance\n",
    "\n",
    "### Real-World Applications:\n",
    "- Medical AI deployment decision-making\n",
    "- Continuous model monitoring in production\n",
    "- Regulatory compliance reporting\n",
    "- Clinical validation studies\n",
    "\n",
    "### Next Steps:\n",
    "- Implement bias and fairness testing\n",
    "- Add robustness evaluation\n",
    "- Create automated monitoring pipelines\n",
    "- Generate TRIPOD-compliant reports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üìö Additional Resources\n",
    "\n",
    "### Papers and Guidelines:\n",
    "- **TRIPOD**: Transparent Reporting of a multivariable prediction model for Individual Prognosis Or Diagnosis\n",
    "- **STARD**: Standards for Reporting of Diagnostic Accuracy Studies\n",
    "- **CONSORT-AI**: Reporting guidelines for clinical trials involving AI interventions\n",
    "\n",
    "### Datasets:\n",
    "- **MedQA**: 11,450 USMLE-style questions\n",
    "- **PubMedQA**: 273K biomedical literature questions\n",
    "- **MedMCQA**: 194K questions covering 21 medical subjects\n",
    "- **MMLU Medical**: 1,089 questions across 6 medical topics\n",
    "\n",
    "### Tools:\n",
    "- **Papers with Code**: Medical AI benchmarks leaderboard\n",
    "- **HuggingFace Evaluate**: Comprehensive evaluation metrics\n",
    "- **scikit-learn**: Machine learning evaluation tools\n",
    "- **TorchMetrics**: Deep learning metrics library"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
