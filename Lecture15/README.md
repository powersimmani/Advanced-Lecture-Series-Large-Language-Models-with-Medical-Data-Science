# Lecture 15: Explainable Medical AI

## üìã Overview

**Lecture Title:** Explainable Medical AI: Building Trust Through Transparency  
**Course:** Advanced Lecture Series - Large Language Models with Medical Data Science  
**Instructor:** Ho-min Park  
**Email:** homin.park@ghent.ac.kr | powersimmani@gmail.com

This lecture explores interpretability and explainability methods for AI systems in medical applications, covering attention-based techniques, clinical explanation generation, and user-centered design principles.

---

## üéØ Learning Objectives

1. **Understanding XAI Methods** - Master attention visualization, LRP, gradient-based attribution, and model-agnostic techniques
2. **Clinical Explanation Generation** - Learn to create natural language explanations, counterfactuals, and decision paths
3. **User Requirements** - Address needs of physicians, patients, and regulatory bodies
4. **Practical Implementation** - Apply XAI tools (SHAP, LIME, Captum) to medical AI systems
5. **Trust & Safety** - Balance performance with interpretability while ensuring clinical trust

---

## üìö Lecture Contents

### Part 1: Attention-Based Interpretability (Slides 3-10)
- **Attention Visualization**: Heatmaps showing model focus areas in medical images
- **Layer-wise Relevance Propagation (LRP)**: Backpropagating relevance scores through layers
- **Gradient-Based Attribution**: Saliency maps and sensitivity analysis
- **Integrated Gradients**: Path integration for robust attribution
- **SHAP Values**: Game-theoretic feature importance for medical predictions
- **LIME**: Local interpretable model-agnostic explanations for clinical text
- **Concept Activation Vectors (CAV)**: Testing for high-level medical concepts

### Part 2: Clinical Explanation Generation (Slides 11-17)
- **Natural Language Explanations**: Generating human-readable clinical explanations
- **Counterfactual Explanations**: "What-if" scenarios for treatment planning
- **Decision Path Visualization**: Tracing model reasoning from input to output
- **Uncertainty Communication**: Expressing confidence intervals and prediction uncertainty
- **Evidence Highlighting**: Marking relevant features that support predictions
- **Contrastive Explanations**: Explaining why A rather than B was predicted

### Part 3: Clinical User Requirements (Slides 18-26)
- **Physician Needs**: Integrating XAI into clinical workflows efficiently
- **Patient-Facing Explanations**: Accessible language and visual communication
- **Regulatory Documentation**: FDA compliance and validation requirements
- **Audit Trail Generation**: Creating immutable decision logs
- **Trust Calibration**: Balancing appropriate trust vs. over-reliance
- **Error Analysis & Debugging**: Tools for identifying and fixing model issues
- **Performance vs. Interpretability Trade-off**: Balancing accuracy with transparency

### Applications & Future Directions (Slides 27-30)
- **Case Study**: ICU mortality prediction explanations
- **Hands-On**: Implementing SHAP, LIME, and Captum for medical AI
- **Future Research**: Multimodal explanations, foundation model XAI, standardization
- **Conclusion**: Key takeaways and resources

---

## üí° Key Concepts

### XAI Methods Comparison

| Method | Type | Advantages | Best Use Case |
|--------|------|------------|---------------|
| **Attention Visualization** | Post-hoc | Intuitive, fast | Vision models |
| **LRP** | Post-hoc | Preserves relevance | Deep networks |
| **Integrated Gradients** | Post-hoc | Axiom satisfaction | Any differentiable model |
| **SHAP** | Model-agnostic | Theoretically grounded | Tabular data |
| **LIME** | Model-agnostic | Simple approximation | Text, images |
| **CAV** | Post-hoc | Concept-level | High-level reasoning |

### Clinical Application Priorities

1. **Accuracy First**: XAI should not significantly reduce model performance
2. **Actionable Insights**: Explanations must be clinically useful, not just interpretable
3. **Time Efficiency**: Fit into existing clinical workflows without delays
4. **Trust Calibration**: Help clinicians know when to trust AI vs. rely on expertise
5. **Regulatory Compliance**: Meet FDA and healthcare system documentation requirements

### Implementation Tools

```python
# SHAP Example
import shap
explainer = shap.DeepExplainer(model, background_data)
shap_values = explainer.shap_values(test_data)
shap.image_plot(shap_values, test_images)

# LIME Example
from lime import lime_text
explainer = lime_text.LimeTextExplainer()
explanation = explainer.explain_instance(text, classifier_fn)

# Captum (PyTorch) Example
from captum.attr import IntegratedGradients
ig = IntegratedGradients(model)
attributions = ig.attribute(input_tensor, target=predicted_class)
```

---

## üè• Clinical Considerations

### For Physicians
- Explanations should highlight diagnostically relevant features
- Match explanation complexity to physician expertise level
- Integrate seamlessly into clinical workflow (< 30 seconds)
- Provide uncertainty estimates to guide decision-making

### For Patients
- Use plain language avoiding medical jargon
- Visual explanations (diagrams, heatmaps) enhance understanding
- Address emotional concerns with empathy
- Adapt to varying health literacy levels

### For Regulators
- Document model development process thoroughly
- Provide validation evidence for interpretability claims
- Maintain audit trails of all predictions
- Demonstrate safety and effectiveness through XAI

---

## üîç Research Frontiers

- **Multimodal XAI**: Combining imaging, text, and structured data explanations
- **Foundation Model Interpretability**: Explaining large language and vision models
- **Causal Explanations**: Moving beyond correlation to causation
- **Interactive Explanations**: Dynamic, user-driven exploration of model behavior
- **Evaluation Metrics**: Standardized measures of explanation quality
- **Real-time XAI**: Fast explanations for time-critical clinical decisions

---

## üìñ Key Takeaways

‚úÖ XAI is essential for clinical adoption of medical AI systems  
‚úÖ Multiple methods exist; choose based on model type and use case  
‚úÖ Explanations must be clinically actionable, not just interpretable  
‚úÖ Balance performance with interpretability based on clinical stakes  
‚úÖ Different users (physicians, patients, regulators) have different needs  
‚úÖ Trust calibration prevents both over-reliance and under-utilization  
‚úÖ Regulatory compliance requires robust documentation and audit trails

---

## üõ†Ô∏è Prerequisites

- Understanding of deep learning fundamentals
- Familiarity with medical imaging and clinical data
- Basic knowledge of Python and PyTorch/TensorFlow
- Awareness of healthcare regulations (helpful but not required)

---

## üì¶ Slide Files

This lecture contains **30 HTML slides** organized as follows:

- **Slide 01**: Title slide
- **Slide 02**: Lecture contents overview
- **Slides 03-10**: Part 1 - Attention-Based Interpretability
- **Slides 11-17**: Part 2 - Clinical Explanation Generation
- **Slides 18-26**: Part 3 - Clinical User Requirements
- **Slides 27-30**: Applications, hands-on, future research, and conclusion

All slides use a consistent design based on the course template with:
- Blue gradient backgrounds (#1E64C8) for title and section slides
- White backgrounds for content slides
- Interactive hover effects
- Clear visual hierarchy
- Accessible typography (Aptos, Segoe UI)

---

## üìö Recommended Resources

### Papers
- "Attention is All You Need" - Vaswani et al. (2017)
- "Layer-wise Relevance Propagation" - Bach et al. (2015)
- "Axiomatic Attribution for Deep Networks" - Sundararajan et al. (2017)
- "A Unified Approach to Interpreting Model Predictions" - Lundberg & Lee (2017)
- "Why Should I Trust You?" - Ribeiro et al. (2016)

### Tools & Libraries
- SHAP: https://github.com/slundberg/shap
- LIME: https://github.com/marcotcr/lime
- Captum: https://captum.ai/
- InterpretML: https://interpret.ml/

### Guidelines
- FDA Guidance on AI/ML-Based Software as Medical Devices
- EU AI Act - High-Risk AI Systems Requirements

---

## üåü About the Instructor

**Ho-min Park** is a researcher and educator specializing in medical AI and data science. His work focuses on developing interpretable and trustworthy AI systems for healthcare applications.

**Contact:**
- üìß homin.park@ghent.ac.kr
- üìß powersimmani@gmail.com
- üèõÔ∏è Ghent University

---

## üìÑ License & Usage

These lecture materials are provided for educational purposes. Please contact the instructor for questions about usage and distribution.

---

**Last Updated:** November 2025  
**Version:** 1.0 